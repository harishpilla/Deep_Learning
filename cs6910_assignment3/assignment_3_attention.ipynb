{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a56e4a2e",
      "metadata": {
        "id": "a56e4a2e"
      },
      "outputs": [],
      "source": [
        "# Import the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "import random\n",
        "from math import log\n",
        "\n",
        "import tensorflow \n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import LSTM, SimpleRNN, GRU, Embedding, Dense, TimeDistributed, Concatenate, AdditiveAttention "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install wandb and wandbcallback libraries\n",
        "%pip install wandb -q\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvElZMAcEqdg",
        "outputId": "60ec4cb0-1227-45a1-d450-68937dbaa0da"
      },
      "id": "LvElZMAcEqdg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8 MB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 52.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 45.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.font_manager import FontProperties\n",
        "from IPython.display import HTML as html_print\n",
        "import time\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "Ck70W2o9Fw8P"
      },
      "id": "Ck70W2o9Fw8P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the dataset\n"
      ],
      "metadata": {
        "id": "W0HZLliq4vAw"
      },
      "id": "W0HZLliq4vAw"
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "if not os.path.exists('/content/dakshina_dataset_v1.0.tar'):\n",
        "    !wget \"https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67pDt3xkEr06",
        "outputId": "256fdccb-f5f6-4038-b9d1-699400359b38"
      },
      "id": "67pDt3xkEr06",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-07 06:27:05--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.121.128, 108.177.120.128, 142.251.6.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.121.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G   139MB/s    in 11s     \n",
            "\n",
            "2022-05-07 06:27:16 (167 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip every file\n",
        "!tar -xvf /content/dakshina_dataset_v1.0.tar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPvCnsn6Etg3",
        "outputId": "323d82a8-3dc2-44ce-a6e9-6f74859bcff0"
      },
      "id": "ZPvCnsn6Etg3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dakshina_dataset_v1.0/bn/\n",
            "dakshina_dataset_v1.0/bn/lexicons/\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/romanized/\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/\n",
            "dakshina_dataset_v1.0/gu/lexicons/\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/romanized/\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/\n",
            "dakshina_dataset_v1.0/hi/lexicons/\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/romanized/\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/\n",
            "dakshina_dataset_v1.0/kn/lexicons/\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/kn/romanized/\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/\n",
            "dakshina_dataset_v1.0/ml/lexicons/\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.rom.txt\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/romanized/\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/\n",
            "dakshina_dataset_v1.0/mr/lexicons/\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/romanized/\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/\n",
            "dakshina_dataset_v1.0/pa/lexicons/\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/pa/romanized/\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/\n",
            "dakshina_dataset_v1.0/sd/lexicons/\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/sd/romanized/\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/\n",
            "dakshina_dataset_v1.0/si/lexicons/\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/romanized/\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/\n",
            "dakshina_dataset_v1.0/ta/lexicons/\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/romanized/\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/\n",
            "dakshina_dataset_v1.0/te/lexicons/\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/romanized/\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ur/\n",
            "dakshina_dataset_v1.0/ur/lexicons/\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/romanized/\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training dataset file\n",
        "train_file = open(\"/content/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\")\n",
        "read_train_file = csv.reader(train_file, delimiter=\"\\t\")\n",
        "\n",
        "# Load the validation dataset file\n",
        "val_file = open(\"/content/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\")\n",
        "read_val_file = csv.reader(val_file, delimiter=\"\\t\")\n",
        "\n",
        "# Load the test dataset file\n",
        "test_file = open(\"/content/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\")\n",
        "read_test_file = csv.reader(test_file, delimiter=\"\\t\")\n",
        "\n",
        "# Path to save the predictions file\n",
        "predictions_path = 'predictions.tsv'"
      ],
      "metadata": {
        "id": "QyyeiUwvEvS7"
      },
      "id": "QyyeiUwvEvS7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess and Encode Data"
      ],
      "metadata": {
        "id": "ZckeDDQeH8Bk"
      },
      "id": "ZckeDDQeH8Bk"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all the input words(English) and target words(Telugu) in Training dataset\n",
        "telugu_words = []\n",
        "english_words = []\n",
        "\n",
        "for i in read_train_file:   \n",
        "    telugu_words.append(\"\\t\" + str(i[0]) + \"\\n\")\n",
        "    english_words.append(str(i[1]))\n",
        "\n",
        "# Get all the input words(English) and target words(Telugu) in Validation dataset\n",
        "val_telugu_words = []\n",
        "val_english_words = []\n",
        "\n",
        "for i in read_val_file:\n",
        "    val_telugu_words.append(\"\\t\" + str(i[0]) + \"\\n\")\n",
        "    val_english_words.append(str(i[1]))\n",
        "\n",
        "\n",
        "# Get all the input words(English) and target words(Telugu) in Test dataset\n",
        "test_telugu_words = []\n",
        "test_english_words = []\n",
        "\n",
        "for i in read_test_file:\n",
        "    test_telugu_words.append(\"\\t\" + str(i[0]) + \"\\n\")\n",
        "    test_english_words.append(str(i[1]))\n"
      ],
      "metadata": {
        "id": "uVFcA9-RE1Qx"
      },
      "id": "uVFcA9-RE1Qx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the total list of input words(English)\n",
        "total_input_words = english_words + val_english_words + test_english_words\n",
        "\n",
        "# Get the total list of target words(Telugu)\n",
        "total_output_words = telugu_words + val_telugu_words + test_telugu_words"
      ],
      "metadata": {
        "id": "TxFzi3gtE2uJ"
      },
      "id": "TxFzi3gtE2uJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the total input and target language characters \n",
        "\n",
        "english_characters = set()\n",
        "english_characters.add(\" \")\n",
        "telugu_characters = set()\n",
        "telugu_characters.add(\" \")\n",
        "\n",
        "for word in total_input_words:\n",
        "    for char in word:\n",
        "        if char not in english_characters:\n",
        "            english_characters.add(char)\n",
        "\n",
        "for word in total_output_words:\n",
        "    for char in word:\n",
        "        if char not in telugu_characters:\n",
        "            telugu_characters.add(char)"
      ],
      "metadata": {
        "id": "iINDSGHJE4b2"
      },
      "id": "iINDSGHJE4b2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the list of input characters(English) and target characters(Telugu)\n",
        "english_characters = sorted(list(english_characters))\n",
        "telugu_characters = sorted(list(telugu_characters))\n",
        "\n",
        "# Get total number of characters in Input \n",
        "num_encoder_tokens = len(english_characters)\n",
        "# Get total number of characters in Output \n",
        "num_decoder_tokens = len(telugu_characters)\n",
        "\n",
        "# Get maximum length of the word in total input words\n",
        "max_encoder_seq_length = max([len(text) for text in total_input_words])\n",
        "# Get maximum length of the word in total target words\n",
        "max_decoder_seq_length = max([len(text) for text in total_output_words])\n",
        "\n",
        "print(\"Summary of the dataset :\")\n",
        "print(\"Number of train samples :\" , len(english_words))\n",
        "print(\"Number of val samples :\" , len(val_english_words))\n",
        "print(\"Number of test samples :\" , len(test_english_words))\n",
        "print(\"Number of unique input tokens :\" , num_encoder_tokens)\n",
        "print(\"Number of unique output tokens :\" , num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\" , max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\" , max_decoder_seq_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myI3PSgCE56-",
        "outputId": "5283d0e5-6467-42d8-cd12-d926d8e730d6"
      },
      "id": "myI3PSgCE56-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary of the dataset :\n",
            "Number of train samples : 58550\n",
            "Number of val samples : 5683\n",
            "Number of test samples : 5747\n",
            "Number of unique input tokens : 27\n",
            "Number of unique output tokens : 66\n",
            "Max sequence length for inputs: 25\n",
            "Max sequence length for outputs: 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary for every english character with an index associated to it\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(english_characters)])\n",
        "# Dictionary for every telugu character with an index associated to it\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(telugu_characters)])\n",
        "\n",
        "# Dictionary for every english character with an index associated to it\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "# Dictionary for every english character with an index associated to it\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "# Preparing train encoder and decoder inputs and decoder target\n",
        "encoder_input_data = np.zeros((len(english_words), max_encoder_seq_length), dtype=\"float32\")\n",
        "decoder_input_data = np.zeros((len(english_words), max_decoder_seq_length), dtype=\"float32\")\n",
        "decoder_target_data = np.zeros((len(english_words), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "# Writing the encoder , decoder characters with the indexes in input and target token index \n",
        "for i, (english, telugu) in enumerate(zip(english_words, telugu_words)):\n",
        "    for t, char in enumerate(english):\n",
        "        encoder_input_data[i, t] = input_token_index[char]\n",
        "    \n",
        "    for t, char in enumerate(telugu):\n",
        "        decoder_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            # One got encoding the decoder_target_data  and decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0    \n",
        "    decoder_input_data[i, t+1:] = target_token_index[' ']\n",
        "    decoder_target_data[i, t :, target_token_index[' ']] = 1.0"
      ],
      "metadata": {
        "id": "0Td6oWd5E7BS"
      },
      "id": "0Td6oWd5E7BS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing validation encoder and decoder inputs\n",
        "\n",
        "encoder_val_input_data = np.zeros((len(val_english_words), max_encoder_seq_length), dtype=\"float32\")\n",
        "decoder_val_input_data = np.zeros((len(val_english_words), max_decoder_seq_length), dtype=\"float32\")\n",
        "decoder_val_target_data = np.zeros((len(val_english_words), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "for i, (english, telugu) in enumerate(zip(val_english_words, val_telugu_words)):\n",
        "    for t, char in enumerate(english):\n",
        "        encoder_val_input_data[i, t] = input_token_index[char]\n",
        "  \n",
        "    for t, char in enumerate(telugu):\n",
        "        decoder_val_input_data[i, t] =  target_token_index[char]\n",
        "        if t > 0:\n",
        "            # One got encoding the validation decoder target data  and validation decoder target data will be ahead by one timestep and will not include the start character.\n",
        "            decoder_val_target_data[i, t - 1, target_token_index[char]] = 1.0   \n",
        "    decoder_val_input_data[i, t+1:] = target_token_index[' ']\n",
        "    decoder_val_target_data[i, t :, target_token_index[' ']] = 1.0"
      ],
      "metadata": {
        "id": "ieQIs1mHE9IB"
      },
      "id": "ieQIs1mHE9IB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing test encoder and decoder inputs\n",
        "\n",
        "encoder_test_input_data = np.zeros((len(test_english_words), max_encoder_seq_length), dtype=\"float32\")\n",
        "decoder_test_input_data = np.zeros((len(test_english_words), max_decoder_seq_length), dtype=\"float32\")\n",
        "decoder_test_target_data = np.zeros((len(test_english_words), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "for i, (english, telugu) in enumerate(zip(test_english_words, test_telugu_words)):\n",
        "    for t, char in enumerate(english):\n",
        "        encoder_test_input_data[i, t] = input_token_index[char]\n",
        "  \n",
        "    for t, char in enumerate(telugu):\n",
        "        decoder_test_input_data[i, t] =  target_token_index[char]\n",
        "        if t > 0:\n",
        "            # One got encoding the validation decoder target data  and validation decoder target data will be ahead by one timestep and will not include the start character.\n",
        "            decoder_test_target_data[i, t - 1, target_token_index[char]] = 1.0   \n",
        "    decoder_test_input_data[i, t+1:] = target_token_index[' ']\n",
        "    decoder_test_target_data[i, t :, target_token_index[' ']] = 1.0"
      ],
      "metadata": {
        "id": "3klNgsNnvh7H"
      },
      "id": "3klNgsNnvh7H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the validation telugu and english words to numpy arrays\n",
        "val_telugu_words = np.array(val_telugu_words)\n",
        "val_english_words = np.array(val_english_words)\n",
        "\n",
        "# Converting the test telugu and english words to numpy arrays\n",
        "test_telugu_words = np.array(test_telugu_words)\n",
        "test_english_words = np.array(test_english_words)"
      ],
      "metadata": {
        "id": "JGtZsL7mE-u7"
      },
      "id": "JGtZsL7mE-u7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(predictions_path, 'w') as f:\n",
        "        f.write('Input English Words, Predicted Telugu Words, Actual Telugu Words\\n')"
      ],
      "metadata": {
        "id": "wSStbTSH0c4e"
      },
      "id": "wSStbTSH0c4e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Build Model"
      ],
      "metadata": {
        "id": "_7Wfd-zjIDZ8"
      },
      "id": "_7Wfd-zjIDZ8"
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomRNN(object):\n",
        "    \n",
        "    def __init__(self, input_embedding_size, cell_type='GRU', hidden_layer_size=32,\n",
        "                 num_encoder_layers=2, num_decoder_layers = 2, dropout=0.1, \n",
        "                 batch_size=32, epochs=25, is_test_model=False):\n",
        "\n",
        "        self.cell_type = cell_type\n",
        "        self.input_embedding_size = input_embedding_size\n",
        "        self.cell_type = cell_type\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "        self.num_encoder_layers = num_encoder_layers\n",
        "        self.num_decoder_layers = num_decoder_layers\n",
        "        self.dropout = dropout\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.is_test_model = is_test_model\n",
        "\n",
        "\n",
        "    def training(self):\n",
        "        \"\"\"\n",
        "        Method to train the model using LSTM or GRU or RNN and\n",
        "        return model along with encoder_layers and decoder_layers.\n",
        "        \"\"\"\n",
        "\n",
        "        # Choose customFunction based on cell_type\n",
        "        customFunction = None\n",
        "        if self.cell_type == 'LSTM':\n",
        "            customFunction = LSTM\n",
        "        elif self.cell_type == 'GRU':\n",
        "            customFunction = GRU\n",
        "        elif self.cell_type == 'RNN':\n",
        "            customFunction = SimpleRNN\n",
        "\n",
        "        # Encoders\n",
        "        encoder_inputs = Input(shape=(None,))\n",
        "        encoder_embedding = Embedding(num_encoder_tokens,self.input_embedding_size, input_length = max_encoder_seq_length)(encoder_inputs)\n",
        "        \n",
        "        # get encoder_layers and encoder_states\n",
        "        encoder_layers = []\n",
        "        encoder_states = []    \n",
        "        if self.cell_type == 'LSTM':\n",
        "            encoder = customFunction(self.hidden_layer_size, return_sequences=True, return_state=True, dropout = self.dropout)\n",
        "            encoder_layers.append(encoder)\n",
        "            encoder_outputs, state_h, state_c = encoder(encoder_embedding)\n",
        "            encoder_states.append([state_h, state_c])\n",
        "\n",
        "            for i in range(1,self.num_encoder_layers):\n",
        "                encoder = customFunction(self.hidden_layer_size,return_sequences=True,return_state=True, dropout = self.dropout) \n",
        "                encoder_layers.append(encoder)\n",
        "                encoder_outputs, state_h, state_c = encoder(encoder_outputs)\n",
        "                encoder_states.append([state_h, state_c])\n",
        "        else:\n",
        "            encoder = customFunction(self.hidden_layer_size, return_sequences=True, return_state=True, dropout = self.dropout)\n",
        "            encoder_layers.append(encoder)\n",
        "            encoder_outputs, state_h = encoder(encoder_embedding)\n",
        "            encoder_states.append([state_h])\n",
        "\n",
        "            for i in range(1,self.num_encoder_layers):\n",
        "                encoder = customFunction(self.hidden_layer_size,return_sequences=True,return_state=True, dropout = self.dropout) \n",
        "                encoder_layers.append(encoder)\n",
        "                encoder_outputs, state_h = encoder(encoder_outputs)\n",
        "                encoder_states.append([state_h])\n",
        "        \n",
        "        # Decoders\n",
        "        decoder_inputs = Input(shape=(None,))\n",
        "        decoder_embedding = Embedding(num_decoder_tokens,self.input_embedding_size, input_length = max_decoder_seq_length)(decoder_inputs)\n",
        "\n",
        "        # get decoder_layers\n",
        "        decoder_layers = []\n",
        "        if self.cell_type == 'LSTM':\n",
        "            decoder_lstm = customFunction(self.hidden_layer_size, return_sequences=True, return_state=True, dropout = self.dropout)\n",
        "            decoder_layers.append(decoder_lstm)\n",
        "            decoder_outputs, _ , _ = decoder_lstm(decoder_embedding, initial_state=encoder_states[0])\n",
        "            for i in range(1,self.num_decoder_layers):\n",
        "                decoder_lstm = customFunction(self.hidden_layer_size, return_sequences=True, return_state=True, dropout = self.dropout)\n",
        "                decoder_layers.append(decoder_lstm)\n",
        "                decoder_outputs, _ , _  = decoder_lstm(decoder_outputs, initial_state=encoder_states[i])\n",
        "        else:\n",
        "            decoder_GRU = customFunction(self.hidden_layer_size, return_sequences=True, return_state=True, dropout = self.dropout)\n",
        "            decoder_layers.append(decoder_GRU)\n",
        "            decoder_outputs, _ = decoder_GRU(decoder_embedding, initial_state=encoder_states[0])\n",
        "            for i in range(1,self.num_decoder_layers):\n",
        "                decoder_GRU = customFunction(self.hidden_layer_size, return_sequences=True, return_state=True, dropout = self.dropout)\n",
        "                decoder_layers.append(decoder_GRU)\n",
        "                decoder_outputs, _  = decoder_GRU(decoder_outputs, initial_state=encoder_states[i])\n",
        "\n",
        "        # Add decoder attention layer\n",
        "        decoder_attention = AdditiveAttention(name=\"decoder_attention\")\n",
        "        decoder_concat = Concatenate(name=\"decoder_concat\")\n",
        "        context_vector, _ = decoder_attention([decoder_outputs, encoder_outputs], return_attention_scores=True)\n",
        "        decoder_outputs = decoder_concat([decoder_outputs, context_vector])\n",
        "        \n",
        "        # Add dense layer to decoder\n",
        "        decoder_dense = Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "        decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "        # Create a model, compile and fit the model using the optimizer.\n",
        "        model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "        model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "        model.fit(\n",
        "                [encoder_input_data, decoder_input_data],\n",
        "                decoder_target_data,\n",
        "                batch_size=self.batch_size,\n",
        "                epochs=self.epochs,\n",
        "                callbacks=[WandbCallback()]\n",
        "            )\n",
        "\n",
        "        return model, encoder_layers, decoder_layers\n",
        "\n",
        "\n",
        "    def inference_model(self, model, encoder_layers, decoder_layers):\n",
        "        \"\"\"\n",
        "        Create the encoder_model, decoder_model and return using the\n",
        "        arguments model, encoder_layers and decoder_layers\n",
        "        \"\"\"\n",
        "        \n",
        "        # generate encoder_model \n",
        "        encoder_inputs = model.input[0]  # input_1\n",
        "        encoder_states = []\n",
        "        encoder_outputs = model.layers[2](encoder_inputs)    # embedding 1\n",
        "\n",
        "        if self.cell_type == 'LSTM':\n",
        "            for i in range(self.num_encoder_layers):\n",
        "                encoder_outputs, state_h_enc, state_c_enc = encoder_layers[i](encoder_outputs)\n",
        "                encoder_states += [state_h_enc, state_c_enc]\n",
        "        else:\n",
        "            for i in range(self.num_encoder_layers):\n",
        "                encoder_outputs, state_h_enc = encoder_layers[i](encoder_outputs)\n",
        "                encoder_states += [state_h_enc]\n",
        "\n",
        "        encoder_model = Model(encoder_inputs, encoder_states + [encoder_outputs])\n",
        "\n",
        "\n",
        "        decoder_inputs = model.input[1]       # input_2\n",
        "        decoder_outputs = model.layers[3]((decoder_inputs))  # embedding 2\n",
        "\n",
        "        decoder_states = []\n",
        "        decoder_states_inputs = []\n",
        "        \n",
        "        if self.cell_type == 'LSTM':\n",
        "            for i in range(self.num_decoder_layers):\n",
        "                decoder_states_inputs += [Input(shape=(self.hidden_layer_size,)), Input(shape=(self.hidden_layer_size,))]\n",
        "                j = 0\n",
        "            for i in range(self.num_decoder_layers):\n",
        "                decoder_outputs, state_h_dec, state_c_dec = decoder_layers[i](decoder_outputs, initial_state=decoder_states_inputs[i+j:i+j+2])\n",
        "                decoder_states += [state_h_dec , state_c_dec]\n",
        "                j += 1\n",
        "        else:\n",
        "            for i in range(self.num_decoder_layers):\n",
        "                decoder_states_inputs += [Input(shape=(self.hidden_layer_size,))]\n",
        "            for i in range(self.num_decoder_layers):\n",
        "                decoder_outputs, state_h_dec = decoder_layers[i](decoder_outputs, initial_state=decoder_states_inputs[i])\n",
        "                decoder_states += [state_h_dec]\n",
        "\n",
        "        attention_layer = model.layers[4+2*self.num_encoder_layers]\n",
        "        attention_input = Input(shape=(max_encoder_seq_length,self.hidden_layer_size))   \n",
        "        context_vec, attention_weights = attention_layer([decoder_outputs, attention_input], return_attention_scores=True)\n",
        "        \n",
        "        concat_layer = model.layers[5+2*self.num_encoder_layers]\n",
        "        decoder_outputs = concat_layer([decoder_outputs, context_vec])\n",
        "\n",
        "        decoder_dense = model.layers[6+2*self.num_encoder_layers]\n",
        "        decoder_outputs = decoder_dense(decoder_outputs)\n",
        "        decoder_model = Model([decoder_inputs] + decoder_states_inputs + [attention_input], [decoder_outputs] + decoder_states + [attention_weights])\n",
        "\n",
        "        return encoder_model, decoder_model\n",
        "\n",
        "\n",
        "    def decode_sequence(self, input_seq, encoder_model, decoder_model):\n",
        "        \"\"\"\n",
        "        Method to decode an input sequence using the encoder_model\n",
        "        and decoder_model\n",
        "        \"\"\"\n",
        "        states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "        # get attention input\n",
        "        attention_input = states_value[-1]\n",
        "        states_value = states_value[:-1]\n",
        "        target_seq = np.zeros((1, 1)) \n",
        "        target_seq[0, 0] = target_token_index[\"\\t\"]\n",
        "        attention_weights = []\n",
        "\n",
        "        # continue decoding input sequence till '\\n' is found or the sequence\n",
        "        # exceeds the maximum decoder sequence length.\n",
        "        stop_condition = False\n",
        "        decoded_sentence = \"\"\n",
        "        while not stop_condition:\n",
        "            output_tokens = decoder_model.predict([target_seq] + states_value + [attention_input])\n",
        "            sampled_token_index = np.argmax(output_tokens[0][0, -1, :])\n",
        "            sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "            decoded_sentence += sampled_char\n",
        "            if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "                stop_condition = True\n",
        "\n",
        "            target_seq = np.zeros((1, 1))\n",
        "            target_seq[0, 0] = sampled_token_index\n",
        "            states_value = output_tokens[1:-1]\n",
        "            attention_weights.append(output_tokens[-1][0][0])\n",
        "            \n",
        "        return decoded_sentence, attention_weights\n",
        "\n",
        "    \n",
        "    def test_and_calculate_accuraccy(self, encoder_model, decoder_model, tmp_english_words, \n",
        "                                     tmp_telugu_words, tmp_encoder_input_data, is_val_accuracy=True):\n",
        "        \"\"\"\n",
        "        Calculate accuracy using the data and write it to prediction file.\n",
        "        \"\"\"\n",
        "        correct = 0\n",
        "        n = tmp_telugu_words.shape[0]\n",
        "        for i in range(n):\n",
        "            input = tmp_encoder_input_data[i:i+1]\n",
        "            output, _ = self.decode_sequence(input,encoder_model, decoder_model)\n",
        "            with open(predictions_path, 'a') as f:\n",
        "                    f.write('{} , {} , {}\\n'.format(tmp_english_words[i].strip(), output.strip(), tmp_telugu_words[i].strip()))\n",
        "            if output.strip() == tmp_telugu_words[i].strip():\n",
        "                correct += 1\n",
        "\n",
        "        # log accuracy to wandb\n",
        "        if is_val_accuracy:\n",
        "            wandb.log({'val_accuracy' : correct*100/n})\n",
        "            print('val_accuracy', correct*100/n)\n",
        "        else:\n",
        "            wandb.log({'test_accuracy' : correct*100/n})\n",
        "            print('test_accuracy', correct*100/n)\n",
        "\n"
      ],
      "metadata": {
        "id": "7DHTqx6sFBvQ"
      },
      "id": "7DHTqx6sFBvQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sweep config"
      ],
      "metadata": {
        "id": "UPwcSFfOISi7"
      },
      "id": "UPwcSFfOISi7"
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'metric':\n",
        "    {\n",
        "        'goal': 'maximize',\n",
        "        'name': 'val_accuracy'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'input_embedding_size':\n",
        "        {\n",
        "            'values': [128, 256, 512]\n",
        "        },\n",
        "        'hidden_layer_size':\n",
        "        {\n",
        "            'values': [128, 256, 512]\n",
        "        },\n",
        "        'cell_type':\n",
        "        {\n",
        "            'values': ['LSTM', 'RNN', 'GRU']\n",
        "        },\n",
        "        'num_layers':\n",
        "        {\n",
        "            'values': [1, 2, 3]\n",
        "        },\n",
        "        'batch_size':\n",
        "        {\n",
        "            'values': [128, 256, 512]\n",
        "        },\n",
        "        'dropout':\n",
        "        {\n",
        "            'values': [0.1,0.2,0.3]\n",
        "        },\n",
        "        'epochs':\n",
        "        {\n",
        "            'values': [25]\n",
        "        }\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "vizw0Nx5GQJY"
      },
      "id": "vizw0Nx5GQJY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, entity=\"cs21m010-cs21m041\", project=\"DL_Assignment_3_b\")"
      ],
      "metadata": {
        "id": "f424NbphGTFI"
      },
      "id": "f424NbphGTFI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "\n",
        "    # Create a new WandB run\n",
        "    wandb.init(config=sweep_config)\n",
        "    \n",
        "    # Construct the run name\n",
        "    config = wandb.config\n",
        "    wandb.run.name = 'ATT_' + config.cell_type + '' + str(config.input_embedding_size) + '_hs' + str(config.hidden_layer_size) + 'bs' + str(config.batch_size)\n",
        "    \n",
        "    print('training...')\n",
        "    model_rnn_obj = CustomRNN(config.input_embedding_size, config.cell_type , config.hidden_layer_size, \n",
        "                              config.num_layers, config.num_layers, config.dropout, config.batch_size, config.epochs)\n",
        "    model, encoder_layers, decoder_layers = model_rnn_obj.training()    \n",
        "\n",
        "    print('inferrencing...')\n",
        "    encoder_model, decoder_model = model_rnn_obj.inference_model(model, encoder_layers, decoder_layers)\n",
        "    \n",
        "    print('decoding sequence...')\n",
        "    model_rnn_obj.test_and_calculate_accuraccy(encoder_model, decoder_model, val_english_words, val_telugu_words, encoder_val_input_data)"
      ],
      "metadata": {
        "id": "lZ82zKS5GU_2"
      },
      "id": "lZ82zKS5GU_2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, train, count = 15)"
      ],
      "metadata": {
        "id": "Dnw09xJ3GZtV"
      },
      "id": "Dnw09xJ3GZtV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fitting the Best Model"
      ],
      "metadata": {
        "id": "JhdMWLqfr_26"
      },
      "id": "JhdMWLqfr_26"
    },
    {
      "cell_type": "code",
      "source": [
        "best_sweep_config = {\n",
        "    'method': 'random',\n",
        "    'metric':\n",
        "    {\n",
        "        'goal': 'maximize',\n",
        "        'name': 'val_accuracy'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'input_embedding_size':\n",
        "        {\n",
        "            'values': [128]\n",
        "        },\n",
        "        'hidden_layer_size':\n",
        "        {\n",
        "            'values': [512]\n",
        "        },\n",
        "        'cell_type':\n",
        "        {\n",
        "            'values': ['GRU']\n",
        "        },\n",
        "        'num_layers':\n",
        "        {\n",
        "            'values': [1]\n",
        "        },\n",
        "        'batch_size':\n",
        "        {\n",
        "            'values': [128]\n",
        "        },\n",
        "        'dropout':\n",
        "        {\n",
        "            'values': [0.1]\n",
        "        },\n",
        "        'epochs':\n",
        "        {\n",
        "            'values': [25]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "7HYxvObhGEr7"
      },
      "id": "7HYxvObhGEr7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cstr(s, color='black'):\n",
        "    if s == ' ':\n",
        "        return \"<text style=color:#000;padding-left:10px;background-color:{}> </text>\".format(color, s)\n",
        "    else:\n",
        "        return \"<text style=color:#000;background-color:{}>{} </text>\".format(color, s)\n",
        "\n",
        "def print_color(t):            \n",
        "      display(html_print(''.join([cstr(ti, color=ci) for ti,ci in t])))\n",
        "\n",
        "def get_clr(value):\n",
        "      colors = [ '#FFFFFF', '#FFFFFF', '#FFFFFF', '#FFFFFF', '#f9e8e8', '#f9e8e8', '#f9d4d4', '#f9bdbd', '#f8a8a8', '#f68f8f', '#f68f8f', '#f47676', '#f45f5f', '#f45f5f', '#f34343', '#f34343', '#f33b3b', '#f33b3b', '#f33b3b', '#f42e2e', '#f42e2e']\n",
        "      value = int((value * 100) / 5)\n",
        "      return colors[value]\n",
        "\n",
        "def visualization(attention_weights, outputs, index):\n",
        "    attention_weights = attention_weights[:-1]\n",
        "\n",
        "    attention_weights = [attention_weights[i][:len(test_english_words[index])] for i in range(len(attention_weights))]\n",
        "    attention_weights = np.asarray(attention_weights)\n",
        "    english_weights = np.identity(max(attention_weights.shape[0], attention_weights.shape[1])) * np.max([np.max(attention_weights,axis=1)])\n",
        "\n",
        "    for i in range(len(attention_weights)):\n",
        "        text_english = []\n",
        "        text_telugu = []\n",
        "        for j in range(attention_weights[i].shape[0]):\n",
        "            text_english.append((test_english_words[index][j], get_clr(attention_weights[i][j])))\n",
        "          \n",
        "        for j in range(len(attention_weights)):\n",
        "            text_telugu.append((outputs[j], get_clr(english_weights[i][j])))\n",
        "        print_color(text_telugu)\n",
        "        print_color(text_english)"
      ],
      "metadata": {
        "id": "mQSgSasV22iu"
      },
      "id": "mQSgSasV22iu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-HnRi2f722tJ"
      },
      "id": "-HnRi2f722tJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_heatmap(input_words, prediction_words, attention_weights):\n",
        "    figures = []\n",
        "    figures , axs = plt.subplots(3,3)\n",
        "    figures.set_size_inches(23, 15)\n",
        "    rows,cols = -1,0\n",
        "    for i in range(9):\n",
        "\n",
        "        ylabel = [\"\"] + [x for x in prediction_words[i]]\n",
        "        xlabel = [\"\"] +  [char for char in input_words[i]]\n",
        "        \n",
        "        for j in range(len(attention_weights[i])):\n",
        "            attention_weights[i][j] = attention_weights[i][j][1:len(xlabel)]\n",
        "            \n",
        "        attention_weights[i] = attention_weights[i][:-1]\n",
        "        if i%3 == 0:\n",
        "            rows+=1\n",
        "            cols=0\n",
        "            \n",
        "        _ = axs[rows][cols].matshow(np.array(attention_weights[i]), cmap=plt.cm.gray_r)\n",
        "        axs[rows][cols].set_xticklabels(xlabel)\n",
        "        axs[rows][cols].set_yticklabels(ylabel, fontproperties=FontProperties(fname = \"/content/drive/MyDrive/fonts/nirmala.ttf\"))\n",
        "        cols+=1\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "DMWgbduP28wA"
      },
      "id": "DMWgbduP28wA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(best_sweep_config, entity=\"cs21m010-cs21m041\", project=\"DL_Assignment_3_b\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lpqa3h7AGHR4",
        "outputId": "a3d9053f-8f54-46a4-c18c-81dfafc6f12a"
      },
      "id": "Lpqa3h7AGHR4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 460y1xof\n",
            "Sweep URL: https://wandb.ai/cs21m010-cs21m041/DL_Assignment_3_b/sweeps/460y1xof\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Consider Best configuration\n",
        "def consider_best_model():\n",
        "    \n",
        "    # Create a new WandB run\n",
        "    wandb.init(config=best_sweep_config)\n",
        "    \n",
        "    # Construct the run name\n",
        "    config = wandb.config\n",
        "    wandb.run.name = 'ATT_Best_' + config.cell_type + '' + str(config.input_embedding_size) + '_hs' + str(config.hidden_layer_size) + 'bs' + str(config.batch_size)\n",
        "    \n",
        "    # Taking Best configuration\n",
        "    print('training...')\n",
        "    best_model_obj = CustomRNN(config.input_embedding_size, config.cell_type , config.hidden_layer_size, \n",
        "              config.num_layers, config.num_layers, config.dropout, \n",
        "              config.batch_size, config.epochs, is_test_model=True)\n",
        "    model, encoder_layers, decoder_layers = best_model_obj.training()\n",
        "\n",
        "    print('inferrencing...')\n",
        "    encoder_model, decoder_model = best_model_obj.inference_model(model, encoder_layers, decoder_layers)\n",
        "\n",
        "    print('calculating test accuracy and decoding test inputs...')\n",
        "    best_model_obj.test_and_calculate_accuraccy(encoder_model, decoder_model, test_english_words, \n",
        "                                              test_telugu_words, encoder_test_input_data, is_val_accuracy=False)\n",
        "    \n",
        "    # for visualisation each ouput word character and corresponding attention input word character\n",
        "    index = 700\n",
        "    input = encoder_test_input_data[700:701]\n",
        "    outputs, attn_weights = best_model_obj.decode_sequence(input, encoder_model, decoder_model)\n",
        "    visualization(attn_weights, outputs, 700)\n",
        "\n",
        "    # for attention heatmaps\n",
        "    indexes = [700, 994, 1070, 1111, 1206, 1441, 1691, 1977, 2535]\n",
        "    inputs = []\n",
        "    predictions = []\n",
        "    attentions = []\n",
        "\n",
        "    for i in indexes:\n",
        "        input = encoder_test_input_data[i:i+1]\n",
        "        inputs.append(test_english_words[i].strip())\n",
        "        output, attention_weights = best_model_obj.decode_sequence(input, encoder_model, decoder_model)\n",
        "        predictions.append(output.strip())\n",
        "        attentions.append(attention_weights)\n",
        "\n",
        "    visualize_heatmap(inputs, predictions, attentions)\n"
      ],
      "metadata": {
        "id": "MamcGqFxL9j8"
      },
      "id": "MamcGqFxL9j8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, consider_best_model, count=1)"
      ],
      "metadata": {
        "id": "nUEGjhqeG3Y0"
      },
      "id": "nUEGjhqeG3Y0",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "assignment_3_part_b.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}