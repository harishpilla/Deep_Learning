{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a56e4a2e",
      "metadata": {
        "id": "a56e4a2e"
      },
      "outputs": [],
      "source": [
        "# Import the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "import random\n",
        "from math import log\n",
        "\n",
        "import tensorflow \n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import LSTM, SimpleRNN, GRU, Embedding, Dense, TimeDistributed, Concatenate, AdditiveAttention "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install wandb and wandbcallback libraries\n",
        "%pip install wandb -q\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvElZMAcEqdg",
        "outputId": "1e52f710-4c13-4cab-b8df-a6dc9adc1097"
      },
      "id": "LvElZMAcEqdg",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8 MB 13.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 49.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 6.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "if not os.path.exists('/content/dakshina_dataset_v1.0.tar'):\n",
        "    !wget \"https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67pDt3xkEr06",
        "outputId": "75807547-fc79-408c-e329-7daaaa8d8c9c"
      },
      "id": "67pDt3xkEr06",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-05 16:44:25--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 66.102.1.128, 142.251.5.128, 64.233.167.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|66.102.1.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G   120MB/s    in 16s     \n",
            "\n",
            "2022-05-05 16:44:42 (117 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip every file\n",
        "!tar -xvf /content/dakshina_dataset_v1.0.tar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPvCnsn6Etg3",
        "outputId": "d7eb1431-5a44-416e-9c6e-4e9fdba877f4"
      },
      "id": "ZPvCnsn6Etg3",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dakshina_dataset_v1.0/bn/\n",
            "dakshina_dataset_v1.0/bn/lexicons/\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/romanized/\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/\n",
            "dakshina_dataset_v1.0/gu/lexicons/\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/romanized/\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/\n",
            "dakshina_dataset_v1.0/hi/lexicons/\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/romanized/\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/\n",
            "dakshina_dataset_v1.0/kn/lexicons/\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/kn/romanized/\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/\n",
            "dakshina_dataset_v1.0/ml/lexicons/\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.rom.txt\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/romanized/\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/\n",
            "dakshina_dataset_v1.0/mr/lexicons/\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/romanized/\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/\n",
            "dakshina_dataset_v1.0/pa/lexicons/\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/pa/romanized/\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/\n",
            "dakshina_dataset_v1.0/sd/lexicons/\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/sd/romanized/\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/\n",
            "dakshina_dataset_v1.0/si/lexicons/\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/romanized/\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/\n",
            "dakshina_dataset_v1.0/ta/lexicons/\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/romanized/\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/\n",
            "dakshina_dataset_v1.0/te/lexicons/\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/romanized/\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ur/\n",
            "dakshina_dataset_v1.0/ur/lexicons/\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/romanized/\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training dataset file\n",
        "train_file = open(\"/content/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\")\n",
        "read_train_file = csv.reader(train_file, delimiter=\"\\t\")\n",
        "\n",
        "# Load the validation dataset file\n",
        "val_file = open(\"/content/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\")\n",
        "read_val_file = csv.reader(val_file, delimiter=\"\\t\")\n",
        "\n",
        "# Load the test dataset file\n",
        "test_file = open(\"/content/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\")\n",
        "read_test_file = csv.reader(test_file, delimiter=\"\\t\")\n",
        "\n",
        "# Path to save the predictions file\n",
        "predictions_path = 'predictions.tsv'"
      ],
      "metadata": {
        "id": "QyyeiUwvEvS7"
      },
      "id": "QyyeiUwvEvS7",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess and Encode Data"
      ],
      "metadata": {
        "id": "ZckeDDQeH8Bk"
      },
      "id": "ZckeDDQeH8Bk"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all the input words(English) and target words(Telugu) in Training dataset\n",
        "telugu_words = []\n",
        "english_words = []\n",
        "\n",
        "for i in read_train_file:   \n",
        "    telugu_words.append(\"\\t\" + str(i[0]) + \"\\n\")\n",
        "    english_words.append(str(i[1]))\n",
        "\n",
        "# Get all the input words(English) and target words(Telugu) in Validation dataset\n",
        "val_telugu_words = []\n",
        "val_english_words = []\n",
        "\n",
        "for i in read_val_file:\n",
        "    val_telugu_words.append(\"\\t\" + str(i[0]) + \"\\n\")\n",
        "    val_english_words.append(str(i[1]))\n",
        "\n",
        "\n",
        "# Get all the input words(English) and target words(Telugu) in Test dataset\n",
        "test_telugu_words = []\n",
        "test_english_words = []\n",
        "\n",
        "for i in read_test_file:\n",
        "    test_telugu_words.append(\"\\t\" + str(i[0]) + \"\\n\")\n",
        "    test_english_words.append(str(i[1]))\n"
      ],
      "metadata": {
        "id": "uVFcA9-RE1Qx"
      },
      "id": "uVFcA9-RE1Qx",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the total list of input words(English)\n",
        "total_input_words = english_words + val_english_words + test_english_words\n",
        "\n",
        "# Get the total list of target words(Telugu)\n",
        "total_output_words = telugu_words + val_telugu_words + test_telugu_words"
      ],
      "metadata": {
        "id": "TxFzi3gtE2uJ"
      },
      "id": "TxFzi3gtE2uJ",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the total input and target language characters \n",
        "\n",
        "english_characters = set()\n",
        "english_characters.add(\" \")\n",
        "telugu_characters = set()\n",
        "telugu_characters.add(\" \")\n",
        "\n",
        "for word in total_input_words:\n",
        "    for char in word:\n",
        "        if char not in english_characters:\n",
        "            english_characters.add(char)\n",
        "\n",
        "for word in total_output_words:\n",
        "    for char in word:\n",
        "        if char not in telugu_characters:\n",
        "            telugu_characters.add(char)"
      ],
      "metadata": {
        "id": "iINDSGHJE4b2"
      },
      "id": "iINDSGHJE4b2",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the list of input characters(English) and target characters(Telugu)\n",
        "english_characters = sorted(list(english_characters))\n",
        "telugu_characters = sorted(list(telugu_characters))\n",
        "\n",
        "# Get total number of characters in Input \n",
        "num_encoder_tokens = len(english_characters)\n",
        "# Get total number of characters in Output \n",
        "num_decoder_tokens = len(telugu_characters)\n",
        "\n",
        "# Get maximum length of the word in total input words\n",
        "max_encoder_seq_length = max([len(text) for text in total_input_words])\n",
        "# Get maximum length of the word in total target words\n",
        "max_decoder_seq_length = max([len(text) for text in total_output_words])\n",
        "\n",
        "print(\"Summary of the dataset :\")\n",
        "print(\"Number of train samples :\" , len(english_words))\n",
        "print(\"Number of val samples :\" , len(val_english_words))\n",
        "print(\"Number of test samples :\" , len(test_english_words))\n",
        "print(\"Number of unique input tokens :\" , num_encoder_tokens)\n",
        "print(\"Number of unique output tokens :\" , num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\" , max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\" , max_decoder_seq_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myI3PSgCE56-",
        "outputId": "c820e8d4-01ae-4bb6-f43d-066789d30f7b"
      },
      "id": "myI3PSgCE56-",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary of the dataset :\n",
            "Number of train samples : 58550\n",
            "Number of val samples : 5683\n",
            "Number of test samples : 5747\n",
            "Number of unique input tokens : 27\n",
            "Number of unique output tokens : 66\n",
            "Max sequence length for inputs: 25\n",
            "Max sequence length for outputs: 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary for every english character with an index associated to it\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(english_characters)])\n",
        "# Dictionary for every telugu character with an index associated to it\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(telugu_characters)])\n",
        "\n",
        "# Dictionary for every english character with an index associated to it\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "# Dictionary for every english character with an index associated to it\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "# Preparing train encoder and decoder inputs and decoder target\n",
        "encoder_input_data = np.zeros((len(english_words), max_encoder_seq_length), dtype=\"float32\")\n",
        "decoder_input_data = np.zeros((len(english_words), max_decoder_seq_length), dtype=\"float32\")\n",
        "decoder_target_data = np.zeros((len(english_words), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "# Writing the encoder , decoder characters with the indexes in input and target token index \n",
        "for i, (english, telugu) in enumerate(zip(english_words, telugu_words)):\n",
        "    for t, char in enumerate(english):\n",
        "        encoder_input_data[i, t] = input_token_index[char]\n",
        "    \n",
        "    for t, char in enumerate(telugu):\n",
        "        decoder_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            # One got encoding the decoder_target_data  and decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0    \n",
        "    decoder_input_data[i, t+1:] = target_token_index[' ']\n",
        "    decoder_target_data[i, t :, target_token_index[' ']] = 1.0"
      ],
      "metadata": {
        "id": "0Td6oWd5E7BS"
      },
      "id": "0Td6oWd5E7BS",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing validation encoder and decoder inputs\n",
        "\n",
        "encoder_val_input_data = np.zeros((len(val_english_words), max_encoder_seq_length), dtype=\"float32\")\n",
        "decoder_val_input_data = np.zeros((len(val_english_words), max_decoder_seq_length), dtype=\"float32\")\n",
        "decoder_val_target_data = np.zeros((len(val_english_words), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "for i, (english, telugu) in enumerate(zip(val_english_words, val_telugu_words)):\n",
        "    for t, char in enumerate(english):\n",
        "        encoder_val_input_data[i, t] = input_token_index[char]\n",
        "  \n",
        "    for t, char in enumerate(telugu):\n",
        "        decoder_val_input_data[i, t] =  target_token_index[char]\n",
        "        if t > 0:\n",
        "            # One got encoding the validation decoder target data  and validation decoder target data will be ahead by one timestep and will not include the start character.\n",
        "            decoder_val_target_data[i, t - 1, target_token_index[char]] = 1.0   \n",
        "    decoder_val_input_data[i, t+1:] = target_token_index[' ']\n",
        "    decoder_val_target_data[i, t :, target_token_index[' ']] = 1.0"
      ],
      "metadata": {
        "id": "ieQIs1mHE9IB"
      },
      "id": "ieQIs1mHE9IB",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the validation telugu and english words to nupy arrays\n",
        "val_telugu_words = np.array(val_telugu_words)\n",
        "val_english_words = np.array(val_english_words)"
      ],
      "metadata": {
        "id": "JGtZsL7mE-u7"
      },
      "id": "JGtZsL7mE-u7",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Build Model"
      ],
      "metadata": {
        "id": "_7Wfd-zjIDZ8"
      },
      "id": "_7Wfd-zjIDZ8"
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomRNN(object):\n",
        "    \n",
        "    def __init__(self, input_embedding_size, cell_type='GRU', hidden_layer_size=32,\n",
        "                 num_encoder_layers=2, num_decoder_layers = 2, dropout=0.1, \n",
        "                 batch_size=32, epochs=25):\n",
        "\n",
        "        self.cell_type = cell_type\n",
        "        self.input_embedding_size = input_embedding_size\n",
        "        self.cell_type = cell_type\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "        self.num_encoder_layers = num_encoder_layers\n",
        "        self.num_decoder_layers = num_decoder_layers\n",
        "        self.dropout = dropout\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "\n",
        "\n",
        "    def training(self):\n",
        "        \"\"\"\n",
        "        Method to train the model using LSTM or GRU or RNN and\n",
        "        return model along with encoder_layers and decoder_layers.\n",
        "        \"\"\"\n",
        "\n",
        "        # Choose customFunction based on cell_type\n",
        "        customFunction = None\n",
        "        if self.cell_type == 'LSTM':\n",
        "            customFunction = LSTM\n",
        "        elif self.cell_type == 'GRU':\n",
        "            customFunction = GRU\n",
        "        elif self.cell_type == 'RNN':\n",
        "            customFunction = SimpleRNN\n",
        "\n",
        "        # Encoders\n",
        "        encoder_inputs = Input(shape=(None,))\n",
        "        encoder_embedding = Embedding(num_encoder_tokens,self.input_embedding_size, input_length = max_encoder_seq_length)(encoder_inputs)\n",
        "        \n",
        "        # get encoder_layers and encoder_states\n",
        "        encoder_layers = []\n",
        "        encoder_states = []    \n",
        "        if self.cell_type == 'LSTM':\n",
        "            encoder = customFunction(self.hidden_layer_size, return_sequences=True, return_state=True, dropout = self.dropout)\n",
        "            encoder_layers.append(encoder)\n",
        "            encoder_outputs, state_h, state_c = encoder(encoder_embedding)\n",
        "            encoder_states.append([state_h, state_c])\n",
        "\n",
        "            for i in range(1,self.num_encoder_layers):\n",
        "                encoder = customFunction(self.hidden_layer_size,return_sequences=True,return_state=True, dropout = self.dropout) \n",
        "                encoder_layers.append(encoder)\n",
        "                encoder_outputs, state_h, state_c = encoder(encoder_outputs)\n",
        "                encoder_states.append([state_h, state_c])\n",
        "        else:\n",
        "            encoder = customFunction(self.hidden_layer_size, return_sequences=True, return_state=True, dropout = self.dropout)\n",
        "            encoder_layers.append(encoder)\n",
        "            encoder_outputs, state_h = encoder(encoder_embedding)\n",
        "            encoder_states.append([state_h])\n",
        "\n",
        "            for i in range(1,self.num_encoder_layers):\n",
        "                encoder = customFunction(self.hidden_layer_size,return_sequences=True,return_state=True, dropout = self.dropout) \n",
        "                encoder_layers.append(encoder)\n",
        "                encoder_outputs, state_h = encoder(encoder_outputs)\n",
        "                encoder_states.append([state_h])\n",
        "        \n",
        "        # Decoders\n",
        "        decoder_inputs = Input(shape=(None,))\n",
        "        decoder_embedding = Embedding(num_decoder_tokens,self.input_embedding_size, input_length = max_decoder_seq_length)(decoder_inputs)\n",
        "\n",
        "        # get decoder_layers\n",
        "        decoder_layers = []\n",
        "        if self.cell_type == 'LSTM':\n",
        "            decoder_lstm = customFunction(self.hidden_layer_size, return_sequences=True, return_state=True, dropout = self.dropout)\n",
        "            decoder_layers.append(decoder_lstm)\n",
        "            decoder_outputs, _ , _ = decoder_lstm(decoder_embedding, initial_state=encoder_states[0])\n",
        "            for i in range(1,self.num_decoder_layers):\n",
        "                decoder_lstm = customFunction(self.hidden_layer_size, return_sequences=True, return_state=True, dropout = self.dropout)\n",
        "                decoder_layers.append(decoder_lstm)\n",
        "                decoder_outputs, _ , _  = decoder_lstm(decoder_outputs, initial_state=encoder_states[i])\n",
        "        else:\n",
        "            decoder_GRU = customFunction(self.hidden_layer_size, return_sequences=True, return_state=True, dropout = self.dropout)\n",
        "            decoder_layers.append(decoder_GRU)\n",
        "            decoder_outputs, _ = decoder_GRU(decoder_embedding, initial_state=encoder_states[0])\n",
        "            for i in range(1,self.num_decoder_layers):\n",
        "                decoder_GRU = customFunction(self.hidden_layer_size, return_sequences=True, return_state=True, dropout = self.dropout)\n",
        "                decoder_layers.append(decoder_GRU)\n",
        "                decoder_outputs, _  = decoder_GRU(decoder_outputs, initial_state=encoder_states[i])\n",
        "\n",
        "        # Add decoder attention layer\n",
        "        decoder_attention = AdditiveAttention(name=\"decoder_attention\")\n",
        "        decoder_concat = Concatenate(name=\"decoder_concat\")\n",
        "        context_vector, _ = decoder_attention([decoder_outputs, encoder_outputs], return_attention_scores=True)\n",
        "        decoder_outputs = decoder_concat([decoder_outputs, context_vector])\n",
        "        \n",
        "        # Add dense layer to decoder\n",
        "        decoder_dense = Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "        decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "        # Create a model, compile and fit the model using the optimizer.\n",
        "        model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "        model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "        model.fit(\n",
        "            [encoder_input_data, decoder_input_data],\n",
        "            decoder_target_data,\n",
        "            batch_size=self.batch_size,\n",
        "            epochs=self.epochs,\n",
        "            callbacks=[WandbCallback()]\n",
        "        )\n",
        "\n",
        "        return model, encoder_layers, decoder_layers\n",
        "\n",
        "\n",
        "    def inference_model(self, model, encoder_layers, decoder_layers):\n",
        "        \"\"\"\n",
        "        Create the encoder_model, decoder_model and return using the\n",
        "        arguments model, encoder_layers and decoder_layers\n",
        "        \"\"\"\n",
        "        \n",
        "        # generate encoder_model \n",
        "        encoder_inputs = model.input[0]  # input_1\n",
        "        encoder_states = []\n",
        "        enc_emb = model.layers[2]     # embedding 1\n",
        "        encoder_outputs = enc_emb(encoder_inputs)\n",
        "\n",
        "        if self.cell_type == 'RNN' or self.cell_type ==\"GRU\":\n",
        "            for i in range(self.num_encoder_layers):\n",
        "                encoder_outputs, state_h_enc = encoder_layers[i](encoder_outputs)\n",
        "                encoder_states += [state_h_enc] \n",
        "        else:\n",
        "            for i in range(self.num_encoder_layers):\n",
        "                encoder_outputs, state_h_enc, state_c_enc = encoder_layers[i](encoder_outputs)\n",
        "                encoder_states += [state_h_enc, state_c_enc]   \n",
        "\n",
        "        encoder_model = Model(encoder_inputs, encoder_states + [encoder_outputs])\n",
        "\n",
        "\n",
        "        # generate decoder_model\n",
        "        input_names = [[\"input_100\",\"input_101\"],[\"input_102\",\"input_103\"],[\"input_104\",\"input_105\"],\"input_106\"]\n",
        "\n",
        "        decoder_inputs = model.input[1]       # input_2\n",
        "        decoder_embedding = model.layers[3]   # embedding 2\n",
        "        decoder_outputs = decoder_embedding(decoder_inputs)\n",
        "        decoder_states = []\n",
        "        decoder_states_inputs = []\n",
        "        \n",
        "        if self.cell_type == 'RNN' or self.cell_type ==\"GRU\":\n",
        "            for i in range(self.num_decoder_layers):\n",
        "                decoder_states_inputs += [Input(shape=(self.hidden_layer_size,), name=input_names[i][0])]\n",
        "            for i in range(self.num_decoder_layers):\n",
        "                decoder_outputs, state_h_dec = decoder_layers[i](decoder_outputs, initial_state=decoder_states_inputs[i])\n",
        "                decoder_states += [state_h_dec]\n",
        "        else:\n",
        "            for i in range(self.num_decoder_layers):\n",
        "                decoder_states_inputs += [Input(shape=(self.hidden_layer_size,), name=input_names[i][0]), Input(shape=(self.hidden_layer_size,), name=input_names[i][1])]\n",
        "            j = 0\n",
        "            for i in range(self.num_decoder_layers):\n",
        "                decoder_outputs, state_h_dec, state_c_dec = decoder_layers[i](decoder_outputs, initial_state=decoder_states_inputs[i+j:i+j+2])\n",
        "                decoder_states += [state_h_dec , state_c_dec]\n",
        "                j += 1\n",
        "\n",
        "        attention_layer = model.layers[4+2*self.num_encoder_layers]\n",
        "        attention_input = Input(shape=(max_encoder_seq_length,self.hidden_layer_size), name=input_names[-1])   \n",
        "        context_vec, attention_weights = attention_layer([decoder_outputs, attention_input], return_attention_scores=True)\n",
        "        \n",
        "        concat_layer = model.layers[5+2*self.num_encoder_layers]\n",
        "        decoder_outputs = concat_layer([decoder_outputs, context_vec])\n",
        "\n",
        "        decoder_dense = model.layers[6+2*self.num_encoder_layers]\n",
        "        decoder_outputs = decoder_dense(decoder_outputs)\n",
        "        decoder_model = Model([decoder_inputs] + decoder_states_inputs + [attention_input], [decoder_outputs] + decoder_states + [attention_weights])\n",
        "\n",
        "        return encoder_model, decoder_model\n",
        "\n",
        "\n",
        "    def decode_sequence(self, input_seq, encoder_model, decoder_model):\n",
        "        \"\"\"\n",
        "        Method to decode an input sequence using the encoder_model\n",
        "        and decoder_model\n",
        "        \"\"\"\n",
        "        states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "        # get attention input\n",
        "        attention_input = states_value[-1]\n",
        "        states_value = states_value[:-1]\n",
        "        target_seq = np.zeros((1, 1)) \n",
        "        target_seq[0, 0] = target_token_index[\"\\t\"]\n",
        "        attention_weights = []\n",
        "\n",
        "        # continue decoding input sequence till '\\n' is found or the sequence\n",
        "        # exceeds the maximum decoder sequence length.\n",
        "        stop_condition = False\n",
        "        decoded_sentence = \"\"\n",
        "        while not stop_condition:\n",
        "            output_tokens = decoder_model.predict([target_seq] + states_value + [attention_input])\n",
        "            sampled_token_index = np.argmax(output_tokens[0][0, -1, :])\n",
        "            sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "            decoded_sentence += sampled_char\n",
        "            if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "                stop_condition = True\n",
        "\n",
        "            target_seq = np.zeros((1, 1))\n",
        "            target_seq[0, 0] = sampled_token_index\n",
        "            states_value = output_tokens[1:-1]\n",
        "            attention_weights.append(output_tokens[-1][0][0])\n",
        "            \n",
        "        return decoded_sentence, attention_weights\n",
        "\n",
        "    \n",
        "    def test_and_calculate_accuraccy(self, encoder_model, decoder_model):\n",
        "        \"\"\"\n",
        "        Calculate accuracy using the data and write it to prediction file.\n",
        "        \"\"\"\n",
        "        correct = 0\n",
        "        n = val_telugu_words.shape[0]\n",
        "        for i in range(n):\n",
        "          input = encoder_val_input_data[i:i+1]\n",
        "          output, _ = self.decode_sequence(input,encoder_model, decoder_model)\n",
        "          with open(predictions_path, 'a') as f:\n",
        "                    f.write('{} , {}\\n'.format(output.strip(), val_telugu_words[i].strip()))\n",
        "          if output.strip() == val_telugu_words[i].strip():\n",
        "              correct += 1\n",
        "        # log accuracy to wandb\n",
        "        wandb.log({'val_accuracy' : correct*100/n})\n",
        "        print('val_accuracy', correct*100/n)\n"
      ],
      "metadata": {
        "id": "7DHTqx6sFBvQ"
      },
      "id": "7DHTqx6sFBvQ",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sweep config"
      ],
      "metadata": {
        "id": "UPwcSFfOISi7"
      },
      "id": "UPwcSFfOISi7"
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'metric':\n",
        "    {'goal': 'maximize',\n",
        "     'name': 'val_accuracy'\n",
        "     },\n",
        "    'parameters': {\n",
        "        'input_embedding_size':\n",
        "        {\n",
        "            'values': [128, 256, 512]\n",
        "        },\n",
        "        'hidden_layer_size':\n",
        "        {\n",
        "            'values': [128, 256, 512]\n",
        "        },\n",
        "        'cell_type':\n",
        "        {\n",
        "            'values': ['LSTM', 'RNN', 'GRU']\n",
        "        },\n",
        "        'num_layers':\n",
        "        {'values': [1, 2, 3]\n",
        "         },\n",
        "        'batch_size':\n",
        "        {\n",
        "            'values': [128, 256, 512]\n",
        "        },\n",
        "        'dropout':\n",
        "        {\n",
        "            'values': [0.1]\n",
        "        },\n",
        "        'epochs':\n",
        "        {\n",
        "            'values': [1]\n",
        "        }\n",
        "    }}\n"
      ],
      "metadata": {
        "id": "vizw0Nx5GQJY"
      },
      "id": "vizw0Nx5GQJY",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, entity=\"cs21m010-cs21m041\", project=\"DL_Assignment_3_b\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f424NbphGTFI",
        "outputId": "d69e02b6-ef2d-4318-90d2-4bf8c843ce90"
      },
      "id": "f424NbphGTFI",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 3rbz2280\n",
            "Sweep URL: https://wandb.ai/cs21m010-cs21m041/DL_Assignment_3_b/sweeps/3rbz2280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "\n",
        "    # Create a new WandB run\n",
        "    wandb.init(config=sweep_config)\n",
        "    \n",
        "    # Construct the run name\n",
        "    config = wandb.config\n",
        "    wandb.run.name = 'ATT_' + config.cell_type + '' + str(config.input_embedding_size) + '_hs' + str(config.hidden_layer_size) + 'bs' + str(config.batch_size)\n",
        "    \n",
        "    model_rnn_obj = CustomRNN(config.input_embedding_size, config.cell_type , config.hidden_layer_size, config.num_layers, config.num_layers, config.dropout, config.batch_size, config.epochs)\n",
        "    model, encoder_layers, decoder_layers = model_rnn_obj.training()\n",
        "    \n",
        "    print('training...')\n",
        "\n",
        "    encoder_model, decoder_model = model_rnn_obj.inference_model(model, encoder_layers, decoder_layers)\n",
        "    \n",
        "    print('inferrencing...')\n",
        "\n",
        "    model_rnn_obj.test_and_calculate_accuraccy(encoder_model,decoder_model)"
      ],
      "metadata": {
        "id": "lZ82zKS5GU_2"
      },
      "id": "lZ82zKS5GU_2",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, train, count = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "Dnw09xJ3GZtV",
        "outputId": "5c265243-e9e7-49d3-f452-38f974ddfcd3"
      },
      "id": "Dnw09xJ3GZtV",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ra8ymdni with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_size: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_embedding_size: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220505_170626-ra8ymdni</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/cs21m010-cs21m041/DL_Assignment_3_b/runs/ra8ymdni\" target=\"_blank\">pretty-sweep-1</a></strong> to <a href=\"https://wandb.ai/cs21m010-cs21m041/DL_Assignment_3_b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/cs21m010-cs21m041/DL_Assignment_3_b/sweeps/3rbz2280\" target=\"_blank\">https://wandb.ai/cs21m010-cs21m041/DL_Assignment_3_b/sweeps/3rbz2280</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "458/458 [==============================] - 55s 116ms/step - loss: 1.0361 - accuracy: 0.7194 - _timestamp: 1651770448.0000 - _runtime: 62.0000\n",
            "training...\n",
            "inferrencing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MamcGqFxL9j8"
      },
      "id": "MamcGqFxL9j8",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "assignment_3_part_b.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}