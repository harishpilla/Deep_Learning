{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "e45c6f8d",
      "metadata": {
        "id": "e45c6f8d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import tensorflow \n",
        "import keras\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, GRU, Dropout, SimpleRNN\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from math import log"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install wandb -q\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "metadata": {
        "id": "mj8H54q04eta"
      },
      "id": "mj8H54q04eta",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "if not os.path.exists('/content/dakshina_dataset_v1.0.tar'):\n",
        "    !wget \"https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\""
      ],
      "metadata": {
        "id": "3RneRWbD3oCj"
      },
      "id": "3RneRWbD3oCj",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvf /content/dakshina_dataset_v1.0.tar"
      ],
      "metadata": {
        "id": "mDdPDwCy36Dg",
        "outputId": "8382ee7e-446b-4f69-b6f9-01c648ab31d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mDdPDwCy36Dg",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dakshina_dataset_v1.0/bn/\n",
            "dakshina_dataset_v1.0/bn/lexicons/\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/romanized/\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/\n",
            "dakshina_dataset_v1.0/gu/lexicons/\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/romanized/\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/\n",
            "dakshina_dataset_v1.0/hi/lexicons/\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/romanized/\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/\n",
            "dakshina_dataset_v1.0/kn/lexicons/\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/kn/romanized/\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/\n",
            "dakshina_dataset_v1.0/ml/lexicons/\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.rom.txt\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/romanized/\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/\n",
            "dakshina_dataset_v1.0/mr/lexicons/\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/romanized/\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/\n",
            "dakshina_dataset_v1.0/pa/lexicons/\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/pa/romanized/\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/\n",
            "dakshina_dataset_v1.0/sd/lexicons/\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/sd/romanized/\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/\n",
            "dakshina_dataset_v1.0/si/lexicons/\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/romanized/\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/\n",
            "dakshina_dataset_v1.0/ta/lexicons/\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/romanized/\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/\n",
            "dakshina_dataset_v1.0/te/lexicons/\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/romanized/\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ur/\n",
            "dakshina_dataset_v1.0/ur/lexicons/\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/romanized/\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_path = '/content/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv'\n",
        "validation_data_path = '/content/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv'\n",
        "test_data_path = '/content/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv'\n",
        "\n",
        "df_train = pd.read_csv(train_data_path, sep='\\t', header=None)\n",
        "df_val = pd.read_csv(validation_data_path, sep='\\t', header=None)\n",
        "df_test = pd.read_csv(test_data_path, sep='\\t', header=None)"
      ],
      "metadata": {
        "id": "pmx3VVhpURl_"
      },
      "id": "pmx3VVhpURl_",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_path = '/content/predictions.tsv'"
      ],
      "metadata": {
        "id": "8RhaIQ1o5OlO"
      },
      "id": "8RhaIQ1o5OlO",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(df, input_texts, target_texts, input_characters=None, target_characters=None, is_test_data=False):\n",
        "    \n",
        "    for  _, row in df.iterrows():\n",
        "        input_text, target_text = str(row[0]), str(row[1])\n",
        "        input_texts.append(input_text)\n",
        "        target_text = '\\t' + target_text + '\\n'\n",
        "        target_texts.append(target_text)\n",
        "        \n",
        "        if not is_test_data:\n",
        "            for char in input_text:\n",
        "                if char not in input_characters:\n",
        "                    input_characters.add(char)\n",
        "\n",
        "            for char in target_text:\n",
        "                if char not in target_characters:\n",
        "                    target_characters.add(char)\n",
        "\n",
        "    max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "    max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "    if not is_test_data:\n",
        "        input_characters = sorted(list(input_characters))\n",
        "        target_characters = sorted(list(target_characters))\n",
        "        num_encoder_tokens = len(input_characters)\n",
        "        num_decoder_tokens = len(target_characters)\n",
        "\n",
        "        return max_encoder_seq_length, max_decoder_seq_length, num_encoder_tokens, num_decoder_tokens\n",
        "    \n",
        "    return max_encoder_seq_length, max_decoder_seq_length"
      ],
      "metadata": {
        "id": "dBL7HWehFkBn"
      },
      "id": "dBL7HWehFkBn",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vectors(input_texts, target_texts, input_token_index, target_token_index,\n",
        "                max_encoder_seq_length, num_encoder_tokens, \n",
        "                max_decoder_seq_length=None, num_decoder_tokens=None,\n",
        "                is_test_data=False):\n",
        "      \n",
        "    encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length), dtype=\"float32\")\n",
        "\n",
        "    if not is_test_data:\n",
        "        decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length), dtype=\"float32\")\n",
        "        decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "        for t, char in enumerate(input_text):\n",
        "            encoder_input_data[i, t] = input_token_index[char]\n",
        "        encoder_input_data[i, t + 1 :] = input_token_index[\" \"]\n",
        "\n",
        "\n",
        "        if not is_test_data:\n",
        "            for t, char in enumerate(target_text):\n",
        "                decoder_input_data[i, t] = target_token_index[char]\n",
        "                if t > 0:\n",
        "                    # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "                    # decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "                    decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "            decoder_input_data[i, t + 1: ] = target_token_index[\" \"]\n",
        "            decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0    \n",
        "\n",
        "    if is_test_data:\n",
        "        return encoder_input_data\n",
        "\n",
        "    return encoder_input_data, decoder_input_data, decoder_target_data"
      ],
      "metadata": {
        "id": "58JABACiLR7G"
      },
      "id": "58JABACiLR7G",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Train, Validation and Test data"
      ],
      "metadata": {
        "id": "qtrqWffEJace"
      },
      "id": "qtrqWffEJace"
    },
    {
      "cell_type": "code",
      "source": [
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "input_characters = set(' ')\n",
        "target_characters = set(' ')\n",
        "\n",
        "max_encoder_seq_length, max_decoder_seq_length, num_encoder_tokens, num_decoder_tokens = load_data(df_train, input_texts, target_texts, input_characters, target_characters)\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)"
      ],
      "metadata": {
        "id": "XwhSrjplHqoc",
        "outputId": "088bf974-db57-4e1d-87a0-7f9d4894c131",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "XwhSrjplHqoc",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 58550\n",
            "Number of unique input tokens: 64\n",
            "Number of unique output tokens: 29\n",
            "Max sequence length for inputs: 20\n",
            "Max sequence length for outputs: 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_input_texts = []\n",
        "val_target_texts = []\n",
        "\n",
        "val_max_encoder_seq_length, val_max_decoder_seq_length, val_num_encoder_tokens, val_num_decoder_tokens = load_data(\n",
        "    df_val, val_input_texts, val_target_texts, input_characters, target_characters)\n",
        "\n",
        "print(\"Number of samples:\", len(val_input_texts))\n",
        "print(\"Number of unique input tokens:\", val_num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", val_num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", val_max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", val_max_decoder_seq_length)"
      ],
      "metadata": {
        "id": "I97sHjSyIjba",
        "outputId": "7dcc6706-0762-4ecc-cb2d-eb4f2bbedc71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "I97sHjSyIjba",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 5683\n",
            "Number of unique input tokens: 64\n",
            "Number of unique output tokens: 29\n",
            "Max sequence length for inputs: 19\n",
            "Max sequence length for outputs: 23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_texts = []\n",
        "test_target_texts = []\n",
        "\n",
        "test_max_encoder_seq_length, test_max_decoder_seq_length = load_data(df_test, test_input_texts, test_target_texts, is_test_data=True)\n",
        "\n",
        "print(\"Number of Test samples:\", len(test_input_texts))\n",
        "print(\"Test Max sequence length for inputs:\", test_max_encoder_seq_length)\n",
        "print(\"Test Max sequence length for outputs:\", test_max_decoder_seq_length)"
      ],
      "metadata": {
        "id": "FI3C6mc2Jnk8",
        "outputId": "ecc74763-779d-477a-c852-b42a91d55878",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "FI3C6mc2Jnk8",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Test samples: 5747\n",
            "Test Max sequence length for inputs: 18\n",
            "Test Max sequence length for outputs: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])"
      ],
      "metadata": {
        "id": "zR4X9PmLhR53"
      },
      "id": "zR4X9PmLhR53",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
      ],
      "metadata": {
        "id": "myxcPh_f48Zd"
      },
      "id": "myxcPh_f48Zd",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder_input_data, decoder_input_data, decoder_target_data = get_vectors(\n",
        "#     input_texts, target_texts, input_token_index, target_token_index, \n",
        "#     max_encoder_seq_length, num_encoder_tokens, max_decoder_seq_length,\n",
        "#     num_decoder_tokens)"
      ],
      "metadata": {
        "id": "KIj198FFPahj"
      },
      "id": "KIj198FFPahj",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# val_encoder_input_data, val_decoder_input_data, val_decoder_target_data = get_vectors(\n",
        "#     val_input_texts, val_target_texts, input_token_index, target_token_index,\n",
        "#     val_max_encoder_seq_length, val_num_encoder_tokens, \n",
        "#     val_max_decoder_seq_length, val_num_decoder_tokens)"
      ],
      "metadata": {
        "id": "IRzK9ts5Ozof"
      },
      "id": "IRzK9ts5Ozof",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_encoder_input_data = get_vectors(\n",
        "#     test_input_texts, test_target_texts, input_token_index, target_token_index,\n",
        "#     test_max_encoder_seq_length, num_encoder_tokens, is_test_data=True)"
      ],
      "metadata": {
        "id": "Dc-0MfdiRkTE"
      },
      "id": "Dc-0MfdiRkTE",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t] = input_token_index[char]\n",
        "    #encoder_input_data[i, t + 1 :] = input_token_index[\"P\"]\n",
        "    encoder_input_data[i, t + 1 :] = input_token_index[\" \"]\n",
        "\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    #decoder_input_data[i, t + 1: ] = target_token_index[\"P\"]\n",
        "    decoder_input_data[i, t + 1: ] = target_token_index[\" \"]\n",
        "    #decoder_target_data[i, t:, target_token_index[\"P\"]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "\n",
        "val_encoder_input_data = np.zeros(\n",
        "    (len(input_texts), val_max_encoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "val_decoder_input_data = np.zeros(\n",
        "    (len(input_texts), val_max_decoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "val_decoder_target_data = np.zeros(\n",
        "    (len(input_texts), val_max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(val_input_texts, val_target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        val_encoder_input_data[i, t] = input_token_index[char]\n",
        "    #encoder_input_data[i, t + 1 :] = input_token_index[\"P\"]\n",
        "    val_encoder_input_data[i, t + 1 :] = input_token_index[\" \"]\n",
        "\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        val_decoder_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            val_decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    #decoder_input_data[i, t + 1: ] = target_token_index[\"P\"]\n",
        "    val_decoder_input_data[i, t + 1: ] = target_token_index[\" \"]\n",
        "    #decoder_target_data[i, t:, target_token_index[\"P\"]] = 1.0\n",
        "    val_decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
      ],
      "metadata": {
        "id": "Ko2nsRcK4_6D"
      },
      "id": "Ko2nsRcK4_6D",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_token_index)\n",
        "print(target_token_index)\n",
        "print(reverse_input_char_index)\n",
        "print(reverse_target_char_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW1Y7QDj5B5H",
        "outputId": "350c1ce3-493f-459b-a748-9e882f577b9c"
      },
      "id": "XW1Y7QDj5B5H",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ం': 0, 'ౌ': 1, 'వ': 2, 'ఐ': 3, 'ఏ': 4, 'జ': 5, 'డ': 6, 'ో': 7, 'ణ': 8, 'ఉ': 9, 'ఢ': 10, 'ీ': 11, 'ద': 12, 'ఎ': 13, 'స': 14, 'ె': 15, 'శ': 16, 'ళ': 17, 'ై': 18, 'ా': 19, 'ఔ': 20, 'ః': 21, '\\u200c': 22, 'ల': 23, 'ర': 24, 'భ': 25, 'ఠ': 26, 'క': 27, 'ఒ': 28, 'ృ': 29, 'థ': 30, 'ొ': 31, 'హ': 32, 'య': 33, 'ఖ': 34, 'మ': 35, 'న': 36, 'గ': 37, 'ష': 38, 'ట': 39, 'ఫ': 40, 'ఛ': 41, 'బ': 42, 'త': 43, 'ఞ': 44, 'ఋ': 45, ' ': 46, 'ి': 47, 'ఘ': 48, 'ఆ': 49, 'ఊ': 50, 'ఇ': 51, 'ే': 52, 'అ': 53, 'చ': 54, 'ప': 55, 'ధ': 56, 'ఱ': 57, 'ూ': 58, '్': 59, 'ు': 60, 'ఝ': 61, 'ఈ': 62, 'ఓ': 63}\n",
            "{'x': 0, 'w': 1, 'a': 2, 'm': 3, 'p': 4, 'v': 5, 'h': 6, 't': 7, 'i': 8, 'n': 9, 'l': 10, 'r': 11, 'y': 12, 'z': 13, 'c': 14, 'q': 15, 'u': 16, 'j': 17, ' ': 18, '\\n': 19, '\\t': 20, 'b': 21, 's': 22, 'o': 23, 'f': 24, 'e': 25, 'k': 26, 'g': 27, 'd': 28}\n",
            "{0: 'ం', 1: 'ౌ', 2: 'వ', 3: 'ఐ', 4: 'ఏ', 5: 'జ', 6: 'డ', 7: 'ో', 8: 'ణ', 9: 'ఉ', 10: 'ఢ', 11: 'ీ', 12: 'ద', 13: 'ఎ', 14: 'స', 15: 'ె', 16: 'శ', 17: 'ళ', 18: 'ై', 19: 'ా', 20: 'ఔ', 21: 'ః', 22: '\\u200c', 23: 'ల', 24: 'ర', 25: 'భ', 26: 'ఠ', 27: 'క', 28: 'ఒ', 29: 'ృ', 30: 'థ', 31: 'ొ', 32: 'హ', 33: 'య', 34: 'ఖ', 35: 'మ', 36: 'న', 37: 'గ', 38: 'ష', 39: 'ట', 40: 'ఫ', 41: 'ఛ', 42: 'బ', 43: 'త', 44: 'ఞ', 45: 'ఋ', 46: ' ', 47: 'ి', 48: 'ఘ', 49: 'ఆ', 50: 'ఊ', 51: 'ఇ', 52: 'ే', 53: 'అ', 54: 'చ', 55: 'ప', 56: 'ధ', 57: 'ఱ', 58: 'ూ', 59: '్', 60: 'ు', 61: 'ఝ', 62: 'ఈ', 63: 'ఓ'}\n",
            "{0: 'x', 1: 'w', 2: 'a', 3: 'm', 4: 'p', 5: 'v', 6: 'h', 7: 't', 8: 'i', 9: 'n', 10: 'l', 11: 'r', 12: 'y', 13: 'z', 14: 'c', 15: 'q', 16: 'u', 17: 'j', 18: ' ', 19: '\\n', 20: '\\t', 21: 'b', 22: 's', 23: 'o', 24: 'f', 25: 'e', 26: 'k', 27: 'g', 28: 'd'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test, y_test = val_encoder_input_data, val_target_texts"
      ],
      "metadata": {
        "id": "E6KLsqAW5DcP"
      },
      "id": "E6KLsqAW5DcP",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(predictions_path, 'w') as f:\n",
        "    f.write('Telugu\\tPredicted\\tActual\\n')"
      ],
      "metadata": {
        "id": "yFWuPo9g5IJh"
      },
      "id": "yFWuPo9g5IJh",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Model"
      ],
      "metadata": {
        "id": "dMhX6Gga5UKF"
      },
      "id": "dMhX6Gga5UKF"
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomRNN(object):\n",
        "\n",
        "    def __init__(self,cell_type = 'GRU',in_emb = 32, hidden_size=32, learning_rate= 1e-3, \n",
        "                dropout=0.4,pred_type ='greedy',epochs = 10, batch_size = 32,beam_width = 5,\n",
        "                num_enc = 1,num_dec = 1):\n",
        "      \n",
        "      self.cell_type = cell_type\n",
        "      self.in_emb = in_emb\n",
        "      self.hidden_size = hidden_size\n",
        "      self.learning_rate = learning_rate\n",
        "      self.dropout = dropout\n",
        "      self.pred_type = pred_type\n",
        "      self.epochs = epochs\n",
        "      self.batch_size = batch_size\n",
        "      self.beam_width = beam_width\n",
        "      self.num_enc = num_enc\n",
        "      self.num_dec = num_dec\n",
        "\n",
        "    def build_fit(self, encoder_input_data, decoder_input_data, decoder_target_data, x_test, y_test):\n",
        "      \n",
        "      # Define an input sequence and process it.\n",
        "      encoder_inputs = Input(shape=(None, ), name='Enc_inputs')\n",
        "\n",
        "      # Add an Embedding layer expecting input vocab of size num_encoder_tokens, and output embedding dimension of size in_enc.\n",
        "      enc_emb = Embedding(num_encoder_tokens, self.in_emb, mask_zero=True, name='Enc_emb')(encoder_inputs)\n",
        "      encoder_outputs = enc_emb\n",
        "\n",
        "      customFunction = None\n",
        "      if self.cell_type == 'LSTM':\n",
        "          customFunction = LSTM\n",
        "      elif self.cell_type == 'GRU':\n",
        "          customFunction = GRU\n",
        "      elif self.cell_type == 'RNN':\n",
        "          customFunction = SimpleRNN\n",
        "      \n",
        "      encoder_lstm = customFunction(self.hidden_size, return_state=True, dropout = self.dropout, return_sequences=True, name=\"Enc_hidden_1\")\n",
        "      if self.cell_type == 'LSTM':\n",
        "        encoder_outputs, state_h, state_c = encoder_lstm(encoder_outputs)\n",
        "        encoder_states = [state_h, state_c]\n",
        "      else:\n",
        "        encoder_outputs, state_h = encoder_lstm(encoder_outputs)\n",
        "        encoder_states = [state_h]\n",
        "\n",
        "      # Add a LSTM layer with hidden_size internal units.\n",
        "      for i in range(2, self.num_enc + 1):\n",
        "        layer_name = ('Enc_hidden_%d') %i\n",
        "        encoder_lstm = customFunction(self.hidden_size, return_state=True, dropout = self.dropout, return_sequences=True, name=layer_name)\n",
        "        if self.cell_type == 'LSTM':\n",
        "          encoder_outputs, state_h, state_c = encoder_lstm(encoder_outputs, initial_state=encoder_states)\n",
        "          encoder_states = [state_h, state_c]\n",
        "        else:\n",
        "          encoder_outputs, state_h = encoder_lstm(encoder_outputs, initial_state=encoder_states)\n",
        "          encoder_states = [state_h]\n",
        "\n",
        "      # Set up the decoder, using `encoder_states` as initial state.\n",
        "      decoder_inputs = Input(shape=(None,), name='Dec_inputs')\n",
        "      dec_emb_layer = Embedding(num_decoder_tokens, self.hidden_size, mask_zero=True, name='Dec_emb')\n",
        "      dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "      # We set up our decoder to return full output sequences, and to return internal states as well. \n",
        "      # We don't use the return states in the training model, but we will use them in inference.\n",
        "      decoder_outputs = dec_emb\n",
        "\n",
        "      decoder_lstm = customFunction(self.hidden_size, return_sequences=True, return_state=True, dropout = self.dropout, name=\"Dec_hidden_1\")\n",
        "      if self.cell_type == 'LSTM':\n",
        "        decoder_outputs, _, _ = decoder_lstm(decoder_outputs, initial_state=encoder_states)\n",
        "      else:\n",
        "        decoder_outputs, _ = decoder_lstm(decoder_outputs, initial_state=encoder_states)\n",
        "\n",
        "      for i in range(2, self.num_dec + 1):\n",
        "        layer_name = ('Dec_hidden_%d') %i\n",
        "        decoder_lstm = customFunction(self.hidden_size, return_sequences=True, return_state=True, dropout=self.dropout, name=layer_name)\n",
        "        if self.cell_type == 'LSTM':\n",
        "          decoder_outputs, _, _ = decoder_lstm(decoder_outputs, initial_state=encoder_states)\n",
        "        else:\n",
        "          decoder_outputs, _ = decoder_lstm(decoder_outputs, initial_state=encoder_states)\n",
        "\n",
        "      decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='dense')\n",
        "      decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "      # Define the model that takes encoder and decoder input to output decoder_outputs\n",
        "      model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "      model.summary()\n",
        "\n",
        "      #plot_model(model, to_file='model.png', show_shapes=True)\n",
        "      \n",
        "      # Define the optimizer\n",
        "      optimizer = Adam(learning_rate=self.learning_rate, beta_1=0.9, beta_2=0.999)\n",
        "      model.compile(loss = \"categorical_crossentropy\", optimizer = optimizer, metrics=['accuracy'])\n",
        "    \n",
        "      print('fitting the model....')\n",
        "\n",
        "      model.fit(\n",
        "          [encoder_input_data, decoder_input_data],\n",
        "          decoder_target_data,\n",
        "          batch_size=self.batch_size,\n",
        "          epochs=self.epochs,\n",
        "          callbacks = [WandbCallback()]\n",
        "          )\n",
        "      \n",
        "      #model.save(\"s2s\")\n",
        "      \n",
        "      #model = keras.models.load_model(\"s2s\")\n",
        "\n",
        "      print('before the inference of the model....')\n",
        "      \n",
        "      encoder_model, decoder_model = self.inference_model(model)\n",
        "\n",
        "      print('getting the inference of the model....')\n",
        "\n",
        "      global_total = 0\n",
        "      global_correct = 0\n",
        "      for i in range(len(val_input_texts)):\n",
        "        #input_seq = val_encoder_input_data[i : i + 1]\n",
        "        input_seq = x_test[i : i + 1]\n",
        "        result = self.decode_sequence(encoder_model,decoder_model,input_seq)\n",
        "        #target = val_target_texts[i]\n",
        "        target = y_test[i]\n",
        "        target = target[1:len(target)-1]\n",
        "        result = result[0:len(result)-1]\n",
        "        with open(predictions_path, 'a') as f:\n",
        "            f.write('{}\\t{}\\t{}\\n'.format(df_val[0][i], result.strip(), target.strip()))\n",
        "        #print(\"Target: %s \\n Result: %s\" % (target, result))\n",
        "\n",
        "        if result.strip() == target.strip():\n",
        "          global_correct = global_correct + 1\n",
        "        \n",
        "        global_total = global_total + 1\n",
        "        accuracy_epoch = global_correct/global_total\n",
        "        if global_total % 50 == 0:\n",
        "          wandb.log({'epoch_accuracy' : accuracy_epoch})\n",
        "        #print(\"Accuracy: %s\" % (accuracy_epoch))\n",
        "      \n",
        "      val_accuracy = global_correct/global_total\n",
        "      #print(val_accuracy)\n",
        "\n",
        "\n",
        "      wandb.log({'val_accuracy' : val_accuracy})\n",
        "      \n",
        "    def inference_model(self,model):\n",
        "      encoder_inputs = model.input[0]  # input_1\n",
        "      if self.cell_type == 'RNN' or self.cell_type == 'GRU':\n",
        "        encoder_outputs, state_h_enc = model.get_layer('Enc_hidden_'+ str(self.num_enc)).output\n",
        "        encoder_states = [state_h_enc]\n",
        "        encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "        decoder_inputs = model.input[1]  # input_1\n",
        "        decoder_outputs = model.get_layer('Dec_emb')(decoder_inputs)\n",
        "        decoder_states_inputs = []\n",
        "        decoder_states = []\n",
        "\n",
        "        for i in range(1,self.num_dec +1):\n",
        "          decoder_state_input_h = keras.Input(shape=(self.hidden_size,))\n",
        "          curr_states_inputs = [decoder_state_input_h]\n",
        "          decoder = model.get_layer('Dec_hidden_'+ str(i))\n",
        "          decoder_outputs, state_h_dec = decoder(decoder_outputs, initial_state=curr_states_inputs)\n",
        "\n",
        "          decoder_states += [state_h_dec]\n",
        "          decoder_states_inputs += curr_states_inputs\n",
        "\n",
        "      elif self.cell_type == 'LSTM':\n",
        "        encoder_outputs, state_h_enc, state_c_enc = model.get_layer('Enc_hidden_'+ str(self.num_enc)).output  # lstm_1\n",
        "        encoder_states = [state_h_enc, state_c_enc]\n",
        "        encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "        decoder_inputs = model.input[1]  # input_1\n",
        "        decoder_outputs = model.get_layer('Dec_emb')(decoder_inputs)\n",
        "        decoder_states_inputs = []\n",
        "        decoder_states = []\n",
        "\n",
        "        for i in range(1,self.num_dec +1):\n",
        "          decoder_state_input_h = keras.Input(shape=(self.hidden_size,))\n",
        "          decoder_state_input_c = keras.Input(shape=(self.hidden_size,))\n",
        "          curr_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "          decoder = model.get_layer('Dec_hidden_'+ str(i))\n",
        "          decoder_outputs, state_h_dec, state_c_dec = decoder(decoder_outputs, initial_state=curr_states_inputs)\n",
        "\n",
        "          decoder_states += [state_h_dec, state_c_dec]\n",
        "          decoder_states_inputs += curr_states_inputs\n",
        "\n",
        "\n",
        "      decoder_dense = model.get_layer('dense')\n",
        "      decoder_outputs = decoder_dense(decoder_outputs)\n",
        "      decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "      return encoder_model,decoder_model\n",
        "\n",
        "    def decode_sequence(self,encoder_model,decoder_model,input_seq):\n",
        "      # Encode the input as state vectors.\n",
        "      states_value = [encoder_model.predict(input_seq)] * self.num_dec\n",
        "      \n",
        "      # Generate empty target sequence of length 1.\n",
        "      target_seq = np.zeros((1, 1))\n",
        "      # Populate the first character of target sequence with the start character.\n",
        "      target_seq[0, 0] = target_token_index['b']\n",
        "\n",
        "      # Sampling loop for a batch of sequences\n",
        "      # (to simplify, here we assume a batch of size 1).\n",
        "      stop_condition = False\n",
        "      decoded_sentence = \"\"\n",
        "\n",
        "      while not stop_condition:\n",
        "          if self.cell_type == 'RNN' or self.cell_type == 'GRU':\n",
        "            dummy = decoder_model.predict([target_seq] + [states_value])\n",
        "            output_tokens, states_value = dummy[0],dummy[1:]\n",
        "            \n",
        "          elif self.cell_type == 'LSTM':  \n",
        "            dummy = decoder_model.predict([target_seq] + states_value)\n",
        "            output_tokens, states_value = dummy[0],dummy[1:]\n",
        "          \n",
        "          #print(output_tokens[0,:,:])\n",
        "          if self.pred_type == 'greedy':\n",
        "            beam_w = 1\n",
        "          elif self.pred_type == 'beam_search':\n",
        "            beam_w = self.beam_width\n",
        "          sampled_token_index = self.beam_search_decoder(output_tokens[0,:,:], beam_w)\n",
        "          sampled_token_index = sampled_token_index[beam_w-1][0]\n",
        "\n",
        "          # Sample a token\n",
        "          sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "          sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "          decoded_sentence += sampled_char\n",
        "\n",
        "          # Exit condition: either hit max length\n",
        "          # or find stop character.\n",
        "          if sampled_char == 'E' or len(decoded_sentence) > max_decoder_seq_length:\n",
        "              stop_condition = True\n",
        "\n",
        "          # Update the target sequence (of length 1).\n",
        "          target_seq = np.zeros((1, 1))\n",
        "          target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "          # Update state\n",
        "\n",
        "      return decoded_sentence\n",
        "    \n",
        "    def beam_search_decoder(self,data, k):\n",
        "      sequences = [[list(), 0.0]]\n",
        "      # walk over each step in sequence\n",
        "      for row in data:\n",
        "        all_candidates = list()\n",
        "        # expand each current candidate\n",
        "        for i in range(len(sequences)):\n",
        "          seq, score = sequences[i]\n",
        "          for j in range(len(row)):\n",
        "            candidate = [seq + [j], score - log(row[j])]\n",
        "            #candidate = [seq + [j], score - log1p(row[j])]\n",
        "            all_candidates.append(candidate)\n",
        "        # order all candidates by score\n",
        "        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "        # select k best\n",
        "        sequences = ordered[:k]\n",
        "      return sequences"
      ],
      "metadata": {
        "id": "Z1Ya6JFE5VkE"
      },
      "id": "Z1Ya6JFE5VkE",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sweep Config"
      ],
      "metadata": {
        "id": "onxvU8-85g3P"
      },
      "id": "onxvU8-85g3P"
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes', \n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "\n",
        "        'dropout': {\n",
        "            'values': [0.0, 0.1, 0.2]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-3, 1e-4]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [64, 128]\n",
        "        },\n",
        "        'in_emb': {\n",
        "            'values': [32, 64, 128]\n",
        "        },\n",
        "        'num_enc': {\n",
        "            'values': [1]\n",
        "        },\n",
        "        'num_dec': {\n",
        "            'values': [1]\n",
        "        },\n",
        "        'hidden_size':{\n",
        "            'values': [32]\n",
        "        },\n",
        "        'cell_type': {\n",
        "            # 'values': ['RNN', 'GRU', 'LSTM']\n",
        "            'values': ['GRU']\n",
        "        },\n",
        "        'dec_search': {\n",
        "              'values': ['greedy']\n",
        "            # 'values': ['beam_search', 'greedy']\n",
        "        },\n",
        "        'beam_width':{\n",
        "            'values': [1]\n",
        "        },\n",
        "        'epochs': {\n",
        "            'values': [10]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "UbN4yF355jA9"
      },
      "id": "UbN4yF355jA9",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, entity=\"cs21m010-cs21m041\", project=\"DL_Assignment_3_a\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTVK0bfI5kh2",
        "outputId": "d20008b1-9360-4dc2-a115-f026bc65853d"
      },
      "id": "DTVK0bfI5kh2",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: xakmj1sd\n",
            "Sweep URL: https://wandb.ai/cs21m010-cs21m041/DL_Assignment_3_a/sweeps/xakmj1sd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "\n",
        "    # Create a new WandB run\n",
        "    wandb.init(config=sweep_config)\n",
        "    \n",
        "    # Construct the run name\n",
        "    config = wandb.config\n",
        "    wandb.run.name = str(config.cell_type) + '_' + config.dec_search + '_bs_' + str(config.batch_size)\n",
        "    \n",
        "    model_rnn = CustomRNN(cell_type = config.cell_type, in_emb = config.in_emb, hidden_size=config.hidden_size,\n",
        "                  learning_rate= config.learning_rate, dropout=config.dropout,pred_type = config.dec_search,epochs = config.epochs,\n",
        "                  batch_size = config.batch_size, beam_width = config.beam_width, num_enc = config.num_enc,num_dec = config.num_dec)\n",
        "    \n",
        "    model_rnn.build_fit(encoder_input_data,decoder_input_data,decoder_target_data,x_test, y_test)"
      ],
      "metadata": {
        "id": "m5dHjntQ5mLF"
      },
      "id": "m5dHjntQ5mLF",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, train, count = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D-X5sp-N5oa8",
        "outputId": "59e42fe8-3b94-4ee0-8ec5-5a9cbace357d"
      },
      "id": "D-X5sp-N5oa8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kq4m1a2s with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_search: greedy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tin_emb: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_dec: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_enc: 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.15"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220422_111359-kq4m1a2s</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/cs21m010-cs21m041/DL_Assignment_3_a/runs/kq4m1a2s\" target=\"_blank\">stoic-sweep-1</a></strong> to <a href=\"https://wandb.ai/cs21m010-cs21m041/DL_Assignment_3_a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/cs21m010-cs21m041/DL_Assignment_3_a/sweeps/xakmj1sd\" target=\"_blank\">https://wandb.ai/cs21m010-cs21m041/DL_Assignment_3_a/sweeps/xakmj1sd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Enc_inputs (InputLayer)        [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Dec_inputs (InputLayer)        [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Enc_emb (Embedding)            (None, None, 64)     4096        ['Enc_inputs[0][0]']             \n",
            "                                                                                                  \n",
            " Dec_emb (Embedding)            (None, None, 32)     928         ['Dec_inputs[0][0]']             \n",
            "                                                                                                  \n",
            " Enc_hidden_1 (GRU)             [(None, None, 32),   9408        ['Enc_emb[0][0]']                \n",
            "                                 (None, 32)]                                                      \n",
            "                                                                                                  \n",
            " Dec_hidden_1 (GRU)             [(None, None, 32),   6336        ['Dec_emb[0][0]',                \n",
            "                                 (None, 32)]                      'Enc_hidden_1[0][1]']           \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 29)     957         ['Dec_hidden_1[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 21,725\n",
            "Trainable params: 21,725\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "fitting the model....\n",
            "Epoch 1/10\n",
            "458/458 [==============================] - 37s 57ms/step - loss: 1.2598 - accuracy: 0.6862 - _timestamp: 1650626093.0000 - _runtime: 54.0000\n",
            "Epoch 2/10\n",
            "458/458 [==============================] - 26s 56ms/step - loss: 0.8718 - accuracy: 0.7392 - _timestamp: 1650626119.0000 - _runtime: 80.0000\n",
            "Epoch 3/10\n",
            "458/458 [==============================] - 26s 57ms/step - loss: 0.8217 - accuracy: 0.7479 - _timestamp: 1650626145.0000 - _runtime: 106.0000\n",
            "Epoch 4/10\n",
            "458/458 [==============================] - 27s 59ms/step - loss: 0.7867 - accuracy: 0.7576 - _timestamp: 1650626172.0000 - _runtime: 133.0000\n",
            "Epoch 5/10\n",
            "458/458 [==============================] - 29s 63ms/step - loss: 0.7489 - accuracy: 0.7679 - _timestamp: 1650626201.0000 - _runtime: 162.0000\n",
            "Epoch 6/10\n",
            "458/458 [==============================] - 26s 56ms/step - loss: 0.7107 - accuracy: 0.7776 - _timestamp: 1650626227.0000 - _runtime: 188.0000\n",
            "Epoch 7/10\n",
            "458/458 [==============================] - 26s 56ms/step - loss: 0.6819 - accuracy: 0.7856 - _timestamp: 1650626253.0000 - _runtime: 214.0000\n",
            "Epoch 8/10\n",
            "458/458 [==============================] - 26s 57ms/step - loss: 0.6598 - accuracy: 0.7922 - _timestamp: 1650626279.0000 - _runtime: 240.0000\n",
            "Epoch 9/10\n",
            "458/458 [==============================] - 26s 57ms/step - loss: 0.6397 - accuracy: 0.7981 - _timestamp: 1650626305.0000 - _runtime: 266.0000\n",
            "Epoch 10/10\n",
            "458/458 [==============================] - 26s 56ms/step - loss: 0.6228 - accuracy: 0.8031 - _timestamp: 1650626330.0000 - _runtime: 291.0000\n",
            "before the inference of the model....\n",
            "getting the inference of the model....\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "assignment_3.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}