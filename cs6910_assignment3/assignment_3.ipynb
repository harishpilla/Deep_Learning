{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "e45c6f8d",
      "metadata": {
        "id": "e45c6f8d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import tensorflow \n",
        "import keras\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, GRU, Dropout, SimpleRNN\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from math import log"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install wandb -q\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "metadata": {
        "id": "mj8H54q04eta"
      },
      "id": "mj8H54q04eta",
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "if not os.path.exists('/content/dakshina_dataset_v1.0.tar'):\n",
        "    !wget \"https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\""
      ],
      "metadata": {
        "id": "3RneRWbD3oCj"
      },
      "id": "3RneRWbD3oCj",
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvf /content/dakshina_dataset_v1.0.tar"
      ],
      "metadata": {
        "id": "mDdPDwCy36Dg",
        "outputId": "4f17a9d8-d508-4cf1-f9d5-550155b11938",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mDdPDwCy36Dg",
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dakshina_dataset_v1.0/bn/\n",
            "dakshina_dataset_v1.0/bn/lexicons/\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/romanized/\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/\n",
            "dakshina_dataset_v1.0/gu/lexicons/\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/romanized/\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/\n",
            "dakshina_dataset_v1.0/hi/lexicons/\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/romanized/\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/\n",
            "dakshina_dataset_v1.0/kn/lexicons/\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/kn/romanized/\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/\n",
            "dakshina_dataset_v1.0/ml/lexicons/\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.rom.txt\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/romanized/\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/\n",
            "dakshina_dataset_v1.0/mr/lexicons/\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/romanized/\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/\n",
            "dakshina_dataset_v1.0/pa/lexicons/\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/pa/romanized/\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/\n",
            "dakshina_dataset_v1.0/sd/lexicons/\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/sd/romanized/\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/\n",
            "dakshina_dataset_v1.0/si/lexicons/\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/romanized/\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/\n",
            "dakshina_dataset_v1.0/ta/lexicons/\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/romanized/\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/\n",
            "dakshina_dataset_v1.0/te/lexicons/\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/romanized/\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ur/\n",
            "dakshina_dataset_v1.0/ur/lexicons/\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/romanized/\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_path = '/content/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv'\n",
        "validation_data_path = '/content/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv'\n",
        "test_data_path = '/content/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv'\n",
        "\n",
        "df_train = pd.read_csv(train_data_path, sep='\\t', header=None)\n",
        "df_val = pd.read_csv(validation_data_path, sep='\\t', header=None)\n",
        "df_test = pd.read_csv(test_data_path, sep='\\t', header=None)"
      ],
      "metadata": {
        "id": "pmx3VVhpURl_"
      },
      "id": "pmx3VVhpURl_",
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_path = '/content/predictions.tsv'"
      ],
      "metadata": {
        "id": "8RhaIQ1o5OlO"
      },
      "id": "8RhaIQ1o5OlO",
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(df, input_texts, target_texts, input_characters=None, target_characters=None, is_test_data=False):\n",
        "    \n",
        "    for  _, row in df.iterrows():\n",
        "        input_text, target_text = str(row[0]), str(row[1])\n",
        "        input_texts.append(input_text)\n",
        "        target_text = '\\t' + target_text + '\\n'\n",
        "        target_texts.append(target_text)\n",
        "        \n",
        "        if not is_test_data:\n",
        "            for char in input_text:\n",
        "                if char not in input_characters:\n",
        "                    input_characters.add(char)\n",
        "\n",
        "            for char in target_text:\n",
        "                if char not in target_characters:\n",
        "                    target_characters.add(char)\n",
        "\n",
        "    max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "    max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "    if not is_test_data:\n",
        "        input_characters = sorted(list(input_characters))\n",
        "        target_characters = sorted(list(target_characters))\n",
        "        num_encoder_tokens = len(input_characters)\n",
        "        num_decoder_tokens = len(target_characters)\n",
        "\n",
        "        return max_encoder_seq_length, max_decoder_seq_length, num_encoder_tokens, num_decoder_tokens\n",
        "    \n",
        "    return max_encoder_seq_length, max_decoder_seq_length"
      ],
      "metadata": {
        "id": "dBL7HWehFkBn"
      },
      "id": "dBL7HWehFkBn",
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vectors(input_texts, target_texts, input_token_index, target_token_index,\n",
        "                max_encoder_seq_length, num_encoder_tokens, \n",
        "                max_decoder_seq_length=None, num_decoder_tokens=None,\n",
        "                is_test_data=False):\n",
        "      \n",
        "    encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length), dtype=\"float32\")\n",
        "\n",
        "    if not is_test_data:\n",
        "        decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length), dtype=\"float32\")\n",
        "        decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "        for t, char in enumerate(input_text):\n",
        "            encoder_input_data[i, t] = input_token_index[char]\n",
        "        encoder_input_data[i, t + 1 :] = input_token_index[\" \"]\n",
        "\n",
        "\n",
        "        if not is_test_data:\n",
        "            for t, char in enumerate(target_text):\n",
        "                decoder_input_data[i, t] = target_token_index[char]\n",
        "                if t > 0:\n",
        "                    # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "                    # decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "                    decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "            decoder_input_data[i, t + 1: ] = target_token_index[\" \"]\n",
        "            decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0    \n",
        "\n",
        "    if is_test_data:\n",
        "        return encoder_input_data\n",
        "\n",
        "    return encoder_input_data, decoder_input_data, decoder_target_data"
      ],
      "metadata": {
        "id": "58JABACiLR7G"
      },
      "id": "58JABACiLR7G",
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Train, Validation and Test data"
      ],
      "metadata": {
        "id": "qtrqWffEJace"
      },
      "id": "qtrqWffEJace"
    },
    {
      "cell_type": "code",
      "source": [
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "input_characters = set(' ')\n",
        "target_characters = set(' ')\n",
        "\n",
        "max_encoder_seq_length, max_decoder_seq_length, num_encoder_tokens, num_decoder_tokens = load_data(df_train, input_texts, target_texts, input_characters, target_characters)\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)"
      ],
      "metadata": {
        "id": "XwhSrjplHqoc",
        "outputId": "4b27f75f-e1a5-4a82-92c0-962773554e9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "XwhSrjplHqoc",
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 58550\n",
            "Number of unique input tokens: 64\n",
            "Number of unique output tokens: 29\n",
            "Max sequence length for inputs: 20\n",
            "Max sequence length for outputs: 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_input_texts = []\n",
        "val_target_texts = []\n",
        "\n",
        "val_max_encoder_seq_length, val_max_decoder_seq_length, val_num_encoder_tokens, val_num_decoder_tokens = load_data(\n",
        "    df_val, val_input_texts, val_target_texts, input_characters, target_characters)\n",
        "\n",
        "print(\"Number of samples:\", len(val_input_texts))\n",
        "print(\"Number of unique input tokens:\", val_num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", val_num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", val_max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", val_max_decoder_seq_length)"
      ],
      "metadata": {
        "id": "I97sHjSyIjba",
        "outputId": "d6a8e5f0-dd41-44b9-d156-2187f26227da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "I97sHjSyIjba",
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 5683\n",
            "Number of unique input tokens: 64\n",
            "Number of unique output tokens: 29\n",
            "Max sequence length for inputs: 19\n",
            "Max sequence length for outputs: 23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_texts = []\n",
        "test_target_texts = []\n",
        "\n",
        "test_max_encoder_seq_length, test_max_decoder_seq_length = load_data(df_test, test_input_texts, test_target_texts, is_test_data=True)\n",
        "\n",
        "print(\"Number of Test samples:\", len(test_input_texts))\n",
        "print(\"Test Max sequence length for inputs:\", test_max_encoder_seq_length)\n",
        "print(\"Test Max sequence length for outputs:\", test_max_decoder_seq_length)"
      ],
      "metadata": {
        "id": "FI3C6mc2Jnk8",
        "outputId": "e99e7ed0-06c9-4f8a-e14b-76df58836c0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "FI3C6mc2Jnk8",
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Test samples: 5747\n",
            "Test Max sequence length for inputs: 18\n",
            "Test Max sequence length for outputs: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])"
      ],
      "metadata": {
        "id": "zR4X9PmLhR53"
      },
      "id": "zR4X9PmLhR53",
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
      ],
      "metadata": {
        "id": "myxcPh_f48Zd"
      },
      "id": "myxcPh_f48Zd",
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder_input_data, decoder_input_data, decoder_target_data = get_vectors(\n",
        "#     input_texts, target_texts, input_token_index, target_token_index, \n",
        "#     max_encoder_seq_length, num_encoder_tokens, max_decoder_seq_length,\n",
        "#     num_decoder_tokens)"
      ],
      "metadata": {
        "id": "KIj198FFPahj"
      },
      "id": "KIj198FFPahj",
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# val_encoder_input_data, val_decoder_input_data, val_decoder_target_data = get_vectors(\n",
        "#     val_input_texts, val_target_texts, input_token_index, target_token_index,\n",
        "#     val_max_encoder_seq_length, val_num_encoder_tokens, \n",
        "#     val_max_decoder_seq_length, val_num_decoder_tokens)"
      ],
      "metadata": {
        "id": "IRzK9ts5Ozof"
      },
      "id": "IRzK9ts5Ozof",
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_encoder_input_data = get_vectors(\n",
        "#     test_input_texts, test_target_texts, input_token_index, target_token_index,\n",
        "#     test_max_encoder_seq_length, num_encoder_tokens, is_test_data=True)"
      ],
      "metadata": {
        "id": "Dc-0MfdiRkTE"
      },
      "id": "Dc-0MfdiRkTE",
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t] = input_token_index[char]\n",
        "    #encoder_input_data[i, t + 1 :] = input_token_index[\"P\"]\n",
        "    encoder_input_data[i, t + 1 :] = input_token_index[\" \"]\n",
        "\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    #decoder_input_data[i, t + 1: ] = target_token_index[\"P\"]\n",
        "    decoder_input_data[i, t + 1: ] = target_token_index[\" \"]\n",
        "    #decoder_target_data[i, t:, target_token_index[\"P\"]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "\n",
        "val_encoder_input_data = np.zeros(\n",
        "    (len(input_texts), val_max_encoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "val_decoder_input_data = np.zeros(\n",
        "    (len(input_texts), val_max_decoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "val_decoder_target_data = np.zeros(\n",
        "    (len(input_texts), val_max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(val_input_texts, val_target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        val_encoder_input_data[i, t] = input_token_index[char]\n",
        "    #encoder_input_data[i, t + 1 :] = input_token_index[\"P\"]\n",
        "    val_encoder_input_data[i, t + 1 :] = input_token_index[\" \"]\n",
        "\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        val_decoder_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            val_decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    #decoder_input_data[i, t + 1: ] = target_token_index[\"P\"]\n",
        "    val_decoder_input_data[i, t + 1: ] = target_token_index[\" \"]\n",
        "    #decoder_target_data[i, t:, target_token_index[\"P\"]] = 1.0\n",
        "    val_decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
      ],
      "metadata": {
        "id": "Ko2nsRcK4_6D"
      },
      "id": "Ko2nsRcK4_6D",
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_token_index)\n",
        "print(target_token_index)\n",
        "print(reverse_input_char_index)\n",
        "print(reverse_target_char_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW1Y7QDj5B5H",
        "outputId": "48262a91-774b-4587-ad13-f0baee5125ff"
      },
      "id": "XW1Y7QDj5B5H",
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'త': 0, '\\u200c': 1, 'ో': 2, 'ఎ': 3, 'ర': 4, 'ణ': 5, 'ి': 6, 'ౌ': 7, 'ఏ': 8, 'థ': 9, 'ఘ': 10, 'ఱ': 11, 'ఆ': 12, 'ఐ': 13, 'ఈ': 14, 'చ': 15, 'ూ': 16, 'ఋ': 17, 'ఊ': 18, 'ఔ': 19, 'గ': 20, 'ష': 21, 'హ': 22, 'ఞ': 23, 'ఒ': 24, 'ు': 25, 'ఇ': 26, 'అ': 27, 'య': 28, 'ళ': 29, 'జ': 30, 'మ': 31, 'ీ': 32, 'ధ': 33, 'ఛ': 34, 'ొ': 35, 'ఓ': 36, 'ే': 37, 'ట': 38, 'ృ': 39, 'శ': 40, 'క': 41, 'ః': 42, 'ల': 43, 'ప': 44, 'ె': 45, 'ై': 46, 'బ': 47, 'ా': 48, ' ': 49, 'ం': 50, 'స': 51, 'ద': 52, 'ఝ': 53, 'ఖ': 54, 'ఉ': 55, '్': 56, 'భ': 57, 'ఫ': 58, 'ఢ': 59, 'న': 60, 'వ': 61, 'ఠ': 62, 'డ': 63}\n",
            "{'k': 0, 's': 1, 'l': 2, 'v': 3, 'n': 4, 'g': 5, 'y': 6, 'o': 7, 'z': 8, 'a': 9, '\\n': 10, 'h': 11, 'u': 12, 'j': 13, 'p': 14, '\\t': 15, 'd': 16, 'w': 17, 'm': 18, 'i': 19, 't': 20, 'e': 21, 'x': 22, 'b': 23, 'f': 24, ' ': 25, 'c': 26, 'r': 27, 'q': 28}\n",
            "{0: 'త', 1: '\\u200c', 2: 'ో', 3: 'ఎ', 4: 'ర', 5: 'ణ', 6: 'ి', 7: 'ౌ', 8: 'ఏ', 9: 'థ', 10: 'ఘ', 11: 'ఱ', 12: 'ఆ', 13: 'ఐ', 14: 'ఈ', 15: 'చ', 16: 'ూ', 17: 'ఋ', 18: 'ఊ', 19: 'ఔ', 20: 'గ', 21: 'ష', 22: 'హ', 23: 'ఞ', 24: 'ఒ', 25: 'ు', 26: 'ఇ', 27: 'అ', 28: 'య', 29: 'ళ', 30: 'జ', 31: 'మ', 32: 'ీ', 33: 'ధ', 34: 'ఛ', 35: 'ొ', 36: 'ఓ', 37: 'ే', 38: 'ట', 39: 'ృ', 40: 'శ', 41: 'క', 42: 'ః', 43: 'ల', 44: 'ప', 45: 'ె', 46: 'ై', 47: 'బ', 48: 'ా', 49: ' ', 50: 'ం', 51: 'స', 52: 'ద', 53: 'ఝ', 54: 'ఖ', 55: 'ఉ', 56: '్', 57: 'భ', 58: 'ఫ', 59: 'ఢ', 60: 'న', 61: 'వ', 62: 'ఠ', 63: 'డ'}\n",
            "{0: 'k', 1: 's', 2: 'l', 3: 'v', 4: 'n', 5: 'g', 6: 'y', 7: 'o', 8: 'z', 9: 'a', 10: '\\n', 11: 'h', 12: 'u', 13: 'j', 14: 'p', 15: '\\t', 16: 'd', 17: 'w', 18: 'm', 19: 'i', 20: 't', 21: 'e', 22: 'x', 23: 'b', 24: 'f', 25: ' ', 26: 'c', 27: 'r', 28: 'q'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test, y_test = val_encoder_input_data, val_target_texts"
      ],
      "metadata": {
        "id": "E6KLsqAW5DcP"
      },
      "id": "E6KLsqAW5DcP",
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(predictions_path, 'w') as f:\n",
        "    f.write('Telugu\\tPredicted\\tActual\\n')"
      ],
      "metadata": {
        "id": "yFWuPo9g5IJh"
      },
      "id": "yFWuPo9g5IJh",
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Model"
      ],
      "metadata": {
        "id": "dMhX6Gga5UKF"
      },
      "id": "dMhX6Gga5UKF"
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomRNN(object):\n",
        "\n",
        "    def __init__(self, cell_type='GRU', in_emb=32, hidden_size=32, learning_rate=1e-3,\n",
        "                 dropout=0.4, pred_type='greedy', epochs=10, batch_size=32, beam_width=5,\n",
        "                 num_enc=1, num_dec=1):\n",
        "\n",
        "        self.cell_type = cell_type\n",
        "        self.in_emb = in_emb\n",
        "        self.hidden_size = hidden_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.dropout = dropout\n",
        "        self.pred_type = pred_type\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.beam_width = beam_width\n",
        "        self.num_enc = num_enc\n",
        "        self.num_dec = num_dec\n",
        "\n",
        "    def build_fit(self, encoder_input_data, decoder_input_data, decoder_target_data, x_test, y_test):\n",
        "\n",
        "        # Define an input sequence and process it.\n",
        "        encoder_inputs = Input(shape=(None,), name='Enc_inputs')\n",
        "\n",
        "        # Add an Embedding layer expecting input vocab of size num_encoder_tokens, and output embedding dimension of size in_enc.\n",
        "        enc_emb = Embedding(num_encoder_tokens, self.in_emb, mask_zero=True, name='Enc_emb')(encoder_inputs)\n",
        "        encoder_outputs = enc_emb\n",
        "\n",
        "        customFunction = None\n",
        "        if self.cell_type == 'LSTM':\n",
        "            customFunction = LSTM\n",
        "        elif self.cell_type == 'GRU':\n",
        "            customFunction = GRU\n",
        "        elif self.cell_type == 'RNN':\n",
        "            customFunction = SimpleRNN\n",
        "\n",
        "        encoder_lstm = customFunction(self.hidden_size, return_state=True, dropout=self.dropout, return_sequences=True,\n",
        "                                      name=\"Enc_hidden_1\")\n",
        "        if self.cell_type == 'LSTM':\n",
        "            encoder_outputs, state_h, state_c = encoder_lstm(encoder_outputs)\n",
        "            encoder_states = [state_h, state_c]\n",
        "        else:\n",
        "            encoder_outputs, state_h = encoder_lstm(encoder_outputs)\n",
        "            encoder_states = [state_h]\n",
        "\n",
        "        # Add a LSTM layer with hidden_size internal units.\n",
        "        for i in range(2, self.num_enc + 1):\n",
        "            layer_name = ('Enc_hidden_%d') % i\n",
        "            encoder_lstm = customFunction(self.hidden_size, return_state=True, dropout=self.dropout,\n",
        "                                          return_sequences=True, name=layer_name)\n",
        "            if self.cell_type == 'LSTM':\n",
        "                encoder_outputs, state_h, state_c = encoder_lstm(encoder_outputs, initial_state=encoder_states)\n",
        "                encoder_states = [state_h, state_c]\n",
        "            else:\n",
        "                encoder_outputs, state_h = encoder_lstm(encoder_outputs, initial_state=encoder_states)\n",
        "                encoder_states = [state_h]\n",
        "\n",
        "        # Set up the decoder, using `encoder_states` as initial state.\n",
        "        decoder_inputs = Input(shape=(None,), name='Dec_inputs')\n",
        "        dec_emb_layer = Embedding(num_decoder_tokens, self.hidden_size, mask_zero=True, name='Dec_emb')\n",
        "        dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "        # We set up our decoder to return full output sequences, and to return internal states as well.\n",
        "        # We don't use the return states in the training model, but we will use them in inference.\n",
        "        decoder_outputs = dec_emb\n",
        "\n",
        "        decoder_lstm = customFunction(self.hidden_size, return_sequences=True, return_state=True, dropout=self.dropout,\n",
        "                                      name=\"Dec_hidden_1\")\n",
        "        if self.cell_type == 'LSTM':\n",
        "            decoder_outputs, _, _ = decoder_lstm(decoder_outputs, initial_state=encoder_states)\n",
        "        else:\n",
        "            decoder_outputs, _ = decoder_lstm(decoder_outputs, initial_state=encoder_states)\n",
        "\n",
        "        for i in range(2, self.num_dec + 1):\n",
        "            layer_name = ('Dec_hidden_%d') % i\n",
        "            decoder_lstm = customFunction(self.hidden_size, return_sequences=True, return_state=True,\n",
        "                                          dropout=self.dropout, name=layer_name)\n",
        "            if self.cell_type == 'LSTM':\n",
        "                decoder_outputs, _, _ = decoder_lstm(decoder_outputs, initial_state=encoder_states)\n",
        "            else:\n",
        "                decoder_outputs, _ = decoder_lstm(decoder_outputs, initial_state=encoder_states)\n",
        "\n",
        "        decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='dense')\n",
        "        decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "        # get the model and print summary\n",
        "        model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "        model.summary()\n",
        "\n",
        "        # Define the optimizer\n",
        "        optimizer = Adam(learning_rate=self.learning_rate, beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "        # compiling and fitting the model\n",
        "        print('fitting the model....')\n",
        "        model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n",
        "        model.fit(\n",
        "            [encoder_input_data, decoder_input_data],\n",
        "            decoder_target_data,\n",
        "            batch_size=self.batch_size,\n",
        "            epochs=self.epochs,\n",
        "            callbacks=[WandbCallback()]\n",
        "        )\n",
        "\n",
        "        print('Inference model....')\n",
        "        encoder_model, decoder_model = self.inference_model(model)\n",
        "        self.get_accuracy(encoder_model, decoder_model, x_test, y_test)\n",
        "\n",
        "    def get_accuracy(self, encoder_model, decoder_model, x_test, y_test):\n",
        "        total_no_of_samples = 0\n",
        "        no_of_correct_predictions = 0\n",
        "        for i in range(len(val_input_texts)):\n",
        "            input_seq = x_test[i: i + 1]\n",
        "\n",
        "            if self.pred_type == 'greedy':\n",
        "                predicted_sequence = self.decode_sequence(encoder_model, decoder_model, input_seq)\n",
        "            elif self.pred_type == 'beam_search':\n",
        "                predicted_sequence = self.beam_search_decoder(encoder_model, decoder_model, input_seq, self.beam_width)\n",
        "\n",
        "            predicted_sequence = predicted_sequence[0:len(predicted_sequence) - 1]\n",
        "\n",
        "            actual_sequence = y_test[i]\n",
        "            actual_sequence = actual_sequence[1:len(actual_sequence) - 1]\n",
        "\n",
        "            # write predictions to a file\n",
        "            with open(predictions_path, 'a') as f:\n",
        "                f.write('{}\\t{}\\t{}\\n'.format(df_val[0][i], predicted_sequence.strip(), actual_sequence.strip()))\n",
        "\n",
        "            if predicted_sequence.strip() == actual_sequence.strip():\n",
        "                no_of_correct_predictions += 1\n",
        "            total_no_of_samples += 1\n",
        "\n",
        "        val_accuracy = no_of_correct_predictions / total_no_of_samples\n",
        "        print(val_accuracy)\n",
        "        wandb.log({'val_accuracy': val_accuracy})\n",
        "\n",
        "    def inference_model(self, model):\n",
        "        encoder_inputs = model.input[0]\n",
        "        encoder_states = None\n",
        "        if self.cell_type == 'RNN' or self.cell_type == 'GRU':\n",
        "            encoder_outputs, state_h_enc = model.get_layer('Enc_hidden_' + str(self.num_enc)).output\n",
        "            encoder_states = [state_h_enc]\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            encoder_outputs, state_h_enc, state_c_enc = model.get_layer('Enc_hidden_' + str(self.num_enc)).output\n",
        "            encoder_states = [state_h_enc, state_c_enc]\n",
        "        encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "        decoder_inputs = model.input[1]\n",
        "        decoder_outputs = model.get_layer('Dec_emb')(decoder_inputs)\n",
        "        decoder_states_inputs = []\n",
        "        decoder_states = []\n",
        "\n",
        "        for i in range(1, self.num_dec + 1):\n",
        "            decoder = model.get_layer('Dec_hidden_' + str(i))\n",
        "            if self.cell_type == 'RNN' or self.cell_type == 'GRU':\n",
        "                decoder_state_input_h = keras.Input(shape=(self.hidden_size,))\n",
        "                curr_states_inputs = [decoder_state_input_h]\n",
        "                decoder_outputs, state_h_dec = decoder(decoder_outputs, initial_state=curr_states_inputs)\n",
        "                decoder_states += [state_h_dec]\n",
        "            elif self.cell_type == 'LSTM':\n",
        "                decoder_state_input_h = keras.Input(shape=(self.hidden_size,))\n",
        "                decoder_state_input_c = keras.Input(shape=(self.hidden_size,))\n",
        "                curr_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "                decoder_outputs, state_h_dec, state_c_dec = decoder(decoder_outputs, initial_state=curr_states_inputs)\n",
        "                decoder_states += [state_h_dec, state_c_dec]\n",
        "            decoder_states_inputs += curr_states_inputs\n",
        "\n",
        "        decoder_dense = model.get_layer('dense')\n",
        "        decoder_outputs = decoder_dense(decoder_outputs)\n",
        "        decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "        return encoder_model, decoder_model\n",
        "\n",
        "    def decode_sequence(self, encoder_model, decoder_model, input_seq):\n",
        "        states_value = [encoder_model.predict(input_seq)] * self.num_dec\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = target_token_index['b']\n",
        "\n",
        "        stop_condition = False\n",
        "        decoded_sentence = \"\"\n",
        "\n",
        "        while not stop_condition:\n",
        "            if self.cell_type == 'RNN' or self.cell_type == 'GRU':\n",
        "                tmp = decoder_model.predict([target_seq] + [states_value])\n",
        "                output_tokens, states_value = tmp[0], tmp[1:]\n",
        "\n",
        "            elif self.cell_type == 'LSTM':\n",
        "                tmp = decoder_model.predict([target_seq] + states_value)\n",
        "                output_tokens, states_value = tmp[0], tmp[1:]\n",
        "\n",
        "            # Sample a token\n",
        "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "            sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "            decoded_sentence += sampled_char\n",
        "\n",
        "            if sampled_char == 'E' or len(decoded_sentence) > max_decoder_seq_length:\n",
        "                stop_condition = True\n",
        "\n",
        "            # Update the target sequence (of length 1).\n",
        "            target_seq = np.zeros((1, 1))\n",
        "            target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        return decoded_sentence\n",
        "\n",
        "    def beam_search_decoder(self, encoder_model, decoder_model, input_seq, k):\n",
        "        # Encode the input as state vectors.\n",
        "        states_value = [encoder_model.predict(input_seq)] * self.num_dec\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = target_token_index['b']\n",
        "        stop_condition = False\n",
        "        sequences = [[0.0, 0, states_value, target_seq, list(), list()]]\n",
        "\n",
        "        while not stop_condition:\n",
        "            all_candidates = list()\n",
        "            for i in range(len(sequences)):\n",
        "                output = decoder_model.predict([sequences[i][3]] + sequences[i][2])\n",
        "                output_tokens, states_value = output[0], output[1:]\n",
        "                prob = output_tokens[0, -1, :]\n",
        "\n",
        "                score, end_of_word, states_value, target_sequence, sequence_token, d_word = sequences[i]\n",
        "\n",
        "                if end_of_word == 0:\n",
        "                    for j in range(len(reverse_target_char_index)):\n",
        "                        char = reverse_target_char_index[j]\n",
        "                        target_seq = np.zeros((1, 1))\n",
        "                        target_seq[0, 0] = j\n",
        "                        candidate = [score - np.log(prob[j]), 0, states_value, target_seq, sequence_token + [j], d_word + [char]]\n",
        "                        all_candidates.append(candidate)\n",
        "\n",
        "            ordered = sorted(all_candidates, key=lambda x: x[0])\n",
        "            minimum_length = min(k, len(ordered))\n",
        "            sequences = ordered[:minimum_length]\n",
        "            stop_condition = True\n",
        "            for sequence in range(len(sequences)):\n",
        "                score, end_of_word, states_value, target_sequence, sequence_token, d_word = sequences[sequence]\n",
        "                if d_word[-1] == \"\\n\" or len(d_word) > max_decoder_seq_length:\n",
        "                    end_of_word = 1\n",
        "                sequences[sequence] = [score, end_of_word, states_value, target_sequence, sequence_token, d_word].copy()\n",
        "                if end_of_word == 0:\n",
        "                    stop_condition = False\n",
        "\n",
        "            if sequences[0][-1][-1] == \"\\n\":\n",
        "                stop_condition = True\n",
        "\n",
        "        best_decoded_sentence = ''.join(sequences[0][5])\n",
        "        return best_decoded_sentence"
      ],
      "metadata": {
        "id": "Z1Ya6JFE5VkE"
      },
      "id": "Z1Ya6JFE5VkE",
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sweep Config"
      ],
      "metadata": {
        "id": "onxvU8-85g3P"
      },
      "id": "onxvU8-85g3P"
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes', \n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "\n",
        "        'dropout': {\n",
        "            'values': [0.0, 0.1, 0.2]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-3, 1e-4]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [64, 128]\n",
        "        },\n",
        "        'in_emb': {\n",
        "            'values': [32, 64, 128]\n",
        "        },\n",
        "        'num_enc': {\n",
        "            'values': [1]\n",
        "        },\n",
        "        'num_dec': {\n",
        "            'values': [1]\n",
        "        },\n",
        "        'hidden_size':{\n",
        "            'values': [32]\n",
        "        },\n",
        "        'cell_type': {\n",
        "            # 'values': ['RNN', 'GRU', 'LSTM']\n",
        "            'values': ['GRU']\n",
        "        },\n",
        "        'dec_search': {\n",
        "              'values': ['greedy']\n",
        "            # 'values': ['beam_search', 'greedy']\n",
        "        },\n",
        "        'beam_width':{\n",
        "            'values': [3]\n",
        "        },\n",
        "        'epochs': {\n",
        "            'values': [25]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "UbN4yF355jA9"
      },
      "id": "UbN4yF355jA9",
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, entity=\"cs21m010-cs21m041\", project=\"DL_Assignment_3_a\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTVK0bfI5kh2",
        "outputId": "1aadc80d-7fd7-46f1-ed5d-3792c6f91d77"
      },
      "id": "DTVK0bfI5kh2",
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: xdqx4ghp\n",
            "Sweep URL: https://wandb.ai/cs21m010-cs21m041/DL_Assignment_3_a/sweeps/xdqx4ghp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "\n",
        "    # Create a new WandB run\n",
        "    wandb.init(config=sweep_config)\n",
        "    \n",
        "    # Construct the run name\n",
        "    config = wandb.config\n",
        "    wandb.run.name = str(config.cell_type) + '_' + config.dec_search + '_bs_' + str(config.batch_size)\n",
        "    \n",
        "    model_rnn = CustomRNN(cell_type = config.cell_type, in_emb = config.in_emb, hidden_size=config.hidden_size,\n",
        "                  learning_rate= config.learning_rate, dropout=config.dropout,pred_type = config.dec_search,epochs = config.epochs,\n",
        "                  batch_size = config.batch_size, beam_width = config.beam_width, num_enc = config.num_enc,num_dec = config.num_dec)\n",
        "    \n",
        "    model_rnn.build_fit(encoder_input_data,decoder_input_data,decoder_target_data,x_test, y_test)"
      ],
      "metadata": {
        "id": "m5dHjntQ5mLF"
      },
      "id": "m5dHjntQ5mLF",
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, train, count = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D-X5sp-N5oa8",
        "outputId": "383e497b-4bf4-46e1-e7e1-badca211e6cd"
      },
      "id": "D-X5sp-N5oa8",
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 69i8ug7k with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_search: greedy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tin_emb: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_dec: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_enc: 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.15"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220422_163203-69i8ug7k</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/cs21m010-cs21m041/DL_Assignment_3_a/runs/69i8ug7k\" target=\"_blank\">northern-sweep-1</a></strong> to <a href=\"https://wandb.ai/cs21m010-cs21m041/DL_Assignment_3_a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/cs21m010-cs21m041/DL_Assignment_3_a/sweeps/xdqx4ghp\" target=\"_blank\">https://wandb.ai/cs21m010-cs21m041/DL_Assignment_3_a/sweeps/xdqx4ghp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Enc_inputs (InputLayer)        [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Dec_inputs (InputLayer)        [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Enc_emb (Embedding)            (None, None, 128)    8192        ['Enc_inputs[0][0]']             \n",
            "                                                                                                  \n",
            " Dec_emb (Embedding)            (None, None, 32)     928         ['Dec_inputs[0][0]']             \n",
            "                                                                                                  \n",
            " Enc_hidden_1 (GRU)             [(None, None, 32),   15552       ['Enc_emb[0][0]']                \n",
            "                                 (None, 32)]                                                      \n",
            "                                                                                                  \n",
            " Dec_hidden_1 (GRU)             [(None, None, 32),   6336        ['Dec_emb[0][0]',                \n",
            "                                 (None, 32)]                      'Enc_hidden_1[0][1]']           \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 29)     957         ['Dec_hidden_1[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 31,965\n",
            "Trainable params: 31,965\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "fitting the model....\n",
            "Epoch 1/25\n",
            "458/458 [==============================] - 48s 62ms/step - loss: 1.2230 - accuracy: 0.6936 - _timestamp: 1650645184.0000 - _runtime: 61.0000\n",
            "Epoch 2/25\n",
            "458/458 [==============================] - 28s 62ms/step - loss: 0.8383 - accuracy: 0.7462 - _timestamp: 1650645212.0000 - _runtime: 89.0000\n",
            "Epoch 3/25\n",
            "458/458 [==============================] - 34s 75ms/step - loss: 0.7601 - accuracy: 0.7647 - _timestamp: 1650645246.0000 - _runtime: 123.0000\n",
            "Epoch 4/25\n",
            "458/458 [==============================] - 30s 65ms/step - loss: 0.6972 - accuracy: 0.7817 - _timestamp: 1650645276.0000 - _runtime: 153.0000\n",
            "Epoch 5/25\n",
            "458/458 [==============================] - 28s 62ms/step - loss: 0.6499 - accuracy: 0.7950 - _timestamp: 1650645304.0000 - _runtime: 181.0000\n",
            "Epoch 6/25\n",
            "458/458 [==============================] - 28s 61ms/step - loss: 0.6116 - accuracy: 0.8054 - _timestamp: 1650645332.0000 - _runtime: 209.0000\n",
            "Epoch 7/25\n",
            "458/458 [==============================] - 28s 62ms/step - loss: 0.5811 - accuracy: 0.8141 - _timestamp: 1650645361.0000 - _runtime: 238.0000\n",
            "Epoch 8/25\n",
            "458/458 [==============================] - 28s 61ms/step - loss: 0.5569 - accuracy: 0.8209 - _timestamp: 1650645389.0000 - _runtime: 266.0000\n",
            "Epoch 9/25\n",
            "458/458 [==============================] - 28s 62ms/step - loss: 0.5378 - accuracy: 0.8262 - _timestamp: 1650645417.0000 - _runtime: 294.0000\n",
            "Epoch 10/25\n",
            "458/458 [==============================] - 28s 62ms/step - loss: 0.5216 - accuracy: 0.8307 - _timestamp: 1650645445.0000 - _runtime: 322.0000\n",
            "Epoch 11/25\n",
            "458/458 [==============================] - 28s 62ms/step - loss: 0.5076 - accuracy: 0.8343 - _timestamp: 1650645474.0000 - _runtime: 351.0000\n",
            "Epoch 12/25\n",
            "458/458 [==============================] - 28s 62ms/step - loss: 0.4955 - accuracy: 0.8381 - _timestamp: 1650645502.0000 - _runtime: 379.0000\n",
            "Epoch 13/25\n",
            "458/458 [==============================] - 28s 62ms/step - loss: 0.4847 - accuracy: 0.8416 - _timestamp: 1650645530.0000 - _runtime: 407.0000\n",
            "Epoch 14/25\n",
            "458/458 [==============================] - 28s 61ms/step - loss: 0.4739 - accuracy: 0.8448 - _timestamp: 1650645558.0000 - _runtime: 435.0000\n",
            "Epoch 15/25\n",
            "458/458 [==============================] - 28s 62ms/step - loss: 0.4646 - accuracy: 0.8477 - _timestamp: 1650645586.0000 - _runtime: 463.0000\n",
            "Epoch 16/25\n",
            "458/458 [==============================] - 29s 62ms/step - loss: 0.4559 - accuracy: 0.8505 - _timestamp: 1650645615.0000 - _runtime: 492.0000\n",
            "Epoch 17/25\n",
            "458/458 [==============================] - 28s 61ms/step - loss: 0.4480 - accuracy: 0.8531 - _timestamp: 1650645643.0000 - _runtime: 520.0000\n",
            "Epoch 18/25\n",
            "458/458 [==============================] - 28s 61ms/step - loss: 0.4414 - accuracy: 0.8555 - _timestamp: 1650645671.0000 - _runtime: 548.0000\n",
            "Epoch 19/25\n",
            "458/458 [==============================] - 28s 62ms/step - loss: 0.4348 - accuracy: 0.8576 - _timestamp: 1650645699.0000 - _runtime: 576.0000\n",
            "Epoch 20/25\n",
            "458/458 [==============================] - 28s 62ms/step - loss: 0.4299 - accuracy: 0.8590 - _timestamp: 1650645728.0000 - _runtime: 605.0000\n",
            "Epoch 21/25\n",
            "458/458 [==============================] - 28s 62ms/step - loss: 0.4247 - accuracy: 0.8608 - _timestamp: 1650645756.0000 - _runtime: 633.0000\n",
            "Epoch 22/25\n",
            "458/458 [==============================] - 28s 62ms/step - loss: 0.4205 - accuracy: 0.8622 - _timestamp: 1650645784.0000 - _runtime: 661.0000\n",
            "Epoch 23/25\n",
            "458/458 [==============================] - 28s 62ms/step - loss: 0.4164 - accuracy: 0.8636 - _timestamp: 1650645813.0000 - _runtime: 690.0000\n",
            "Epoch 24/25\n",
            "458/458 [==============================] - 34s 74ms/step - loss: 0.4129 - accuracy: 0.8647 - _timestamp: 1650645847.0000 - _runtime: 724.0000\n",
            "Epoch 25/25\n",
            "458/458 [==============================] - 28s 62ms/step - loss: 0.4090 - accuracy: 0.8660 - _timestamp: 1650645875.0000 - _runtime: 752.0000\n",
            "Inference model....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "assignment_3.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}