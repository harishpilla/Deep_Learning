{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e45c6f8d",
      "metadata": {
        "id": "e45c6f8d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import tensorflow \n",
        "import keras\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, GRU, Dropout, SimpleRNN\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from math import log"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install wandb -q\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj8H54q04eta",
        "outputId": "5f0629c9-78c1-4c41-82ad-ddea088793d6"
      },
      "id": "mj8H54q04eta",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8 MB 4.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 61.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 61.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "if not os.path.exists('/content/dakshina_dataset_v1.0.tar'):\n",
        "    !wget \"https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\""
      ],
      "metadata": {
        "id": "3RneRWbD3oCj",
        "outputId": "03820c3f-f0fd-4846-c182-e3c0bf775d61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3RneRWbD3oCj",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-22 14:53:45--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.189.128, 108.177.97.128, 108.177.125.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.189.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G   226MB/s    in 8.9s    \n",
            "\n",
            "2022-04-22 14:53:55 (216 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvf /content/dakshina_dataset_v1.0.tar"
      ],
      "metadata": {
        "id": "mDdPDwCy36Dg",
        "outputId": "162c1988-4ff5-4aaf-b3e6-fc3e922ba2b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mDdPDwCy36Dg",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dakshina_dataset_v1.0/bn/\n",
            "dakshina_dataset_v1.0/bn/lexicons/\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/romanized/\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/\n",
            "dakshina_dataset_v1.0/gu/lexicons/\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/romanized/\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/\n",
            "dakshina_dataset_v1.0/hi/lexicons/\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/romanized/\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/\n",
            "dakshina_dataset_v1.0/kn/lexicons/\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/kn/romanized/\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/\n",
            "dakshina_dataset_v1.0/ml/lexicons/\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.rom.txt\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/romanized/\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/\n",
            "dakshina_dataset_v1.0/mr/lexicons/\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/romanized/\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/\n",
            "dakshina_dataset_v1.0/pa/lexicons/\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/pa/romanized/\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/\n",
            "dakshina_dataset_v1.0/sd/lexicons/\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/sd/romanized/\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/\n",
            "dakshina_dataset_v1.0/si/lexicons/\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/romanized/\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/\n",
            "dakshina_dataset_v1.0/ta/lexicons/\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/romanized/\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/\n",
            "dakshina_dataset_v1.0/te/lexicons/\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/romanized/\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ur/\n",
            "dakshina_dataset_v1.0/ur/lexicons/\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/romanized/\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_path = '/content/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv'\n",
        "validation_data_path = '/content/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv'\n",
        "test_data_path = '/content/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv'\n",
        "\n",
        "df_train = pd.read_csv(train_data_path, sep='\\t', header=None)\n",
        "df_val = pd.read_csv(validation_data_path, sep='\\t', header=None)\n",
        "df_test = pd.read_csv(test_data_path, sep='\\t', header=None)"
      ],
      "metadata": {
        "id": "pmx3VVhpURl_"
      },
      "id": "pmx3VVhpURl_",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_path = '/content/predictions.tsv'"
      ],
      "metadata": {
        "id": "8RhaIQ1o5OlO"
      },
      "id": "8RhaIQ1o5OlO",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(df, input_texts, target_texts, input_characters=None, target_characters=None, is_test_data=False):\n",
        "    \n",
        "    for  _, row in df.iterrows():\n",
        "        input_text, target_text = str(row[0]), str(row[1])\n",
        "        input_texts.append(input_text)\n",
        "        target_text = '\\t' + target_text + '\\n'\n",
        "        target_texts.append(target_text)\n",
        "        \n",
        "        if not is_test_data:\n",
        "            for char in input_text:\n",
        "                if char not in input_characters:\n",
        "                    input_characters.add(char)\n",
        "\n",
        "            for char in target_text:\n",
        "                if char not in target_characters:\n",
        "                    target_characters.add(char)\n",
        "\n",
        "    max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "    max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "    if not is_test_data:\n",
        "        input_characters = sorted(list(input_characters))\n",
        "        target_characters = sorted(list(target_characters))\n",
        "        num_encoder_tokens = len(input_characters)\n",
        "        num_decoder_tokens = len(target_characters)\n",
        "\n",
        "        return max_encoder_seq_length, max_decoder_seq_length, num_encoder_tokens, num_decoder_tokens\n",
        "    \n",
        "    return max_encoder_seq_length, max_decoder_seq_length"
      ],
      "metadata": {
        "id": "dBL7HWehFkBn"
      },
      "id": "dBL7HWehFkBn",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vectors(input_texts, target_texts, input_token_index, target_token_index,\n",
        "                max_encoder_seq_length, num_encoder_tokens, \n",
        "                max_decoder_seq_length=None, num_decoder_tokens=None,\n",
        "                is_test_data=False):\n",
        "      \n",
        "    encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length), dtype=\"float32\")\n",
        "\n",
        "    if not is_test_data:\n",
        "        decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length), dtype=\"float32\")\n",
        "        decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "        for t, char in enumerate(input_text):\n",
        "            encoder_input_data[i, t] = input_token_index[char]\n",
        "        encoder_input_data[i, t + 1 :] = input_token_index[\" \"]\n",
        "\n",
        "\n",
        "        if not is_test_data:\n",
        "            for t, char in enumerate(target_text):\n",
        "                decoder_input_data[i, t] = target_token_index[char]\n",
        "                if t > 0:\n",
        "                    # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "                    # decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "                    decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "            decoder_input_data[i, t + 1: ] = target_token_index[\" \"]\n",
        "            decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0    \n",
        "\n",
        "    if is_test_data:\n",
        "        return encoder_input_data\n",
        "\n",
        "    return encoder_input_data, decoder_input_data, decoder_target_data"
      ],
      "metadata": {
        "id": "58JABACiLR7G"
      },
      "id": "58JABACiLR7G",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Train, Validation and Test data"
      ],
      "metadata": {
        "id": "qtrqWffEJace"
      },
      "id": "qtrqWffEJace"
    },
    {
      "cell_type": "code",
      "source": [
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "input_characters = set(' ')\n",
        "target_characters = set(' ')\n",
        "\n",
        "max_encoder_seq_length, max_decoder_seq_length, num_encoder_tokens, num_decoder_tokens = load_data(df_train, input_texts, target_texts, input_characters, target_characters)\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)"
      ],
      "metadata": {
        "id": "XwhSrjplHqoc",
        "outputId": "bd523c2d-7e42-43b8-afc9-8bced614bce4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "XwhSrjplHqoc",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 58550\n",
            "Number of unique input tokens: 64\n",
            "Number of unique output tokens: 29\n",
            "Max sequence length for inputs: 20\n",
            "Max sequence length for outputs: 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_input_texts = []\n",
        "val_target_texts = []\n",
        "\n",
        "val_max_encoder_seq_length, val_max_decoder_seq_length, val_num_encoder_tokens, val_num_decoder_tokens = load_data(\n",
        "    df_val, val_input_texts, val_target_texts, input_characters, target_characters)\n",
        "\n",
        "print(\"Number of samples:\", len(val_input_texts))\n",
        "print(\"Number of unique input tokens:\", val_num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", val_num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", val_max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", val_max_decoder_seq_length)"
      ],
      "metadata": {
        "id": "I97sHjSyIjba",
        "outputId": "fea856c8-13c8-4279-bce1-b5364db73797",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "I97sHjSyIjba",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 5683\n",
            "Number of unique input tokens: 64\n",
            "Number of unique output tokens: 29\n",
            "Max sequence length for inputs: 19\n",
            "Max sequence length for outputs: 23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_texts = []\n",
        "test_target_texts = []\n",
        "\n",
        "test_max_encoder_seq_length, test_max_decoder_seq_length = load_data(df_test, test_input_texts, test_target_texts, is_test_data=True)\n",
        "\n",
        "print(\"Number of Test samples:\", len(test_input_texts))\n",
        "print(\"Test Max sequence length for inputs:\", test_max_encoder_seq_length)\n",
        "print(\"Test Max sequence length for outputs:\", test_max_decoder_seq_length)"
      ],
      "metadata": {
        "id": "FI3C6mc2Jnk8",
        "outputId": "096d3017-97d1-44cc-b3f2-a4b7907f9438",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "FI3C6mc2Jnk8",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Test samples: 5747\n",
            "Test Max sequence length for inputs: 18\n",
            "Test Max sequence length for outputs: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])"
      ],
      "metadata": {
        "id": "zR4X9PmLhR53"
      },
      "id": "zR4X9PmLhR53",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
      ],
      "metadata": {
        "id": "myxcPh_f48Zd"
      },
      "id": "myxcPh_f48Zd",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder_input_data, decoder_input_data, decoder_target_data = get_vectors(\n",
        "#     input_texts, target_texts, input_token_index, target_token_index, \n",
        "#     max_encoder_seq_length, num_encoder_tokens, max_decoder_seq_length,\n",
        "#     num_decoder_tokens)"
      ],
      "metadata": {
        "id": "KIj198FFPahj"
      },
      "id": "KIj198FFPahj",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# val_encoder_input_data, val_decoder_input_data, val_decoder_target_data = get_vectors(\n",
        "#     val_input_texts, val_target_texts, input_token_index, target_token_index,\n",
        "#     val_max_encoder_seq_length, val_num_encoder_tokens, \n",
        "#     val_max_decoder_seq_length, val_num_decoder_tokens)"
      ],
      "metadata": {
        "id": "IRzK9ts5Ozof"
      },
      "id": "IRzK9ts5Ozof",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_encoder_input_data = get_vectors(\n",
        "#     test_input_texts, test_target_texts, input_token_index, target_token_index,\n",
        "#     test_max_encoder_seq_length, num_encoder_tokens, is_test_data=True)"
      ],
      "metadata": {
        "id": "Dc-0MfdiRkTE"
      },
      "id": "Dc-0MfdiRkTE",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t] = input_token_index[char]\n",
        "    #encoder_input_data[i, t + 1 :] = input_token_index[\"P\"]\n",
        "    encoder_input_data[i, t + 1 :] = input_token_index[\" \"]\n",
        "\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    #decoder_input_data[i, t + 1: ] = target_token_index[\"P\"]\n",
        "    decoder_input_data[i, t + 1: ] = target_token_index[\" \"]\n",
        "    #decoder_target_data[i, t:, target_token_index[\"P\"]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "\n",
        "val_encoder_input_data = np.zeros(\n",
        "    (len(input_texts), val_max_encoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "val_decoder_input_data = np.zeros(\n",
        "    (len(input_texts), val_max_decoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "val_decoder_target_data = np.zeros(\n",
        "    (len(input_texts), val_max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(val_input_texts, val_target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        val_encoder_input_data[i, t] = input_token_index[char]\n",
        "    #encoder_input_data[i, t + 1 :] = input_token_index[\"P\"]\n",
        "    val_encoder_input_data[i, t + 1 :] = input_token_index[\" \"]\n",
        "\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        val_decoder_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            val_decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    #decoder_input_data[i, t + 1: ] = target_token_index[\"P\"]\n",
        "    val_decoder_input_data[i, t + 1: ] = target_token_index[\" \"]\n",
        "    #decoder_target_data[i, t:, target_token_index[\"P\"]] = 1.0\n",
        "    val_decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
      ],
      "metadata": {
        "id": "Ko2nsRcK4_6D"
      },
      "id": "Ko2nsRcK4_6D",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_token_index)\n",
        "print(target_token_index)\n",
        "print(reverse_input_char_index)\n",
        "print(reverse_target_char_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW1Y7QDj5B5H",
        "outputId": "e0f0333c-f52c-4f7f-8618-a690ef17de73"
      },
      "id": "XW1Y7QDj5B5H",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'త': 0, '\\u200c': 1, 'ో': 2, 'ఎ': 3, 'ర': 4, 'ణ': 5, 'ి': 6, 'ౌ': 7, 'ఏ': 8, 'థ': 9, 'ఘ': 10, 'ఱ': 11, 'ఆ': 12, 'ఐ': 13, 'ఈ': 14, 'చ': 15, 'ూ': 16, 'ఋ': 17, 'ఊ': 18, 'ఔ': 19, 'గ': 20, 'ష': 21, 'హ': 22, 'ఞ': 23, 'ఒ': 24, 'ు': 25, 'ఇ': 26, 'అ': 27, 'య': 28, 'ళ': 29, 'జ': 30, 'మ': 31, 'ీ': 32, 'ధ': 33, 'ఛ': 34, 'ొ': 35, 'ఓ': 36, 'ే': 37, 'ట': 38, 'ృ': 39, 'శ': 40, 'క': 41, 'ః': 42, 'ల': 43, 'ప': 44, 'ె': 45, 'ై': 46, 'బ': 47, 'ా': 48, ' ': 49, 'ం': 50, 'స': 51, 'ద': 52, 'ఝ': 53, 'ఖ': 54, 'ఉ': 55, '్': 56, 'భ': 57, 'ఫ': 58, 'ఢ': 59, 'న': 60, 'వ': 61, 'ఠ': 62, 'డ': 63}\n",
            "{'k': 0, 's': 1, 'l': 2, 'v': 3, 'n': 4, 'g': 5, 'y': 6, 'o': 7, 'z': 8, 'a': 9, '\\n': 10, 'h': 11, 'u': 12, 'j': 13, 'p': 14, '\\t': 15, 'd': 16, 'w': 17, 'm': 18, 'i': 19, 't': 20, 'e': 21, 'x': 22, 'b': 23, 'f': 24, ' ': 25, 'c': 26, 'r': 27, 'q': 28}\n",
            "{0: 'త', 1: '\\u200c', 2: 'ో', 3: 'ఎ', 4: 'ర', 5: 'ణ', 6: 'ి', 7: 'ౌ', 8: 'ఏ', 9: 'థ', 10: 'ఘ', 11: 'ఱ', 12: 'ఆ', 13: 'ఐ', 14: 'ఈ', 15: 'చ', 16: 'ూ', 17: 'ఋ', 18: 'ఊ', 19: 'ఔ', 20: 'గ', 21: 'ష', 22: 'హ', 23: 'ఞ', 24: 'ఒ', 25: 'ు', 26: 'ఇ', 27: 'అ', 28: 'య', 29: 'ళ', 30: 'జ', 31: 'మ', 32: 'ీ', 33: 'ధ', 34: 'ఛ', 35: 'ొ', 36: 'ఓ', 37: 'ే', 38: 'ట', 39: 'ృ', 40: 'శ', 41: 'క', 42: 'ః', 43: 'ల', 44: 'ప', 45: 'ె', 46: 'ై', 47: 'బ', 48: 'ా', 49: ' ', 50: 'ం', 51: 'స', 52: 'ద', 53: 'ఝ', 54: 'ఖ', 55: 'ఉ', 56: '్', 57: 'భ', 58: 'ఫ', 59: 'ఢ', 60: 'న', 61: 'వ', 62: 'ఠ', 63: 'డ'}\n",
            "{0: 'k', 1: 's', 2: 'l', 3: 'v', 4: 'n', 5: 'g', 6: 'y', 7: 'o', 8: 'z', 9: 'a', 10: '\\n', 11: 'h', 12: 'u', 13: 'j', 14: 'p', 15: '\\t', 16: 'd', 17: 'w', 18: 'm', 19: 'i', 20: 't', 21: 'e', 22: 'x', 23: 'b', 24: 'f', 25: ' ', 26: 'c', 27: 'r', 28: 'q'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test, y_test = val_encoder_input_data, val_target_texts"
      ],
      "metadata": {
        "id": "E6KLsqAW5DcP"
      },
      "id": "E6KLsqAW5DcP",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(predictions_path, 'w') as f:\n",
        "    f.write('Telugu\\tPredicted\\tActual\\n')"
      ],
      "metadata": {
        "id": "yFWuPo9g5IJh"
      },
      "id": "yFWuPo9g5IJh",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Model"
      ],
      "metadata": {
        "id": "dMhX6Gga5UKF"
      },
      "id": "dMhX6Gga5UKF"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import tensorflow\n",
        "import keras\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, GRU, Dropout, SimpleRNN\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from math import log\n",
        "\n",
        "\n",
        "class CustomRNN(object):\n",
        "\n",
        "    def __init__(self, cell_type='GRU', in_emb=32, hidden_size=32, learning_rate=1e-3,\n",
        "                 dropout=0.4, pred_type='greedy', epochs=10, batch_size=32, beam_width=5,\n",
        "                 num_enc=1, num_dec=1):\n",
        "\n",
        "        self.cell_type = cell_type\n",
        "        self.in_emb = in_emb\n",
        "        self.hidden_size = hidden_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.dropout = dropout\n",
        "        self.pred_type = pred_type\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.beam_width = beam_width\n",
        "        self.num_enc = num_enc\n",
        "        self.num_dec = num_dec\n",
        "\n",
        "    def build_fit(self, encoder_input_data, decoder_input_data, decoder_target_data, x_test, y_test):\n",
        "\n",
        "        # Define an input sequence and process it.\n",
        "        encoder_inputs = Input(shape=(None,), name='Enc_inputs')\n",
        "\n",
        "        # Add an Embedding layer expecting input vocab of size num_encoder_tokens, and output embedding dimension of size in_enc.\n",
        "        enc_emb = Embedding(num_encoder_tokens, self.in_emb, mask_zero=True, name='Enc_emb')(encoder_inputs)\n",
        "        encoder_outputs = enc_emb\n",
        "\n",
        "        customFunction = None\n",
        "        if self.cell_type == 'LSTM':\n",
        "            customFunction = LSTM\n",
        "        elif self.cell_type == 'GRU':\n",
        "            customFunction = GRU\n",
        "        elif self.cell_type == 'RNN':\n",
        "            customFunction = SimpleRNN\n",
        "\n",
        "        encoder_lstm = customFunction(self.hidden_size, return_state=True, dropout=self.dropout, return_sequences=True,\n",
        "                                      name=\"Enc_hidden_1\")\n",
        "        if self.cell_type == 'LSTM':\n",
        "            encoder_outputs, state_h, state_c = encoder_lstm(encoder_outputs)\n",
        "            encoder_states = [state_h, state_c]\n",
        "        else:\n",
        "            encoder_outputs, state_h = encoder_lstm(encoder_outputs)\n",
        "            encoder_states = [state_h]\n",
        "\n",
        "        # Add a LSTM layer with hidden_size internal units.\n",
        "        for i in range(2, self.num_enc + 1):\n",
        "            layer_name = ('Enc_hidden_%d') % i\n",
        "            encoder_lstm = customFunction(self.hidden_size, return_state=True, dropout=self.dropout,\n",
        "                                          return_sequences=True, name=layer_name)\n",
        "            if self.cell_type == 'LSTM':\n",
        "                encoder_outputs, state_h, state_c = encoder_lstm(encoder_outputs, initial_state=encoder_states)\n",
        "                encoder_states = [state_h, state_c]\n",
        "            else:\n",
        "                encoder_outputs, state_h = encoder_lstm(encoder_outputs, initial_state=encoder_states)\n",
        "                encoder_states = [state_h]\n",
        "\n",
        "        # Set up the decoder, using `encoder_states` as initial state.\n",
        "        decoder_inputs = Input(shape=(None,), name='Dec_inputs')\n",
        "        dec_emb_layer = Embedding(num_decoder_tokens, self.hidden_size, mask_zero=True, name='Dec_emb')\n",
        "        dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "        # We set up our decoder to return full output sequences, and to return internal states as well.\n",
        "        # We don't use the return states in the training model, but we will use them in inference.\n",
        "        decoder_outputs = dec_emb\n",
        "\n",
        "        decoder_lstm = customFunction(self.hidden_size, return_sequences=True, return_state=True, dropout=self.dropout,\n",
        "                                      name=\"Dec_hidden_1\")\n",
        "        if self.cell_type == 'LSTM':\n",
        "            decoder_outputs, _, _ = decoder_lstm(decoder_outputs, initial_state=encoder_states)\n",
        "        else:\n",
        "            decoder_outputs, _ = decoder_lstm(decoder_outputs, initial_state=encoder_states)\n",
        "\n",
        "        for i in range(2, self.num_dec + 1):\n",
        "            layer_name = ('Dec_hidden_%d') % i\n",
        "            decoder_lstm = customFunction(self.hidden_size, return_sequences=True, return_state=True,\n",
        "                                          dropout=self.dropout, name=layer_name)\n",
        "            if self.cell_type == 'LSTM':\n",
        "                decoder_outputs, _, _ = decoder_lstm(decoder_outputs, initial_state=encoder_states)\n",
        "            else:\n",
        "                decoder_outputs, _ = decoder_lstm(decoder_outputs, initial_state=encoder_states)\n",
        "\n",
        "        decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='dense')\n",
        "        decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "        # get the model and print summary\n",
        "        model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "        model.summary()\n",
        "\n",
        "        # Define the optimizer\n",
        "        optimizer = Adam(learning_rate=self.learning_rate, beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "        # compiling and fitting the model\n",
        "        print('fitting the model....')\n",
        "        model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n",
        "        model.fit(\n",
        "            [encoder_input_data, decoder_input_data],\n",
        "            decoder_target_data,\n",
        "            batch_size=self.batch_size,\n",
        "            epochs=self.epochs,\n",
        "            callbacks=[WandbCallback()]\n",
        "        )\n",
        "\n",
        "        print('Inference model....')\n",
        "        encoder_model, decoder_model = self.inference_model(model)\n",
        "        self.get_accuracy(encoder_model, decoder_model, x_test, y_test)\n",
        "\n",
        "    def get_accuracy(self, encoder_model, decoder_model, x_test, y_test):\n",
        "        total_no_of_samples = 0\n",
        "        no_of_correct_predictions = 0\n",
        "        for i in range(len(val_input_texts)):\n",
        "            input_seq = x_test[i: i + 1]\n",
        "            predicted_sequence = self.decode_sequence(encoder_model, decoder_model, input_seq)\n",
        "            predicted_sequence = predicted_sequence[0:len(predicted_sequence) - 1]\n",
        "\n",
        "            actual_sequence = y_test[i]\n",
        "            actual_sequence = actual_sequence[1:len(actual_sequence) - 1]\n",
        "\n",
        "            # write predictions to a file\n",
        "            with open(predictions_path, 'a') as f:\n",
        "                f.write('{}\\t{}\\t{}\\n'.format(df_val[0][i], predicted_sequence.strip(), actual_sequence.strip()))\n",
        "\n",
        "            if predicted_sequence.strip() == actual_sequence.strip():\n",
        "                no_of_correct_predictions += 1\n",
        "            total_no_of_samples += 1\n",
        "\n",
        "            # accuracy_epoch = no_of_correct_predictions / total_no_of_samples\n",
        "            # if total_no_of_samples % 50 == 0:\n",
        "            #     wandb.log({'epoch_accuracy': accuracy_epoch})\n",
        "\n",
        "        val_accuracy = no_of_correct_predictions / total_no_of_samples\n",
        "        print(val_accuracy)\n",
        "        wandb.log({'val_accuracy': val_accuracy})\n",
        "\n",
        "    def inference_model(self, model):\n",
        "        encoder_inputs = model.input[0]\n",
        "        encoder_states = None\n",
        "        if self.cell_type == 'RNN' or self.cell_type == 'GRU':\n",
        "            encoder_outputs, state_h_enc = model.get_layer('Enc_hidden_' + str(self.num_enc)).output\n",
        "            encoder_states = [state_h_enc]\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            encoder_outputs, state_h_enc, state_c_enc = model.get_layer('Enc_hidden_' + str(self.num_enc)).output\n",
        "            encoder_states = [state_h_enc, state_c_enc]\n",
        "        encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "        decoder_inputs = model.input[1]\n",
        "        decoder_outputs = model.get_layer('Dec_emb')(decoder_inputs)\n",
        "        decoder_states_inputs = []\n",
        "        decoder_states = []\n",
        "\n",
        "        for i in range(1, self.num_dec + 1):\n",
        "            decoder = model.get_layer('Dec_hidden_' + str(i))\n",
        "            if self.cell_type == 'RNN' or self.cell_type == 'GRU':\n",
        "                decoder_state_input_h = keras.Input(shape=(self.hidden_size,))\n",
        "                curr_states_inputs = [decoder_state_input_h]\n",
        "                decoder_outputs, state_h_dec = decoder(decoder_outputs, initial_state=curr_states_inputs)\n",
        "                decoder_states += [state_h_dec]\n",
        "            elif self.cell_type == 'LSTM':\n",
        "                decoder_state_input_h = keras.Input(shape=(self.hidden_size,))\n",
        "                decoder_state_input_c = keras.Input(shape=(self.hidden_size,))\n",
        "                curr_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "                decoder_outputs, state_h_dec, state_c_dec = decoder(decoder_outputs, initial_state=curr_states_inputs)\n",
        "                decoder_states += [state_h_dec, state_c_dec]\n",
        "            decoder_states_inputs += curr_states_inputs\n",
        "\n",
        "        decoder_dense = model.get_layer('dense')\n",
        "        decoder_outputs = decoder_dense(decoder_outputs)\n",
        "        decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "        return encoder_model, decoder_model\n",
        "\n",
        "    def decode_sequence(self, encoder_model, decoder_model, input_seq):\n",
        "        # Encode the input as state vectors.\n",
        "        states_value = [encoder_model.predict(input_seq)] * self.num_dec\n",
        "\n",
        "        # Generate empty target sequence of length 1.\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        # Populate the first character of target sequence with the start character.\n",
        "        target_seq[0, 0] = target_token_index['b']\n",
        "\n",
        "        # Sampling loop for a batch of sequences\n",
        "        # (to simplify, here we assume a batch of size 1).\n",
        "        stop_condition = False\n",
        "        decoded_sentence = \"\"\n",
        "\n",
        "        while not stop_condition:\n",
        "            if self.cell_type == 'RNN' or self.cell_type == 'GRU':\n",
        "                dummy = decoder_model.predict([target_seq] + [states_value])\n",
        "                output_tokens, states_value = dummy[0], dummy[1:]\n",
        "\n",
        "            elif self.cell_type == 'LSTM':\n",
        "                dummy = decoder_model.predict([target_seq] + states_value)\n",
        "                output_tokens, states_value = dummy[0], dummy[1:]\n",
        "\n",
        "            # print(output_tokens[0,:,:])\n",
        "            if self.pred_type == 'greedy':\n",
        "                beam_w = 1\n",
        "            elif self.pred_type == 'beam_search':\n",
        "                beam_w = self.beam_width\n",
        "            sampled_token_index = self.beam_search_decoder(output_tokens[0, :, :], beam_w)\n",
        "            sampled_token_index = sampled_token_index[beam_w - 1][0]\n",
        "\n",
        "            # Sample a token\n",
        "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "            sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "            decoded_sentence += sampled_char\n",
        "\n",
        "            # Exit condition: either hit max length\n",
        "            # or find stop character.\n",
        "            if sampled_char == 'E' or len(decoded_sentence) > max_decoder_seq_length:\n",
        "                stop_condition = True\n",
        "\n",
        "            # Update the target sequence (of length 1).\n",
        "            target_seq = np.zeros((1, 1))\n",
        "            target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "            # Update state\n",
        "\n",
        "        return decoded_sentence\n",
        "\n",
        "    def beam_search_decoder(self, data, k):\n",
        "        sequences = [[list(), 0.0]]\n",
        "        # walk over each step in sequence\n",
        "        for row in data:\n",
        "            all_candidates = list()\n",
        "            # expand each current candidate\n",
        "            for i in range(len(sequences)):\n",
        "                seq, score = sequences[i]\n",
        "                for j in range(len(row)):\n",
        "                    candidate = [seq + [j], score - log(row[j])]\n",
        "                    # candidate = [seq + [j], score - log1p(row[j])]\n",
        "                    all_candidates.append(candidate)\n",
        "            # order all candidates by score\n",
        "            ordered = sorted(all_candidates, key=lambda tup: tup[1])\n",
        "            # select k best\n",
        "            sequences = ordered[:k]\n",
        "        return sequences"
      ],
      "metadata": {
        "id": "Z1Ya6JFE5VkE"
      },
      "id": "Z1Ya6JFE5VkE",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sweep Config"
      ],
      "metadata": {
        "id": "onxvU8-85g3P"
      },
      "id": "onxvU8-85g3P"
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes', \n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "\n",
        "        'dropout': {\n",
        "            'values': [0.0, 0.1, 0.2]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-3, 1e-4]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [64, 128]\n",
        "        },\n",
        "        'in_emb': {\n",
        "            'values': [32, 64, 128]\n",
        "        },\n",
        "        'num_enc': {\n",
        "            'values': [1]\n",
        "        },\n",
        "        'num_dec': {\n",
        "            'values': [1]\n",
        "        },\n",
        "        'hidden_size':{\n",
        "            'values': [32]\n",
        "        },\n",
        "        'cell_type': {\n",
        "            # 'values': ['RNN', 'GRU', 'LSTM']\n",
        "            'values': ['GRU']\n",
        "        },\n",
        "        'dec_search': {\n",
        "              'values': ['greedy']\n",
        "            # 'values': ['beam_search', 'greedy']\n",
        "        },\n",
        "        'beam_width':{\n",
        "            'values': [1]\n",
        "        },\n",
        "        'epochs': {\n",
        "            'values': [10]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "UbN4yF355jA9"
      },
      "id": "UbN4yF355jA9",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, entity=\"cs21m010-cs21m041\", project=\"DL_Assignment_3_a\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "DTVK0bfI5kh2",
        "outputId": "939351eb-000b-4d82-f6a2-c2b42ec9b793"
      },
      "id": "DTVK0bfI5kh2",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: hm144054\n",
            "Sweep URL: https://wandb.ai/cs21m010-cs21m041/DL_Assignment_3_a/sweeps/hm144054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "\n",
        "    # Create a new WandB run\n",
        "    wandb.init(config=sweep_config)\n",
        "    \n",
        "    # Construct the run name\n",
        "    config = wandb.config\n",
        "    wandb.run.name = str(config.cell_type) + '_' + config.dec_search + '_bs_' + str(config.batch_size)\n",
        "    \n",
        "    model_rnn = CustomRNN(cell_type = config.cell_type, in_emb = config.in_emb, hidden_size=config.hidden_size,\n",
        "                  learning_rate= config.learning_rate, dropout=config.dropout,pred_type = config.dec_search,epochs = config.epochs,\n",
        "                  batch_size = config.batch_size, beam_width = config.beam_width, num_enc = config.num_enc,num_dec = config.num_dec)\n",
        "    \n",
        "    model_rnn.build_fit(encoder_input_data,decoder_input_data,decoder_target_data,x_test, y_test)"
      ],
      "metadata": {
        "id": "m5dHjntQ5mLF"
      },
      "id": "m5dHjntQ5mLF",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, train, count = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D-X5sp-N5oa8",
        "outputId": "beee471a-4775-4e44-d0f8-4d4771c28ff7"
      },
      "id": "D-X5sp-N5oa8",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ras6g1t6 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_search: greedy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tin_emb: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_dec: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_enc: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs21m010-cs21m041\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.15"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220422_145423-ras6g1t6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/cs21m010-cs21m041/DL_Assignment_3_a/runs/ras6g1t6\" target=\"_blank\">swift-sweep-1</a></strong> to <a href=\"https://wandb.ai/cs21m010-cs21m041/DL_Assignment_3_a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/cs21m010-cs21m041/DL_Assignment_3_a/sweeps/hm144054\" target=\"_blank\">https://wandb.ai/cs21m010-cs21m041/DL_Assignment_3_a/sweeps/hm144054</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Enc_inputs (InputLayer)        [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Dec_inputs (InputLayer)        [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Enc_emb (Embedding)            (None, None, 128)    8192        ['Enc_inputs[0][0]']             \n",
            "                                                                                                  \n",
            " Dec_emb (Embedding)            (None, None, 32)     928         ['Dec_inputs[0][0]']             \n",
            "                                                                                                  \n",
            " Enc_hidden_1 (GRU)             [(None, None, 32),   15552       ['Enc_emb[0][0]']                \n",
            "                                 (None, 32)]                                                      \n",
            "                                                                                                  \n",
            " Dec_hidden_1 (GRU)             [(None, None, 32),   6336        ['Dec_emb[0][0]',                \n",
            "                                 (None, 32)]                      'Enc_hidden_1[0][1]']           \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 29)     957         ['Dec_hidden_1[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 31,965\n",
            "Trainable params: 31,965\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "fitting the model....\n",
            "Epoch 1/10\n",
            "458/458 [==============================] - 38s 63ms/step - loss: 1.2340 - accuracy: 0.6898 - _timestamp: 1650639311.0000 - _runtime: 48.0000\n",
            "Epoch 2/10\n",
            "458/458 [==============================] - 28s 61ms/step - loss: 0.8498 - accuracy: 0.7432 - _timestamp: 1650639339.0000 - _runtime: 76.0000\n",
            "Epoch 3/10\n",
            "458/458 [==============================] - 30s 65ms/step - loss: 0.7981 - accuracy: 0.7521 - _timestamp: 1650639369.0000 - _runtime: 106.0000\n",
            "Epoch 4/10\n",
            "458/458 [==============================] - 28s 62ms/step - loss: 0.7665 - accuracy: 0.7602 - _timestamp: 1650639397.0000 - _runtime: 134.0000\n",
            "Epoch 5/10\n",
            "458/458 [==============================] - 28s 61ms/step - loss: 0.7331 - accuracy: 0.7694 - _timestamp: 1650639426.0000 - _runtime: 163.0000\n",
            "Epoch 6/10\n",
            "458/458 [==============================] - 28s 62ms/step - loss: 0.6960 - accuracy: 0.7818 - _timestamp: 1650639454.0000 - _runtime: 191.0000\n",
            "Epoch 7/10\n",
            "458/458 [==============================] - 28s 62ms/step - loss: 0.6592 - accuracy: 0.7927 - _timestamp: 1650639482.0000 - _runtime: 219.0000\n",
            "Epoch 8/10\n",
            "458/458 [==============================] - 28s 61ms/step - loss: 0.6242 - accuracy: 0.8034 - _timestamp: 1650639510.0000 - _runtime: 247.0000\n",
            "Epoch 9/10\n",
            "458/458 [==============================] - 28s 61ms/step - loss: 0.5958 - accuracy: 0.8124 - _timestamp: 1650639538.0000 - _runtime: 275.0000\n",
            "Epoch 10/10\n",
            "458/458 [==============================] - 28s 61ms/step - loss: 0.5716 - accuracy: 0.8196 - _timestamp: 1650639566.0000 - _runtime: 303.0000\n",
            "Inference model....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "assignment_3.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}