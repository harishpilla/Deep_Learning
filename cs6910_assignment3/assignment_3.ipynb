{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e45c6f8d",
      "metadata": {
        "id": "e45c6f8d"
      },
      "outputs": [],
      "source": [
        "# Import the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "import random\n",
        "from math import log\n",
        "\n",
        "import tensorflow \n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import LSTM, SimpleRNN, GRU, Embedding, Dense, TimeDistributed, Concatenate, AdditiveAttention "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install wandb and wandbcallback libraries\n",
        "%pip install wandb -q\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "metadata": {
        "id": "mj8H54q04eta"
      },
      "id": "mj8H54q04eta",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Dataset"
      ],
      "metadata": {
        "id": "zh1YGuVtC5Jw"
      },
      "id": "zh1YGuVtC5Jw"
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "if not os.path.exists('/content/dakshina_dataset_v1.0.tar'):\n",
        "    !wget \"https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\""
      ],
      "metadata": {
        "id": "3RneRWbD3oCj"
      },
      "id": "3RneRWbD3oCj",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip every file\n",
        "!tar -xvf /content/dakshina_dataset_v1.0.tar"
      ],
      "metadata": {
        "id": "mDdPDwCy36Dg",
        "outputId": "7bf5f568-a59f-4e87-e6f7-1ea1d93d93e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mDdPDwCy36Dg",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dakshina_dataset_v1.0/bn/\n",
            "dakshina_dataset_v1.0/bn/lexicons/\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/romanized/\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/\n",
            "dakshina_dataset_v1.0/gu/lexicons/\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/romanized/\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/\n",
            "dakshina_dataset_v1.0/hi/lexicons/\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/romanized/\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/\n",
            "dakshina_dataset_v1.0/kn/lexicons/\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/kn/romanized/\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/\n",
            "dakshina_dataset_v1.0/ml/lexicons/\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.rom.txt\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/romanized/\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/\n",
            "dakshina_dataset_v1.0/mr/lexicons/\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/romanized/\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/\n",
            "dakshina_dataset_v1.0/pa/lexicons/\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/pa/romanized/\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/\n",
            "dakshina_dataset_v1.0/sd/lexicons/\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/sd/romanized/\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/\n",
            "dakshina_dataset_v1.0/si/lexicons/\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/romanized/\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/\n",
            "dakshina_dataset_v1.0/ta/lexicons/\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/romanized/\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/\n",
            "dakshina_dataset_v1.0/te/lexicons/\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/romanized/\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ur/\n",
            "dakshina_dataset_v1.0/ur/lexicons/\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/romanized/\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training dataset file\n",
        "train_file = open(\"/content/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\")\n",
        "read_train_file = csv.reader(train_file, delimiter=\"\\t\")\n",
        "\n",
        "# Load the validation dataset file\n",
        "val_file = open(\"/content/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\")\n",
        "read_val_file = csv.reader(val_file, delimiter=\"\\t\")\n",
        "\n",
        "# Load the test dataset file\n",
        "test_file = open(\"/content/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\")\n",
        "read_test_file = csv.reader(test_file, delimiter=\"\\t\")\n",
        "\n",
        "# Path to save the predictions file\n",
        "predictions_path = 'predictions.tsv'"
      ],
      "metadata": {
        "id": "pmx3VVhpURl_"
      },
      "id": "pmx3VVhpURl_",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess and Encode Data"
      ],
      "metadata": {
        "id": "SMOaU6RvCyif"
      },
      "id": "SMOaU6RvCyif"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all the input words(English) and target words(Telugu) in Training dataset\n",
        "telugu_words = []\n",
        "english_words = []\n",
        "\n",
        "for i in read_train_file:   \n",
        "    telugu_words.append(\"\\t\" + str(i[0]) + \"\\n\")\n",
        "    english_words.append(str(i[1]))\n",
        "\n",
        "# Get all the input words(English) and target words(Telugu) in Validation dataset\n",
        "val_telugu_words = []\n",
        "val_english_words = []\n",
        "\n",
        "for i in read_val_file:\n",
        "    val_telugu_words.append(\"\\t\" + str(i[0]) + \"\\n\")\n",
        "    val_english_words.append(str(i[1]))\n",
        "\n",
        "\n",
        "# Get all the input words(English) and target words(Telugu) in Test dataset\n",
        "test_telugu_words = []\n",
        "test_english_words = []\n",
        "\n",
        "for i in read_test_file:\n",
        "    test_telugu_words.append(\"\\t\" + str(i[0]) + \"\\n\")\n",
        "    test_english_words.append(str(i[1]))\n"
      ],
      "metadata": {
        "id": "Vi3hdOOkMwy3"
      },
      "id": "Vi3hdOOkMwy3",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the total list of input words(English)\n",
        "total_input_words = english_words + val_english_words + test_english_words\n",
        "\n",
        "# Get the total list of target words(Telugu)\n",
        "total_output_words = telugu_words + val_telugu_words + test_telugu_words"
      ],
      "metadata": {
        "id": "1V2rOYt2Mw2i"
      },
      "id": "1V2rOYt2Mw2i",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the total input and target language characters \n",
        "\n",
        "english_characters = set()\n",
        "english_characters.add(\" \")\n",
        "telugu_characters = set()\n",
        "telugu_characters.add(\" \")\n",
        "\n",
        "for word in total_input_words:\n",
        "    for char in word:\n",
        "        if char not in english_characters:\n",
        "            english_characters.add(char)\n",
        "\n",
        "for word in total_output_words:\n",
        "    for char in word:\n",
        "        if char not in telugu_characters:\n",
        "            telugu_characters.add(char)"
      ],
      "metadata": {
        "id": "VTjZKi88Mw53"
      },
      "id": "VTjZKi88Mw53",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the list of input characters(English) and target characters(Telugu)\n",
        "english_characters = sorted(list(english_characters))\n",
        "telugu_characters = sorted(list(telugu_characters))\n",
        "\n",
        "# Get total number of characters in Input \n",
        "num_encoder_tokens = len(english_characters)\n",
        "# Get total number of characters in Output \n",
        "num_decoder_tokens = len(telugu_characters)\n",
        "\n",
        "# Get maximum length of the word in total input words\n",
        "max_encoder_seq_length = max([len(text) for text in total_input_words])\n",
        "# Get maximum length of the word in total target words\n",
        "max_decoder_seq_length = max([len(text) for text in total_output_words])\n",
        "\n",
        "print(\"Summary of the dataset :\")\n",
        "print(\"Number of train samples :\" , len(english_words))\n",
        "print(\"Number of val samples :\" , len(val_english_words))\n",
        "print(\"Number of test samples :\" , len(test_english_words))\n",
        "print(\"Number of unique input tokens :\" , num_encoder_tokens)\n",
        "print(\"Number of unique output tokens :\" , num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\" , max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\" , max_decoder_seq_length)"
      ],
      "metadata": {
        "id": "2z54LZTzMxJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9f902c7-d529-45a6-b18b-1d38f6a9edb2"
      },
      "id": "2z54LZTzMxJm",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary of the dataset :\n",
            "Number of train samples : 58550\n",
            "Number of val samples : 5683\n",
            "Number of test samples : 5747\n",
            "Number of unique input tokens : 27\n",
            "Number of unique output tokens : 66\n",
            "Max sequence length for inputs: 25\n",
            "Max sequence length for outputs: 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary for every english character with an index associated to it\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(english_characters)])\n",
        "# Dictionary for every telugu character with an index associated to it\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(telugu_characters)])\n",
        "\n",
        "# Dictionary for every english character with an index associated to it\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "# Dictionary for every english character with an index associated to it\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "# Preparing train encoder and decoder inputs and decoder target\n",
        "encoder_input_data = np.zeros((len(english_words), max_encoder_seq_length), dtype=\"float32\")\n",
        "decoder_input_data = np.zeros((len(english_words), max_decoder_seq_length), dtype=\"float32\")\n",
        "decoder_target_data = np.zeros((len(english_words), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "# Writing the encoder , decoder characters with the indexes in input and target token index \n",
        "for i, (english, telugu) in enumerate(zip(english_words, telugu_words)):\n",
        "    for t, char in enumerate(english):\n",
        "        encoder_input_data[i, t] = input_token_index[char]\n",
        "    \n",
        "    for t, char in enumerate(telugu):\n",
        "        decoder_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            # One got encoding the decoder_target_data  and decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0    \n",
        "    decoder_input_data[i, t+1:] = target_token_index[' ']\n",
        "    decoder_target_data[i, t :, target_token_index[' ']] = 1.0"
      ],
      "metadata": {
        "id": "bK_yEk1RObdd"
      },
      "id": "bK_yEk1RObdd",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing validation encoder and decoder inputs\n",
        "\n",
        "encoder_val_input_data = np.zeros((len(val_english_words), max_encoder_seq_length), dtype=\"float32\")\n",
        "decoder_val_input_data = np.zeros((len(val_english_words), max_decoder_seq_length), dtype=\"float32\")\n",
        "decoder_val_target_data = np.zeros((len(val_english_words), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "for i, (english, telugu) in enumerate(zip(val_english_words, val_telugu_words)):\n",
        "    for t, char in enumerate(english):\n",
        "        encoder_val_input_data[i, t] = input_token_index[char]\n",
        "  \n",
        "    for t, char in enumerate(telugu):\n",
        "        decoder_val_input_data[i, t] =  target_token_index[char]\n",
        "        if t > 0:\n",
        "            # One got encoding the validation decoder target data  and validation decoder target data will be ahead by one timestep and will not include the start character.\n",
        "            decoder_val_target_data[i, t - 1, target_token_index[char]] = 1.0   \n",
        "    decoder_val_input_data[i, t+1:] = target_token_index[' ']\n",
        "    decoder_val_target_data[i, t :, target_token_index[' ']] = 1.0"
      ],
      "metadata": {
        "id": "TGi_LjABObiY"
      },
      "id": "TGi_LjABObiY",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing test encoder and decoder inputs\n",
        "\n",
        "encoder_test_input_data = np.zeros((len(test_english_words), max_encoder_seq_length), dtype=\"float32\")\n",
        "decoder_test_input_data = np.zeros((len(test_english_words), max_decoder_seq_length), dtype=\"float32\")\n",
        "decoder_test_target_data = np.zeros((len(test_english_words), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "for i, (english, telugu) in enumerate(zip(test_english_words, test_telugu_words)):\n",
        "    for t, char in enumerate(english):\n",
        "        encoder_test_input_data[i, t] = input_token_index[char]\n",
        "  \n",
        "    for t, char in enumerate(telugu):\n",
        "        decoder_test_input_data[i, t] =  target_token_index[char]\n",
        "        if t > 0:\n",
        "            # One got encoding the validation decoder target data  and validation decoder target data will be ahead by one timestep and will not include the start character.\n",
        "            decoder_test_target_data[i, t - 1, target_token_index[char]] = 1.0   \n",
        "    decoder_test_input_data[i, t+1:] = target_token_index[' ']\n",
        "    decoder_test_target_data[i, t :, target_token_index[' ']] = 1.0"
      ],
      "metadata": {
        "id": "1YuBryfa7-91"
      },
      "id": "1YuBryfa7-91",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the validation telugu and english words to nupy arrays\n",
        "val_telugu_words = np.array(val_telugu_words)\n",
        "val_english_words = np.array(val_english_words)\n",
        "\n",
        "# Converting the test telugu and english words to numpy arrays\n",
        "test_telugu_words = np.array(test_telugu_words)\n",
        "test_english_words = np.array(test_english_words)"
      ],
      "metadata": {
        "id": "71z6GdWuPOxG"
      },
      "id": "71z6GdWuPOxG",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(predictions_path, 'w') as f:\n",
        "    f.write('Input English Words, Predicted Telugu Words, Actual Telugu Words\\n')"
      ],
      "metadata": {
        "id": "4tEqG27G75lK"
      },
      "id": "4tEqG27G75lK",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Build Model"
      ],
      "metadata": {
        "id": "dMhX6Gga5UKF"
      },
      "id": "dMhX6Gga5UKF"
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomRNN(object):\n",
        "    \n",
        "    def __init__(self, input_embedding_size, cell_type='GRU', hidden_layer_size=32,\n",
        "                 num_encoder_layers=2, num_decoder_layers = 2, dropout=0.1, \n",
        "                 batch_size=32, epochs=25, is_test_model=False):\n",
        "\n",
        "        self.cell_type = cell_type\n",
        "        self.input_embedding_size = input_embedding_size\n",
        "        self.cell_type = cell_type\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "        self.num_encoder_layers = num_encoder_layers\n",
        "        self.num_decoder_layers = num_decoder_layers\n",
        "        self.dropout = dropout\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.is_test_model = is_test_model\n",
        "\n",
        "\n",
        "    def training(self):\n",
        "        \"\"\"\n",
        "        Method to train the model using LSTM or GRU or RNN and\n",
        "        return model along with encoder_layers and decoder_layers.\n",
        "        \"\"\"\n",
        "\n",
        "        # Choose customFunction based on cell_type\n",
        "        customFunction = None\n",
        "        if self.cell_type == 'LSTM':\n",
        "            customFunction = LSTM\n",
        "        elif self.cell_type == 'GRU':\n",
        "            customFunction = GRU\n",
        "        elif self.cell_type == 'RNN':\n",
        "            customFunction = SimpleRNN\n",
        "\n",
        "        # Encoders\n",
        "        encoder_inputs = Input(shape=(None,))\n",
        "        encoder_embedding = Embedding(num_encoder_tokens,self.input_embedding_size, input_length = max_encoder_seq_length)(encoder_inputs)\n",
        "        \n",
        "        # get encoder_layers and encoder_states\n",
        "        encoder_layers = []\n",
        "        encoder_states = []    \n",
        "        if self.cell_type == 'LSTM':\n",
        "            encoder = customFunction(self.hidden_layer_size, return_sequences=True, return_state=True, dropout = self.dropout)\n",
        "            encoder_layers.append(encoder)\n",
        "            encoder_outputs, state_h, state_c = encoder(encoder_embedding)\n",
        "            encoder_states.append([state_h, state_c])\n",
        "\n",
        "            for i in range(1,self.num_encoder_layers):\n",
        "                encoder = customFunction(self.hidden_layer_size,return_sequences=True,return_state=True, dropout = self.dropout) \n",
        "                encoder_layers.append(encoder)\n",
        "                encoder_outputs, state_h, state_c = encoder(encoder_outputs)\n",
        "                encoder_states.append([state_h, state_c])\n",
        "        else:\n",
        "            encoder = customFunction(self.hidden_layer_size, return_sequences=True, return_state=True, dropout = self.dropout)\n",
        "            encoder_layers.append(encoder)\n",
        "            encoder_outputs, state_h = encoder(encoder_embedding)\n",
        "            encoder_states.append([state_h])\n",
        "\n",
        "            for i in range(1,self.num_encoder_layers):\n",
        "                encoder = customFunction(self.hidden_layer_size,return_sequences=True,return_state=True, dropout = self.dropout) \n",
        "                encoder_layers.append(encoder)\n",
        "                encoder_outputs, state_h = encoder(encoder_outputs)\n",
        "                encoder_states.append([state_h])\n",
        "        \n",
        "        # Decoders\n",
        "        decoder_inputs = Input(shape=(None,))\n",
        "        decoder_embedding = Embedding(num_decoder_tokens,self.input_embedding_size, input_length = max_decoder_seq_length)(decoder_inputs)\n",
        "\n",
        "        # get decoder_layers\n",
        "        decoder_layers = []\n",
        "        if self.cell_type == 'LSTM':\n",
        "            decoder_lstm = customFunction(self.hidden_layer_size, return_sequences=True, return_state=True, dropout = self.dropout)\n",
        "            decoder_layers.append(decoder_lstm)\n",
        "            decoder_outputs, _ , _ = decoder_lstm(decoder_embedding, initial_state=encoder_states[0])\n",
        "            for i in range(1,self.num_decoder_layers):\n",
        "                decoder_lstm = customFunction(self.hidden_layer_size, return_sequences=True, return_state=True, dropout = self.dropout)\n",
        "                decoder_layers.append(decoder_lstm)\n",
        "                decoder_outputs, _ , _  = decoder_lstm(decoder_outputs, initial_state=encoder_states[i])\n",
        "        else:\n",
        "            decoder_GRU = customFunction(self.hidden_layer_size, return_sequences=True, return_state=True, dropout = self.dropout)\n",
        "            decoder_layers.append(decoder_GRU)\n",
        "            decoder_outputs, _ = decoder_GRU(decoder_embedding, initial_state=encoder_states[0])\n",
        "            for i in range(1,self.num_decoder_layers):\n",
        "                decoder_GRU = customFunction(self.hidden_layer_size, return_sequences=True, return_state=True, dropout = self.dropout)\n",
        "                decoder_layers.append(decoder_GRU)\n",
        "                decoder_outputs, _  = decoder_GRU(decoder_outputs, initial_state=encoder_states[i])\n",
        "        \n",
        "        # Add dense layer to decoder\n",
        "        decoder_dense = Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "        decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "        # Create a model, compile and fit the model using the optimizer.\n",
        "        model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "        model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "        # If test_model, do not add wandb callback\n",
        "        if self.is_test_model:\n",
        "            model.fit(\n",
        "                [encoder_input_data, decoder_input_data],\n",
        "                decoder_target_data,\n",
        "                batch_size=self.batch_size,\n",
        "                epochs=self.epochs\n",
        "            )\n",
        "        else:\n",
        "            model.fit(\n",
        "                [encoder_input_data, decoder_input_data],\n",
        "                decoder_target_data,\n",
        "                batch_size=self.batch_size,\n",
        "                epochs=self.epochs,\n",
        "                callbacks=[WandbCallback()]\n",
        "            )\n",
        "\n",
        "        return model, encoder_layers, decoder_layers\n",
        "\n",
        "\n",
        "    def inference_model(self, model, encoder_layers, decoder_layers):\n",
        "        \"\"\"\n",
        "        Create the encoder_model, decoder_model and return using the\n",
        "        arguments model, encoder_layers and decoder_layers\n",
        "        \"\"\"\n",
        "        \n",
        "        # generate encoder_model \n",
        "        encoder_inputs = model.input[0]  # input_1\n",
        "        encoder_states = []\n",
        "        enc_emb = model.layers[2]     # embedding 1\n",
        "        encoder_outputs = enc_emb(encoder_inputs)\n",
        "\n",
        "        if self.cell_type == 'RNN' or self.cell_type ==\"GRU\":\n",
        "            for i in range(self.num_encoder_layers):\n",
        "                encoder_outputs, state_h_enc = encoder_layers[i](encoder_outputs)\n",
        "                encoder_states += [state_h_enc] \n",
        "        else:\n",
        "            for i in range(self.num_encoder_layers):\n",
        "                encoder_outputs, state_h_enc, state_c_enc = encoder_layers[i](encoder_outputs)\n",
        "                encoder_states += [state_h_enc, state_c_enc]   \n",
        "\n",
        "        encoder_model = Model(encoder_inputs, encoder_states + [encoder_outputs])\n",
        "\n",
        "\n",
        "        # generate decoder_model\n",
        "        input_names = [[\"input_100\",\"input_101\"],[\"input_102\",\"input_103\"],[\"input_104\",\"input_105\"],\"input_106\"]\n",
        "\n",
        "        decoder_inputs = model.input[1]       # input_2\n",
        "        decoder_embedding = model.layers[3]   # embedding 2\n",
        "        decoder_outputs = decoder_embedding(decoder_inputs)\n",
        "        decoder_states = []\n",
        "        decoder_states_inputs = []\n",
        "        \n",
        "        if self.cell_type == 'RNN' or self.cell_type ==\"GRU\":\n",
        "            for i in range(self.num_decoder_layers):\n",
        "                decoder_states_inputs += [Input(shape=(self.hidden_layer_size,), name=input_names[i][0])]\n",
        "            for i in range(self.num_decoder_layers):\n",
        "                decoder_outputs, state_h_dec = decoder_layers[i](decoder_outputs, initial_state=decoder_states_inputs[i])\n",
        "                decoder_states += [state_h_dec]\n",
        "        else:\n",
        "            for i in range(self.num_decoder_layers):\n",
        "                decoder_states_inputs += [Input(shape=(self.hidden_layer_size,), name=input_names[i][0]), Input(shape=(self.hidden_layer_size,), name=input_names[i][1])]\n",
        "            j = 0\n",
        "            for i in range(self.num_decoder_layers):\n",
        "                decoder_outputs, state_h_dec, state_c_dec = decoder_layers[i](decoder_outputs, initial_state=decoder_states_inputs[i+j:i+j+2])\n",
        "                decoder_states += [state_h_dec , state_c_dec]\n",
        "                j += 1\n",
        "\n",
        "        # get the decoder dense layer and add it to decoder_model\n",
        "        decoder_dense = model.layers[4+2*self.num_encoder_layers]\n",
        "        decoder_outputs = decoder_dense(decoder_outputs)\n",
        "        decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "        return encoder_model, decoder_model\n",
        "\n",
        "\n",
        "    def decode_sequence(self, input_seq, encoder_model, decoder_model):\n",
        "        \"\"\"\n",
        "        Method to decode an input sequence using the encoder_model\n",
        "        and decoder_model\n",
        "        \"\"\"\n",
        "        states_value = encoder_model.predict(input_seq)\n",
        "        states_value = states_value[:-1]\n",
        "        target_seq = np.zeros((1, 1)) \n",
        "        target_seq[0, 0] = target_token_index[\"\\t\"]\n",
        "\n",
        "        # continue decoding input sequence till '\\n' is found or the sequence\n",
        "        # exceeds the maximum decoder sequence length.\n",
        "        stop_condition = False\n",
        "        decoded_sentence = \"\"\n",
        "        while not stop_condition:\n",
        "            dec_ip = [target_seq]+states_value\n",
        "            output_tokens = decoder_model.predict(dec_ip)\n",
        "            sampled_token_index = np.argmax(output_tokens[0][0, -1, :])\n",
        "            sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "            decoded_sentence += sampled_char\n",
        "            if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "                stop_condition = True\n",
        "\n",
        "            target_seq = np.zeros((1, 1))\n",
        "            target_seq[0, 0] = sampled_token_index\n",
        "            states_value = output_tokens[1:]\n",
        "            \n",
        "        return decoded_sentence\n",
        "      \n",
        "    \n",
        "    def test_and_calculate_accuraccy(self, encoder_model, decoder_model, tmp_english_words,\n",
        "                                     tmp_telugu_words, tmp_encoder_input_data, is_val_accuracy=True):\n",
        "        \"\"\"\n",
        "        Calculate accuracy using the data and write it to prediction file.\n",
        "        \"\"\"\n",
        "        correct = 0\n",
        "        n = tmp_telugu_words.shape[0]\n",
        "        for i in range(n):\n",
        "            input = tmp_encoder_input_data[i:i+1]\n",
        "            output = self.decode_sequence(input, encoder_model, decoder_model)\n",
        "            with open(predictions_path, 'a') as f:\n",
        "                f.write('{} , {} , {}\\n'.format(tmp_english_words[i].strip(), output.strip(), tmp_telugu_words[i].strip()))\n",
        "            if output.strip() == tmp_telugu_words[i].strip():\n",
        "                correct += 1\n",
        "\n",
        "        # log accuracy to wandb\n",
        "        if is_val_accuracy:\n",
        "            wandb.log({'val_accuracy' : correct*100/n})\n",
        "            print('val_accuracy', correct*100/n)\n",
        "        else:\n",
        "            wandb.log({'test_accuracy' : correct*100/n})\n",
        "            print('test_accuracy', correct*100/n)"
      ],
      "metadata": {
        "id": "Z1Ya6JFE5VkE"
      },
      "id": "Z1Ya6JFE5VkE",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sweep Config"
      ],
      "metadata": {
        "id": "onxvU8-85g3P"
      },
      "id": "onxvU8-85g3P"
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'metric':\n",
        "    {'goal': 'maximize',\n",
        "     'name': 'val_accuracy'\n",
        "     },\n",
        "    'parameters': {\n",
        "        'input_embedding_size':\n",
        "        {\n",
        "            'values': [128, 256, 512]\n",
        "        },\n",
        "        'hidden_layer_size':\n",
        "        {\n",
        "            'values': [128, 256, 512]\n",
        "        },\n",
        "        'cell_type':\n",
        "        {\n",
        "            'values': ['LSTM', 'RNN', 'GRU']\n",
        "        },\n",
        "        'num_layers':\n",
        "        {'values': [1, 2, 3]\n",
        "         },\n",
        "        'batch_size':\n",
        "        {\n",
        "            'values': [128, 256, 512]\n",
        "        },\n",
        "        'dropout':\n",
        "        {\n",
        "            'values': [0.1]\n",
        "        },\n",
        "        'epochs':\n",
        "        {\n",
        "            'values': [25]\n",
        "        }\n",
        "    }}\n"
      ],
      "metadata": {
        "id": "UbN4yF355jA9"
      },
      "id": "UbN4yF355jA9",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, entity=\"cs21m010-cs21m041\", project=\"DL_Assignment_3_a\")"
      ],
      "metadata": {
        "id": "DTVK0bfI5kh2"
      },
      "id": "DTVK0bfI5kh2",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "\n",
        "    # Create a new WandB run\n",
        "    wandb.init(config=sweep_config)\n",
        "    \n",
        "    # Construct the run name\n",
        "    config = wandb.config\n",
        "    wandb.run.name = config.cell_type + '' + str(config.input_embedding_size) + '_hs' + str(config.hidden_layer_size) + 'bs' + str(config.batch_size)\n",
        "    \n",
        "    print('training...')\n",
        "    model_rnn_obj = CustomRNN(config.input_embedding_size, config.cell_type , config.hidden_layer_size, config.num_layers, config.num_layers, config.dropout, config.batch_size, config.epochs)\n",
        "    model, encoder_layers, decoder_layers = model_rnn_obj.training()\n",
        "    \n",
        "    print('inferrencing...')\n",
        "    encoder_model, decoder_model = model_rnn_obj.inference_model(model, encoder_layers, decoder_layers)\n",
        "    \n",
        "    print('decoding sequence...')\n",
        "    model_rnn_obj.test_and_calculate_accuraccy(encoder_model, decoder_model, val_english_words, val_telugu_words, encoder_val_input_data)"
      ],
      "metadata": {
        "id": "m5dHjntQ5mLF"
      },
      "id": "m5dHjntQ5mLF",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, train, count = 15)"
      ],
      "metadata": {
        "id": "D-X5sp-N5oa8"
      },
      "id": "D-X5sp-N5oa8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fitting the Best model"
      ],
      "metadata": {
        "id": "vn2cPxXc8PIe"
      },
      "id": "vn2cPxXc8PIe"
    },
    {
      "cell_type": "code",
      "source": [
        "best_sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'metric': {\n",
        "        'goal': 'maximize',\n",
        "        'name': 'val_accuracy'\n",
        "     },\n",
        "    'parameters': {\n",
        "        'input_embedding_size': {\n",
        "            'values': [512]\n",
        "        },\n",
        "        'hidden_layer_size': {\n",
        "            'values': [256]\n",
        "        },\n",
        "        'cell_type': {\n",
        "            'values': ['LSTM']\n",
        "        },\n",
        "        'num_layers': {\n",
        "            'values': [3]\n",
        "         },\n",
        "        'batch_size': {\n",
        "            'values': [512]\n",
        "        },\n",
        "        'dropout': {\n",
        "            'values': [0.1]\n",
        "        },\n",
        "        'epochs': {\n",
        "            'values': [25]\n",
        "        }\n",
        "    }}\n"
      ],
      "metadata": {
        "id": "ZTXyXH5C8hC4"
      },
      "id": "ZTXyXH5C8hC4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(best_sweep_config, entity=\"cs21m010-cs21m041\", project=\"DL_Assignment_3_a\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zClbnZkjDeNc",
        "outputId": "a284573b-375f-470e-af69-1fb752a790f7"
      },
      "id": "zClbnZkjDeNc",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: v9e4flcj\n",
            "Sweep URL: https://wandb.ai/cs21m010-cs21m041/DL_Assignment_3_a/sweeps/v9e4flcj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Consider Best configuration\n",
        "def consider_best_model():\n",
        "    \n",
        "    # Create a new WandB run\n",
        "    wandb.init(config=best_sweep_config)\n",
        "\n",
        "    # Construct the run name\n",
        "    config = wandb.config\n",
        "    wandb.run.name = 'Best_' + config.cell_type + '' + str(config.input_embedding_size) + '_hs' + str(config.hidden_layer_size) + 'bs' + str(config.batch_size)\n",
        "\n",
        "    print('training...')\n",
        "    best_model_obj = CustomRNN(config.input_embedding_size, config.cell_type , config.hidden_layer_size, config.num_layers, config.num_layers, config.dropout, config.batch_size, config.epochs)\n",
        "    model, encoder_layers, decoder_layers = best_model_obj.training()\n",
        "\n",
        "    print('inferrencing...')\n",
        "    encoder_model, decoder_model = best_model_obj.inference_model(model, encoder_layers, decoder_layers)\n",
        "\n",
        "    print('calculating test accuracy and decoding test inputs...')\n",
        "    best_model_obj.test_and_calculate_accuraccy(encoder_model, decoder_model, test_english_words, \n",
        "                                              test_telugu_words, encoder_test_input_data, is_val_accuracy=False)"
      ],
      "metadata": {
        "id": "fnOXANns8QSH"
      },
      "id": "fnOXANns8QSH",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, consider_best_model, count=1)"
      ],
      "metadata": {
        "id": "2h7L3BiKB6_5"
      },
      "id": "2h7L3BiKB6_5",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}