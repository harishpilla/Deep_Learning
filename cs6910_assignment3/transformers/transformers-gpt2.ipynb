{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install Dependencies","metadata":{}},{"cell_type":"code","source":"# Install transformers\n!pip install transformers\n\n# Install openpyxl\n!pip install openpyxl","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:06:24.846138Z","iopub.execute_input":"2022-05-08T07:06:24.846399Z","iopub.status.idle":"2022-05-08T07:06:43.373283Z","shell.execute_reply.started":"2022-05-08T07:06:24.846364Z","shell.execute_reply":"2022-05-08T07:06:43.372444Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":" # Install Libraries","metadata":{}},{"cell_type":"code","source":"# Transformers\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n\n# Torch\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\n# Numpy and pandas\nimport numpy as np\nimport pandas as pd\n\n# Helper libraries\nimport random,os\nfrom tqdm import tqdm, trange\nimport csv","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:06:43.376197Z","iopub.execute_input":"2022-05-08T07:06:43.376912Z","iopub.status.idle":"2022-05-08T07:06:43.383956Z","shell.execute_reply.started":"2022-05-08T07:06:43.376876Z","shell.execute_reply":"2022-05-08T07:06:43.381817Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"## Read Data (lyrics) from gloom_index.xlsx file","metadata":{}},{"cell_type":"code","source":"lyrics_file_path = '/content/gloom_index.xlsx'\ngenerated_lyrics_file_path = \"/content/generated_lyrics.txt\"","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:06:43.385395Z","iopub.execute_input":"2022-05-08T07:06:43.385650Z","iopub.status.idle":"2022-05-08T07:06:43.393630Z","shell.execute_reply.started":"2022-05-08T07:06:43.385617Z","shell.execute_reply":"2022-05-08T07:06:43.392818Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"data = pd.read_excel(lyrics_file_path)\ndata = data[['lyrics']]","metadata":{"id":"C5fBrswxI_c4","execution":{"iopub.status.busy":"2022-05-08T07:06:43.397238Z","iopub.execute_input":"2022-05-08T07:06:43.397941Z","iopub.status.idle":"2022-05-08T07:06:43.477266Z","shell.execute_reply.started":"2022-05-08T07:06:43.397910Z","shell.execute_reply":"2022-05-08T07:06:43.476636Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"# Generate Lyrics using data","metadata":{"id":"AF7jPIwbrFmW"}},{"cell_type":"code","source":"class Generate_data(Dataset):  \n    def __init__(self, control_code, gpt2_type=\"gpt2\", max_length=1024):\n        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n        self.lyrics = []\n\n        for row in data['lyrics']:\n          try:\n              self.lyrics.append(torch.tensor(self.tokenizer.encode(f\"<|{control_code}|>{row[:max_length]}<|endoftext|>\")))\n          except:\n              pass      \n        self.lyrics_count = len(self.lyrics)\n        \n    def __len__(self):\n        return self.lyrics_count\n\n    def __getitem__(self, item):\n        return self.lyrics[item]","metadata":{"id":"fcSfLPC-NZE1","outputId":"c7bb5e77-08bc-4c5e-9348-fc1cf502cc4f","execution":{"iopub.status.busy":"2022-05-08T07:06:43.479741Z","iopub.execute_input":"2022-05-08T07:06:43.480017Z","iopub.status.idle":"2022-05-08T07:06:43.485938Z","shell.execute_reply.started":"2022-05-08T07:06:43.479983Z","shell.execute_reply":"2022-05-08T07:06:43.485283Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"## Pack Tensor","metadata":{}},{"cell_type":"code","source":"def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n    if packed_tensor is None:\n        return new_tensor, True, None\n    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n        return packed_tensor, False, new_tensor\n    else:\n        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n        return packed_tensor, True, None","metadata":{"id":"5vxYINgANiE6","outputId":"ac5c2478-a9a2-4578-c3ec-8217e08ccf15","execution":{"iopub.status.busy":"2022-05-08T07:06:43.487399Z","iopub.execute_input":"2022-05-08T07:06:43.487891Z","iopub.status.idle":"2022-05-08T07:06:43.496606Z","shell.execute_reply.started":"2022-05-08T07:06:43.487830Z","shell.execute_reply":"2022-05-08T07:06:43.495606Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"## Train using dataset and model","metadata":{}},{"cell_type":"code","source":"def train(dataset, model, tokenizer, batch_size=32, epochs=10, lr=1e-4, max_seq_len=400, warmup_steps=200, gpt2_type=\"gpt2\", output_dir=\".\", output_prefix=\"wreckgar\", test_mode=False):\n    device = torch.device(\"cuda\")\n    model = model.cuda()\n    model.train()\n\n    optimizer = AdamW(model.parameters(), lr=lr)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1)\n    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n    loss = 0\n    batch_count_so_far = 0\n    input_tensor = None\n\n    for epoch in range(epochs):\n        print(\"Training epoch: {} with loss: {}\".format(epoch, loss))\n        for idx, entry in tqdm(enumerate(train_dataloader)):\n            \n            # Pack the tensor\n            (input_tensor, carry_on, _) = pack_tensor(entry, input_tensor, 768)\n\n            if carry_on and idx != len(train_dataloader) - 1:\n                continue\n\n            input_tensor = input_tensor.to(device)\n            outputs = model(input_tensor, labels=input_tensor)\n            loss = outputs[0]\n            loss.backward()\n\n            if (batch_count_so_far % batch_size) == 0:\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n                model.zero_grad()\n\n            batch_count_so_far += 1\n            input_tensor = None\n        \n    return model","metadata":{"id":"BJtmSZvCNi--","execution":{"iopub.status.busy":"2022-05-08T07:06:43.497975Z","iopub.execute_input":"2022-05-08T07:06:43.498487Z","iopub.status.idle":"2022-05-08T07:06:43.511505Z","shell.execute_reply.started":"2022-05-08T07:06:43.498448Z","shell.execute_reply":"2022-05-08T07:06:43.510617Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"## For an input 'generate' function returns the lyrics ","metadata":{}},{"cell_type":"code","source":"def generate(model, tokenizer, prompt, entry_count=10, entry_length=20, top_p=0.8, temperature=1):\n    model.eval()\n    no_of_generated_lyrics = 0\n    lyrics_list = []\n    filter_value = -float(\"Inf\")\n\n    with torch.no_grad():\n        for _ in trange(entry_count):\n            lyric_completed = False\n            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n\n            for i in range(entry_length):\n                output_from_model = model(generated, labels=generated)\n                _, logits = output_from_model[:2]\n                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n\n                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n                sorted_indices_to_remove = cumulative_probs > top_p\n                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n                sorted_indices_to_remove[..., 0] = 0\n                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n                logits[:, indices_to_remove] = filter_value\n\n                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n                generated = torch.cat((generated, next_token), dim=1)\n                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n                    lyric_completed = True\n                if lyric_completed:\n                    no_of_generated_lyrics = no_of_generated_lyrics + 1\n                    output_list = list(generated.squeeze().numpy())\n                    output_text = tokenizer.decode(output_list)\n                    lyrics_list.append(output_text)\n                    break\n            \n            if not lyric_completed:\n              output_list = list(generated.squeeze().numpy())\n              output_text = tokenizer.decode(output_list)\n              lyrics_list.append(output_text)\n                \n    return lyrics_list","metadata":{"id":"eXGIPFh7Nsoq","outputId":"78b57e7e-4b02-4db2-f8ab-ba816678cdf9","execution":{"iopub.status.busy":"2022-05-08T07:06:43.512915Z","iopub.execute_input":"2022-05-08T07:06:43.513234Z","iopub.status.idle":"2022-05-08T07:06:43.527123Z","shell.execute_reply.started":"2022-05-08T07:06:43.513197Z","shell.execute_reply":"2022-05-08T07:06:43.526103Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"# Invoke Generate and get Lyrics","metadata":{}},{"cell_type":"code","source":"# Invoke generate method along with model to get different lyrics on string 'I love deep Learning'\nTEXT_FOR_GENERATING_LYRICS = \"I love Deep Learning\"\n\n# Get the dataset\ndataset = Generate_data(data['lyrics'], gpt2_type=\"gpt2\")  \n\n# Get the tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n\n# Get the model using the dataset and tokenizer from pretrained model\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\nmodel = train(dataset, model, tokenizer)\n\nlyrics = []\nfor i in range(20):\n  lyrics.append(generate(model.to('cpu'), tokenizer, TEXT_FOR_GENERATING_LYRICS, entry_count=1))\n\nwith open(generated_lyrics_file_path, 'w') as f:\n    for one_lyric in lyrics:\n        f.write('{} '.format(one_lyric[0].strip()))\n        print(one_lyric[0].strip())","metadata":{"id":"_xCZJwvUpqr-","execution":{"iopub.status.busy":"2022-05-08T07:06:43.528484Z","iopub.execute_input":"2022-05-08T07:06:43.528996Z","iopub.status.idle":"2022-05-08T07:08:21.129191Z","shell.execute_reply.started":"2022-05-08T07:06:43.528958Z","shell.execute_reply":"2022-05-08T07:08:21.126874Z"},"trusted":true},"execution_count":42,"outputs":[]}]}