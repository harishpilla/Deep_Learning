{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install Dependencies","metadata":{}},{"cell_type":"code","source":"# Install transformers\n!pip install transformers\n\n# Install openpyxl\n!pip install openpyxl","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:52:21.444811Z","iopub.execute_input":"2022-05-08T16:52:21.445142Z","iopub.status.idle":"2022-05-08T16:52:42.132633Z","shell.execute_reply.started":"2022-05-08T16:52:21.445057Z","shell.execute_reply":"2022-05-08T16:52:42.131802Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":" # Install Libraries","metadata":{}},{"cell_type":"code","source":"# Transformers\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n\n# Torch\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\n# Numpy and pandas\nimport numpy as np\nimport pandas as pd\n\n# Helper libraries\nimport random,os\nfrom tqdm import tqdm, trange\nimport csv","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:52:42.134754Z","iopub.execute_input":"2022-05-08T16:52:42.134970Z","iopub.status.idle":"2022-05-08T16:52:48.220437Z","shell.execute_reply.started":"2022-05-08T16:52:42.134943Z","shell.execute_reply":"2022-05-08T16:52:48.219712Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Read Data (lyrics) from gloom_index.xlsx file","metadata":{}},{"cell_type":"code","source":"lyrics_file_path = '/content/lyrics/gloom_index.xlsx'\ngenerated_lyrics_file_path = \"/content/generated_lyrics.txt\"","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:52:48.221721Z","iopub.execute_input":"2022-05-08T16:52:48.221959Z","iopub.status.idle":"2022-05-08T16:52:48.227172Z","shell.execute_reply.started":"2022-05-08T16:52:48.221927Z","shell.execute_reply":"2022-05-08T16:52:48.226417Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data = pd.read_excel(lyrics_file_path)\ndata = data[['lyrics']]","metadata":{"id":"C5fBrswxI_c4","execution":{"iopub.status.busy":"2022-05-08T16:52:48.229277Z","iopub.execute_input":"2022-05-08T16:52:48.230159Z","iopub.status.idle":"2022-05-08T16:52:48.476252Z","shell.execute_reply.started":"2022-05-08T16:52:48.230084Z","shell.execute_reply":"2022-05-08T16:52:48.475391Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Generate Lyrics using data","metadata":{"id":"AF7jPIwbrFmW"}},{"cell_type":"code","source":"class GetDataset(Dataset):  \n    def __init__(self, code, gpt2_type=\"gpt2\", maximum_len=1024):\n        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n        self.lyrics = []\n\n        for row in data['lyrics']:\n          try:\n              self.lyrics.append(torch.tensor(self.tokenizer.encode(f\"<|{code}|>{row[:maximum_len]}<|endoftext|>\")))\n          except:\n              pass      \n        self.lyrics_count = len(self.lyrics)\n        \n    def __len__(self):\n        return self.lyrics_count\n\n    def __getitem__(self, item):\n        return self.lyrics[item]","metadata":{"id":"fcSfLPC-NZE1","outputId":"c7bb5e77-08bc-4c5e-9348-fc1cf502cc4f","execution":{"iopub.status.busy":"2022-05-08T16:52:48.477321Z","iopub.execute_input":"2022-05-08T16:52:48.477575Z","iopub.status.idle":"2022-05-08T16:52:48.486515Z","shell.execute_reply.started":"2022-05-08T16:52:48.477540Z","shell.execute_reply":"2022-05-08T16:52:48.485668Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Pack Tensor","metadata":{}},{"cell_type":"code","source":"def pack_tensor(first_tensor, compressed_tensor, maximum_sequence_length):\n    if compressed_tensor is None:\n        return first_tensor, True, None\n    if first_tensor.size()[1] + compressed_tensor.size()[1] > maximum_sequence_length:\n        return compressed_tensor, False, first_tensor\n    compressed_tensor = torch.cat([first_tensor, compressed_tensor[:, 1:]], dim=1)\n    return compressed_tensor, True, None","metadata":{"id":"5vxYINgANiE6","outputId":"ac5c2478-a9a2-4578-c3ec-8217e08ccf15","execution":{"iopub.status.busy":"2022-05-08T16:52:48.487646Z","iopub.execute_input":"2022-05-08T16:52:48.488011Z","iopub.status.idle":"2022-05-08T16:52:48.495403Z","shell.execute_reply.started":"2022-05-08T16:52:48.487960Z","shell.execute_reply":"2022-05-08T16:52:48.494313Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Train using dataset and model","metadata":{}},{"cell_type":"code","source":"def TrainModel(dataset, model, tokenizer, batch_size=32, epochs=10, lr=1e-4, max_seq_len=400, warmup_steps=200, gpt2_type=\"gpt2\", output_dir=\".\", output_prefix=\"wreckgar\", test_mode=False):\n    model = model.cuda()\n    model.train()\n\n    opti = AdamW(model.parameters(), lr=lr)\n    schedulerWithWarmUp = get_linear_schedule_with_warmup(opti, num_warmup_steps=warmup_steps, num_training_steps=-1)\n    dataLoaderOfTrain = DataLoader(dataset, batch_size=1, shuffle=True)\n    trainingLoss = 0\n    batchCountSoFar = 0\n    inputTensor = None\n\n    for epoch in range(epochs):\n        print(\"Training epoch: {} with loss: {}\".format(epoch, trainingLoss))\n        for idx, entry in tqdm(enumerate(dataLoaderOfTrain)):\n            (inputTensor, carry_on, _) = pack_tensor(entry, inputTensor, 768)\n            if carry_on and idx != len(dataLoaderOfTrain) - 1:\n                continue\n            inputTensor = inputTensor.to(torch.device(\"cuda\"))\n            trainingLoss = model(inputTensor, labels=inputTensor)[0]\n            trainingLoss.backward()\n            if (batchCountSoFar % batch_size) == 0:\n                opti.step()\n                schedulerWithWarmUp.step()\n                opti.zero_grad()\n                model.zero_grad()\n            batchCountSoFar += 1\n            inputTensor = None\n\n    return model","metadata":{"id":"BJtmSZvCNi--","execution":{"iopub.status.busy":"2022-05-08T16:52:48.497657Z","iopub.execute_input":"2022-05-08T16:52:48.497864Z","iopub.status.idle":"2022-05-08T16:52:48.510377Z","shell.execute_reply.started":"2022-05-08T16:52:48.497841Z","shell.execute_reply":"2022-05-08T16:52:48.509563Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## For an input 'generate' function returns the lyrics ","metadata":{}},{"cell_type":"code","source":"def generate(model, tokenizer, prompt, entry_count=10, entry_length=20, top_p=0.8, temperature=1):\n    \n    negativeFilterValue = -float(\"Inf\")\n    lyricsList = []\n    noOfGeneratedLyrics = 0\n\n    # Evaluate the model\n    model.eval()\n\n    # Loop till no change in grad or lyric is completed\n    with torch.no_grad():\n        for _ in trange(entry_count):\n            lyricCompletedFlag = False\n            tensorGenerated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n\n            for i in range(entry_length):\n                outputFromModel = model(tensorGenerated, labels=tensorGenerated)\n                _, logitValues = outputFromModel[:2]\n                logitValues = logitValues[:, -1, :] / (temperature if temperature > 0 else 1.0)\n\n                sortedLogitValues, sortedIndexes = torch.sort(logitValues, descending=True)\n                cumProbabilities = torch.cumsum(F.softmax(sortedLogitValues, dim=-1), dim=-1)\n                sortedIndexesToBeRemoved = cumProbabilities > top_p\n                sortedIndexesToBeRemoved[..., 1:] = sortedIndexesToBeRemoved[..., :-1].clone()\n                sortedIndexesToBeRemoved[..., 0] = 0\n                indices_to_remove = sortedIndexes[sortedIndexesToBeRemoved]\n                logitValues[:, indices_to_remove] = negativeFilterValue\n\n                upcomingToken = torch.multinomial(F.softmax(logitValues, dim=-1), num_samples=1)\n                tensorGenerated = torch.cat((tensorGenerated, upcomingToken), dim=1)\n                if upcomingToken in tokenizer.encode(\"<|endoftext|>\"):\n                    lyricCompletedFlag = True\n                if lyricCompletedFlag:\n                    noOfGeneratedLyrics = noOfGeneratedLyrics + 1\n                    outputListGenerated = list(tensorGenerated.squeeze().numpy())\n                    outputTextDecoded = tokenizer.decode(outputListGenerated)\n                    lyricsList.append(outputTextDecoded)\n                    break\n            \n            if not lyricCompletedFlag:\n              outputListGenerated = list(tensorGenerated.squeeze().numpy())\n              outputTextDecoded = tokenizer.decode(outputListGenerated)\n              lyricsList.append(outputTextDecoded)\n                \n    return lyricsList","metadata":{"id":"eXGIPFh7Nsoq","outputId":"78b57e7e-4b02-4db2-f8ab-ba816678cdf9","execution":{"iopub.status.busy":"2022-05-08T16:52:48.511690Z","iopub.execute_input":"2022-05-08T16:52:48.512103Z","iopub.status.idle":"2022-05-08T16:52:48.526719Z","shell.execute_reply.started":"2022-05-08T16:52:48.512066Z","shell.execute_reply":"2022-05-08T16:52:48.525946Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Invoke Generate and get Lyrics","metadata":{}},{"cell_type":"code","source":"# Invoke generate method along with model to get different lyrics on string 'I love deep Learning'\nTEXT_FOR_GENERATING_LYRICS = \"I love Deep Learning\"\n\n# Get the dataset\ndataset = GetDataset(data['lyrics'], gpt2_type=\"gpt2\")  \n\n# Get the tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n\n# Get the model using the dataset and tokenizer from pretrained model\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\nmodel = TrainModel(dataset, model, tokenizer)\n\nlyrics = []\nfor i in range(20):\n  lyrics.append(generate(model.to('cpu'), tokenizer, TEXT_FOR_GENERATING_LYRICS, entry_count=1))\n\nwith open(generated_lyrics_file_path, 'w') as f:\n    for one_lyric in lyrics:\n        f.write('{} '.format(one_lyric[0].strip()))\n        print(one_lyric[0].strip())","metadata":{"id":"_xCZJwvUpqr-","execution":{"iopub.status.busy":"2022-05-08T16:52:48.528091Z","iopub.execute_input":"2022-05-08T16:52:48.528560Z","iopub.status.idle":"2022-05-08T16:55:04.679209Z","shell.execute_reply.started":"2022-05-08T16:52:48.528518Z","shell.execute_reply":"2022-05-08T16:55:04.677698Z"},"trusted":true},"execution_count":9,"outputs":[]}]}