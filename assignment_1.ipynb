{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13ac854b",
      "metadata": {
        "id": "13ac854b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from keras.utils import np_utils\n",
        "from numpy import random\n",
        "from random import seed\n",
        "import math\n",
        "import wandb\n",
        "from keras.datasets import mnist\n",
        "from sklearn import metrics\n",
        "import seaborn as sn\n",
        "\n",
        "np.seterr(divide='ignore', invalid='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "# (x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "mbDtl1y1e7xb"
      },
      "id": "mbDtl1y1e7xb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data for cross validation\n",
        "x_val = x_train[54000:]\n",
        "y_val = y_train[54000:]\n",
        "x_train = x_train[:54000]\n",
        "y_train = y_train[:54000]  "
      ],
      "metadata": {
        "id": "0hckvt4aHvve"
      },
      "id": "0hckvt4aHvve",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"x_train original shape\", x_train.shape)\n",
        "print(\"y_train original shape\", y_train.shape)\n",
        "print(y_train)"
      ],
      "metadata": {
        "id": "yAI1hqX6frgu"
      },
      "id": "yAI1hqX6frgu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "fig = plt.figure(figsize=(10,12))\n",
        "\n",
        "for i in range(20):\n",
        "    plt.subplot(4,5,i+1)\n",
        "    img = x_train[y_train == (i%10)][i%10+i]\n",
        "    plt.imshow(img, cmap='Greens', interpolation='none')\n",
        "    plt.title(\"Class: {}\".format(class_names[i%10]))\n",
        "    plt.axis('off')\n",
        "    \n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "X_pKlHkgfufw"
      },
      "id": "X_pKlHkgfufw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turn the data into an appropriate matrix form\n",
        "x_train = np.reshape(x_train, (54000, 784, 1))\n",
        "y_train = np.reshape(y_train, (54000, 1))"
      ],
      "metadata": {
        "id": "izkSbHnP7o-A"
      },
      "id": "izkSbHnP7o-A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = np.reshape(x_test, (10000, 784, 1))\n",
        "y_test = np.reshape(y_test, (10000, 1))"
      ],
      "metadata": {
        "id": "8xqODFqZ7qlX"
      },
      "id": "8xqODFqZ7qlX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = np.reshape(x_val, (6000, 784, 1))\n",
        "y_val = np.reshape(y_val, (6000, 1))"
      ],
      "metadata": {
        "id": "cBTvKZO9IRbC"
      },
      "id": "cBTvKZO9IRbC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding\n",
        "targs = np.zeros((54000, 10, 1))\n",
        "for i in range(54000):\n",
        "    targs[i][y_train[i][0]-1][0] = 1 # preparing the target matrix "
      ],
      "metadata": {
        "id": "ngCFo_2j7rr1"
      },
      "id": "ngCFo_2j7rr1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targs_test = np.zeros((10000, 10, 1))\n",
        "for i in range(10000):\n",
        "    targs_test[i][y_test[i][0]-1][0] = 1 # preparing the target matrix "
      ],
      "metadata": {
        "id": "qijjwqUJ7s_D"
      },
      "id": "qijjwqUJ7s_D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targs_val = np.zeros((6000, 10, 1))\n",
        "for i in range(6000):\n",
        "    targs_val[i][y_val[i][0]-1][0] = 1 # preparing the target matrix "
      ],
      "metadata": {
        "id": "T21r3uSQIXQW"
      },
      "id": "T21r3uSQIXQW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize data\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "x_val = x_val / 255"
      ],
      "metadata": {
        "id": "K-hSNL8e7uI2"
      },
      "id": "K-hSNL8e7uI2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"DL_Assignment_1\", entity=\"cs21m010-cs21m041\")"
      ],
      "metadata": {
        "id": "xIckNyF7XwQk"
      },
      "id": "xIckNyF7XwQk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a network\n",
        "def initialize_network(n_inputs, n_hidden, n_outputs, n_layers, config):\n",
        "    network = list()\n",
        "    randomMultiplier = 0.01\n",
        "    \n",
        "    for i in range(n_layers):\n",
        "        hidden_layer = {}\n",
        "        weights = None\n",
        "        bias = None\n",
        "        if i==0:\n",
        "            if config.initialization==\"random\":\n",
        "                weights = randomMultiplier * np.random.randn(n_hidden, n_inputs)\n",
        "            else:\n",
        "                weights = randomMultiplier * (np.random.randn(n_hidden, n_inputs)) \n",
        "                if config.activation == \"relu\":\n",
        "                    weights = weights * math.sqrt(2 / n_inputs)\n",
        "                else:\n",
        "                    weights = weights * math.sqrt(1 / n_inputs)\n",
        "            bias = np.zeros([n_hidden, 1])\n",
        "        else:\n",
        "            if config.initialization==\"random\":                                  \n",
        "                weights = randomMultiplier * np.random.randn(n_hidden, n_hidden)\n",
        "            else:\n",
        "                weights = randomMultiplier * (np.random.randn(n_hidden, n_hidden)) \n",
        "                if config.activation == \"relu\":\n",
        "                    weights = weights * math.sqrt(2 / n_inputs)\n",
        "                else:\n",
        "                    weights = weights * math.sqrt(1 / n_inputs)\n",
        "                                              \n",
        "            bias = np.zeros([n_hidden, 1])\n",
        "\n",
        "        hidden_layer['weights'] = weights\n",
        "        hidden_layer['bias'] = bias\n",
        "        network.append(hidden_layer)\n",
        "    \n",
        "    # Weights and bias for output layer\n",
        "    output_layer = {}\n",
        "    if config.initialization==\"random\":                                  \n",
        "        weights = randomMultiplier * np.random.randn(n_outputs, n_hidden)\n",
        "    else:\n",
        "        weights = randomMultiplier * (np.random.randn(n_outputs,n_hidden)) \n",
        "        if config.activation == \"relu\":\n",
        "              weights = weights * math.sqrt(2 / n_inputs)\n",
        "        else:\n",
        "              weights = weights * math.sqrt(1 / n_inputs)                                        \n",
        "#     weights = randomMultiplier * np.random.randn(n_outputs, n_hidden)\n",
        "    bias = np.zeros([n_outputs, 1])\n",
        "    output_layer['weights'] = weights\n",
        "    output_layer['bias'] = bias\n",
        "    network.append(output_layer)\n",
        "    \n",
        "    return network"
      ],
      "metadata": {
        "id": "3eJwgIxz7vi1"
      },
      "id": "3eJwgIxz7vi1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Activation Functions starts\n",
        "\n",
        "def sigmoid(Z):\n",
        "    A = 1 / (1 + np.exp(-Z))\n",
        "    return A\n",
        "\n",
        "def sigmoidGrad(dA, prevZ):\n",
        "    s = sigmoid(prevZ)\n",
        "    dZ = dA * s * (1 - s)\n",
        "    return dZ\n",
        "\n",
        "def relu(Z):\n",
        "    A = np.maximum(0, Z)\n",
        "    return A\n",
        "    \n",
        "def reluGrad(dA, prevZ):\n",
        "    s = np.maximum(0, prevZ)\n",
        "    dZ = (s>0) * 1 * dA\n",
        "    return dZ \n",
        "\n",
        "def tanh(Z):\n",
        "    A = np.tanh(Z)\n",
        "    return A\n",
        "\n",
        "def tanhGrad(dA, prevZ):\n",
        "    s = np.tanh(prevZ)\n",
        "    dZ = (1 - s**2) * dA\n",
        "    return dZ\n",
        "\n",
        "def linear(Z):\n",
        "    return Z\n",
        " \n",
        "def softmax(Z):\n",
        "    expZ = np.exp(Z - np.max(Z))\n",
        "    return expZ / expZ.sum(axis=0, keepdims=True)\n",
        "\n",
        "def crossEntropyLoss(Y, A, epsilon=1e-15):\n",
        "    m = Y.shape[1]\n",
        "    loss = -1 * (Y * np.log(A + epsilon) + (1 - Y) * np.log(1 - A + epsilon))\n",
        "    cost = 1 / m * np.sum(loss)\n",
        "    return np.squeeze(cost)\n",
        "\n",
        "### Activation Functions ends"
      ],
      "metadata": {
        "id": "ozYDCgoE7xKA"
      },
      "id": "ozYDCgoE7xKA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Intializing activationForward and activationBackward globally\n",
        "activationForward = None\n",
        "activationBackward = None\n",
        "\n",
        "# Function to set forward and backward activation functions\n",
        "def setActivationFns(activationName):\n",
        "    global activationForward\n",
        "    global activationBackward\n",
        "\n",
        "    print('setting activation function as: ', activationName)\n",
        "    if activationName == 'sigmoid':\n",
        "        activationForward = sigmoid\n",
        "        activationBackward = sigmoidGrad\n",
        "    elif activationName == 'tanh':\n",
        "        activationForward = tanh\n",
        "        activationBackward = tanhGrad\n",
        "    elif activationName == 'relu':\n",
        "        activationForward = relu\n",
        "        activationBackward = reluGrad\n",
        "    else:\n",
        "        print('invalid activation name') \n"
      ],
      "metadata": {
        "id": "mjDzahetI2BY"
      },
      "id": "mjDzahetI2BY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = None\n",
        "\n",
        "def setLossFns(lossFunction):\n",
        "    global loss\n",
        "    \n",
        "    if lossFunction == 'crossEntropy':\n",
        "        loss = 'crossEntropy'\n",
        "    else:\n",
        "        loss= 'squaredError'\n",
        "    "
      ],
      "metadata": {
        "id": "pwUZDR-tDktU"
      },
      "id": "pwUZDR-tDktU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method to print Accuracy\n",
        "def printAccuracy(network, x_test, targs_test):\n",
        "    predictedCorrect = 0\n",
        "    predictedWrong = 0\n",
        "    for n in range(len(x_test)):\n",
        "        ins = x_test[n]\n",
        "        A = forward_propagate(network, ins)\n",
        "        classPredicted = np.argmax(A, axis=0)[0]\n",
        "        classActual = np.argmax(targs_test[n], axis=0)[0]\n",
        "        if (classPredicted == classActual):\n",
        "            predictedCorrect = predictedCorrect + 1\n",
        "        else:\n",
        "            predictedWrong = predictedWrong + 1\n",
        "    accuracy = predictedCorrect / len(x_test) \n",
        "    print('Accuracy: ', accuracy)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "walyegRp1Q7N"
      },
      "id": "walyegRp1Q7N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Method to print Confusion Matrix\n",
        "\n",
        "def predict_label(network,x_test,targs_test,y_test):\n",
        "    acc = 0\n",
        "    y_pred=[]\n",
        "\n",
        "    for n in range(len(x_test)):\n",
        "        ins = x_test[n]\n",
        "        A = forward_propagate(network, ins)\n",
        "        classPredicted = np.argmax(A, axis=0)[0]\n",
        "        classPredicted=(classPredicted+1)%10\n",
        "        classActual = np.argmax(targs_test[n], axis=0)[0]\n",
        "        if (classPredicted == classActual):\n",
        "            acc = acc + 1\n",
        "        y_pred.append(classPredicted)\n",
        "\n",
        "    print(\"Accuracy: \",str((acc/len(y_test))*100),\"%\")\n",
        "    cm=metrics.confusion_matrix(y_true=y_test,y_pred=y_pred)\n",
        "    df_cm = pd.DataFrame(cm, index=[i for i in class_names],\n",
        "              columns=[i for i in class_names])\n",
        "    print(df_cm)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    ax = sn.heatmap(df_cm, annot=True,  cmap='Blues', fmt='d',linewidths=3, linecolor='black')\n",
        "    ax.set_yticklabels(class_names,rotation=0)\n",
        "    plt.xlabel(\"True Class\")  # x-axis label\n",
        "    plt.ylabel(\"Predicted Class\")  # y-axis label\n",
        "    plt.title('Confusion Matrix of FASHION-MNIST Dataset', fontsize=20)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-Jb749JmU-Hj"
      },
      "id": "-Jb749JmU-Hj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Common Utils Starts\n",
        "\n",
        "def resetdbanddW(network):\n",
        "    for layer in network:\n",
        "        m, n = layer['weights'].shape\n",
        "        layer['dW'] = np.zeros([m, n])\n",
        "        m, n = layer['bias'].shape\n",
        "        layer['db'] = np.zeros([m, n])\n",
        "\n",
        "def initialize_v_w_and_v_b(network):\n",
        "    for layer in network:\n",
        "        m, n = layer['weights'].shape\n",
        "        layer['v_w'] = np.zeros([m, n])\n",
        "        m, n = layer['bias'].shape\n",
        "        layer['v_b'] = np.zeros([m, n])\n",
        "\n",
        "def update_v_w_and_v_b(network, gamma):\n",
        "    for layer in network:\n",
        "        layer['v_w'] = gamma * layer['prev_v_w']\n",
        "        layer['v_b'] = gamma * layer['prev_v_b']\n",
        "\n",
        "def initialize_prev_v_w_and_prev_v_b(network):\n",
        "    for layer in network:\n",
        "        m, n = layer['weights'].shape\n",
        "        layer['prev_v_w'] = np.zeros([m, n])\n",
        "        m, n = layer['bias'].shape\n",
        "        layer['prev_v_b'] = np.zeros([m, n])\n",
        "\n",
        "### Common Utils Ends"
      ],
      "metadata": {
        "id": "98vH6bUr2FLv"
      },
      "id": "98vH6bUr2FLv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "setActivationFns(\"sigmoid\")"
      ],
      "metadata": {
        "id": "BxJKBzjnS_63"
      },
      "id": "BxJKBzjnS_63",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward propagate input to a network output\n",
        "\n",
        "def forward_propagate(network, X):\n",
        "    x = np.copy(X)\n",
        "\n",
        "    # Apply sigmoid for all previous layers\n",
        "    for layer in network[:-1]:\n",
        "        Z = np.dot(layer['weights'], x) + layer['bias']\n",
        "        layer['prevZ'] = Z  # Preactivation values of current layer\n",
        "        layer['prevA'] = x  # Activation values of previous layer\n",
        "        x = activationForward(Z) \n",
        "\n",
        "    # Apply softmax for the final layer\n",
        "    layer = network[-1]\n",
        "    Z = np.dot(layer['weights'], x) + layer['bias']\n",
        "    layer['prevZ'] = Z\n",
        "    layer['prevA'] = x\n",
        "    x = softmax(Z)\n",
        "    return x"
      ],
      "metadata": {
        "id": "dVypAHGM7zZ8"
      },
      "id": "dVypAHGM7zZ8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Back propagation\n",
        "def backward_propagation(network, actual, predicted):\n",
        "\n",
        "    global loss\n",
        "    if loss==\"crossEntropy\":\n",
        "        dZ = -(actual - predicted)\n",
        "    else:\n",
        "        y_label = predicted[np.argmax(actual)]\n",
        "        dZ = 2 * (y_label - 1) * y_label * ( actual - predicted )\n",
        "\n",
        "    for i in range(len(network) - 1, -1, -1):\n",
        "        # print(network[i]['prevA'])\n",
        "        m = network[i]['prevA'].shape[1]\n",
        "        network[i]['dW'] = network[i]['dW'] +  1 / m * np.dot(dZ, network[i]['prevA'].T)\n",
        "        network[i]['db'] = network[i]['db'] + 1 / m * np.sum(dZ, axis=1, keepdims=True)\n",
        "        prevdA = np.dot(network[i]['weights'].T, dZ)\n",
        "        if i != 0:\n",
        "            dZ = sigmoidGrad(prevdA, network[i-1]['prevZ'])"
      ],
      "metadata": {
        "id": "3kY5zrJs70dd"
      },
      "id": "3kY5zrJs70dd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update(network, learning_rate):\n",
        "    for layer in network:\n",
        "        layer['weights'] = layer['weights'] - learning_rate * layer['dW']\n",
        "        layer['bias'] = layer['bias'] - learning_rate * layer['db']"
      ],
      "metadata": {
        "id": "uDeEpO3l71mI"
      },
      "id": "uDeEpO3l71mI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stochastic Gradient Descent"
      ],
      "metadata": {
        "id": "ttrsnh7B1Yhv"
      },
      "id": "ttrsnh7B1Yhv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimser_sgd(network, config, x_train, targs, x_val, targs_val, x_test, targs_test):\n",
        "    setActivationFns(config.activation)\n",
        "    for epoch in range(config.number_of_epochs):\n",
        "        total_cost = 0\n",
        "        # Do reset accumulated db and dW here\n",
        "        no_of_steps=0\n",
        "        resetdbanddW(network)\n",
        "        for n in range(54000):\n",
        "            ins = x_train[n]\n",
        "            target = targs[n]\n",
        "            A = forward_propagate(network, ins)\n",
        "            backward_propagation(network, target, A)\n",
        "            no_of_steps+=1\n",
        "            if no_of_steps%config.batch_size==0:\n",
        "                update(network, config.learning_rate)\n",
        "                resetdbanddW(network)\n",
        "                no_of_steps=0\n",
        "            total_cost = total_cost + crossEntropyLoss(target, A)\n",
        "        print('Total cost for this epoch: ', total_cost)\n",
        "        if epoch==4:\n",
        "            calculate_loss_and_accuracy(network, config, x_train, targs, x_val, targs_val, x_test, targs_test, epoch, total_cost)\n",
        "\n",
        "    calculate_loss_and_accuracy(network, config, x_train, targs, x_val, targs_val, x_test, targs_test, epoch, total_cost)\n",
        "      "
      ],
      "metadata": {
        "id": "q30J8SiI74Hh"
      },
      "id": "q30J8SiI74Hh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss_and_accuracy(network, config, x_train, targs, x_val, targs_val, x_test, targs_test, epoch, total_cost):\n",
        "    epoch+=1\n",
        "    total_cost_val=0\n",
        "    for n in range(6000):\n",
        "        ins = x_val[n]\n",
        "        target = targs_val[n]\n",
        "        A = forward_propagate(network, ins)\n",
        "        total_cost_val = total_cost_val + crossEntropyLoss(target, A)\n",
        "\n",
        "    accuracy_val = printAccuracy(network, x_val, targs_val)\n",
        "    accuracy_test = printAccuracy(network, x_test, targs_test)\n",
        "    total_cost = total_cost / 54000 \n",
        "    total_cost_val = total_cost_val / 6000\n",
        "    wandb.log({\"val_loss\": total_cost_val, \"val_accuracy\": accuracy_val, \"loss\": total_cost, \"accuracy\": accuracy_test, \"epochs\": epoch })\n",
        "\n"
      ],
      "metadata": {
        "id": "faIZyrnDKiAj"
      },
      "id": "faIZyrnDKiAj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Momentum Gradient Descent"
      ],
      "metadata": {
        "id": "DWxj80JrsBEA"
      },
      "id": "DWxj80JrsBEA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Back propagation for mgd\n",
        "def backward_propagation_mgd(network, actual, predicted):\n",
        "    global loss\n",
        "    if loss==\"crossEntropy\":\n",
        "        dZ = -(actual - predicted)\n",
        "    else:\n",
        "        y_label = predicted[np.argmax(actual)]\n",
        "        dZ = 2 * (y_label - 1) * y_label * ( actual - predicted )\n",
        "\n",
        "    for i in range(len(network) - 1, -1, -1):\n",
        "        # print(network[i]['prevA'])\n",
        "        m = network[i]['prevA'].shape[1]\n",
        "        network[i]['dW'] = network[i]['dW'] + 1 / m * np.dot(dZ, network[i]['prevA'].T)\n",
        "        network[i]['db'] = network[i]['db'] + 1 / m * np.sum(dZ, axis=1, keepdims=True)\n",
        "        prevdA = np.dot(network[i]['weights'].T, dZ)\n",
        "        if i != 0:\n",
        "            dZ = sigmoidGrad(prevdA, network[i-1]['prevZ'])"
      ],
      "metadata": {
        "id": "9l2mJNUXsHFG"
      },
      "id": "9l2mJNUXsHFG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_mgd(network, learning_rate, gamma):\n",
        "    for layer in network:\n",
        "        v_w = gamma * layer['prev_v_w'] + learning_rate * layer['dW']\n",
        "        v_b = gamma * layer['prev_v_b'] + learning_rate * layer['db']\n",
        "\n",
        "        layer['weights'] = layer['weights'] - v_w\n",
        "        layer['bias'] = layer['bias'] - v_b\n",
        "\n",
        "        layer['prev_v_w'] = v_w\n",
        "        layer['prev_v_b'] = v_b"
      ],
      "metadata": {
        "id": "TlebKacbsQ0D"
      },
      "id": "TlebKacbsQ0D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seed(1)\n",
        "# no_of_inputs=784\n",
        "# no_of_neurons_in_hidden_layer=784\n",
        "# no_of_classes=10\n",
        "# no_of_hidden_layers=2\n",
        "# network = initialize_network(no_of_inputs, no_of_neurons_in_hidden_layer, no_of_classes, no_of_hidden_layers)\n",
        "# setActivationFns('tanh')"
      ],
      "metadata": {
        "id": "4kdYly6dsU39"
      },
      "id": "4kdYly6dsU39",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimser_mgd(network, config, x_train, targs, x_val, targs_val, x_test, targs_test):\n",
        "    setActivationFns(config.activation)\n",
        "    initialize_prev_v_w_and_prev_v_b(network)\n",
        "    for epoch in range(config.number_of_epochs):\n",
        "        total_cost = 0\n",
        "        # Do reset accumulated db and dW here\n",
        "        resetdbanddW(network)\n",
        "        no_of_steps=0\n",
        "        for n in range(54000):\n",
        "            ins = x_train[n]\n",
        "            target = targs[n]\n",
        "            A = forward_propagate(network, ins)\n",
        "            backward_propagation_mgd(network, target, A)\n",
        "            no_of_steps+=1\n",
        "            if no_of_steps%config.batch_size==0:\n",
        "                update_mgd(network, config.learning_rate, gamma=0.9)\n",
        "                resetdbanddW(network)\n",
        "                no_of_steps=0\n",
        "            total_cost = total_cost + crossEntropyLoss(target, A)\n",
        "        print('Total cost for this epoch: ', total_cost)\n",
        "        if epoch==4:\n",
        "            calculate_loss_and_accuracy(network, config, x_train, targs, x_val, targs_val, x_test, targs_test, epoch, total_cost)\n",
        "\n",
        "    calculate_loss_and_accuracy(network, config, x_train, targs, x_val, targs_val, x_test, targs_test, epoch, total_cost)"
      ],
      "metadata": {
        "id": "ADJGmdF8sWJD"
      },
      "id": "ADJGmdF8sWJD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nesterov Accelerated Gradient Descent"
      ],
      "metadata": {
        "id": "4mb4K9Aw04UI"
      },
      "id": "4mb4K9Aw04UI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward propagate for NAGD\n",
        "def forward_propagate_nagd(network, X):\n",
        "    x = np.copy(X)\n",
        "    nagdx = np.copy(X)\n",
        "\n",
        "    # Apply activationForward for all previous layers\n",
        "    for layer in network[:-1]:\n",
        "        Z = np.dot(layer['weights'], x) + layer['bias']\n",
        "        nagdZ = np.dot(layer['weights'] - layer['v_w'], nagdx) + layer['bias'] - layer['v_b']\n",
        "        \n",
        "        layer['prevZ'] = Z  # Preactivation values of current layer\n",
        "        layer['prevA'] = x  # Activation values of previous layer\n",
        "\n",
        "        layer['prevnagdZ'] = nagdZ  # Preactivation values of current layer\n",
        "        layer['prevnagdA'] = nagdx # Activation values of previous layer\n",
        "\n",
        "        x = activationForward(Z) \n",
        "        nagdx = activationForward(nagdZ)\n",
        "\n",
        "    # Apply softmax for the final layer\n",
        "    layer = network[-1]\n",
        "    Z = np.dot(layer['weights'], x) + layer['bias']\n",
        "    nagdZ = np.dot(layer['weights'] - layer['v_w'], nagdx) + layer['bias'] - layer['v_b']\n",
        "    layer['prevZ'] = Z\n",
        "    layer['prevA'] = x\n",
        "    layer['prevnagdZ'] = nagdZ\n",
        "    layer['prevnagdA'] = nagdx\n",
        "    # x = softmax(Z)\n",
        "    nagdx = softmax(nagdZ)\n",
        "    return nagdx"
      ],
      "metadata": {
        "id": "rSuOFALZ0-Bt"
      },
      "id": "rSuOFALZ0-Bt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Back propagation for NAGD\n",
        "def backward_propagation_nagd(network, actual, predicted):\n",
        "    global loss\n",
        "    if loss==\"crossEntropy\":\n",
        "        dZ = -(actual - predicted)\n",
        "    else:\n",
        "        y_label = predicted[np.argmax(actual)]\n",
        "        dZ = 2 * (y_label - 1) * y_label * ( actual - predicted )\n",
        "\n",
        "    for i in range(len(network) - 1, -1, -1):\n",
        "        # print(network[i]['prevA'])\n",
        "        m = network[i]['prevA'].shape[1]\n",
        "        network[i]['dW'] = network[i]['dW'] + 1 / m * np.dot(dZ, network[i]['prevnagdA'].T)\n",
        "        network[i]['db'] = network[i]['db'] + 1 / m * np.sum(dZ, axis=1, keepdims=True)\n",
        "        prevdA = np.dot(network[i]['weights'].T, dZ)\n",
        "        if i != 0:\n",
        "            dZ = sigmoidGrad(prevdA, network[i-1]['prevnagdZ'])"
      ],
      "metadata": {
        "id": "zmxXK2wL1sJj"
      },
      "id": "zmxXK2wL1sJj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update for nagd\n",
        "def update_nagd(network, learning_rate, gamma):\n",
        "    for layer in network:\n",
        "        v_w = gamma * layer['prev_v_w'] + learning_rate * layer['dW']\n",
        "        v_b = gamma * layer['prev_v_b'] + learning_rate * layer['db']\n",
        "\n",
        "        layer['weights'] = layer['weights'] - v_w\n",
        "        layer['bias'] = layer['bias'] - v_b\n",
        "\n",
        "        layer['prev_v_w'] = v_w\n",
        "        layer['prev_v_b'] = v_b"
      ],
      "metadata": {
        "id": "WpdQIT1m1ydt"
      },
      "id": "WpdQIT1m1ydt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seed(1)\n",
        "# no_of_inputs=784\n",
        "# no_of_neurons_in_hidden_layer=784\n",
        "# no_of_classes=10\n",
        "# no_of_hidden_layers=2\n",
        "# network = initialize_network(no_of_inputs, no_of_neurons_in_hidden_layer, no_of_classes, no_of_hidden_layers)\n",
        "# setActivationFns('relu')"
      ],
      "metadata": {
        "id": "sZDz84K52w3K"
      },
      "id": "sZDz84K52w3K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimser_nagd(network, config, x_train, targs, x_val, targs_val, x_test, targs_test):\n",
        "    setActivationFns(config.activation)\n",
        "    initialize_prev_v_w_and_prev_v_b(network)\n",
        "    for epoch in range(config.number_of_epochs):\n",
        "        total_cost = 0\n",
        "        # Do reset accumulated db and dW here\n",
        "        resetdbanddW(network)\n",
        "        no_of_steps=0\n",
        "        update_v_w_and_v_b(network, gamma=0.9)\n",
        "        for n in range(54000):\n",
        "            ins = x_train[n]\n",
        "            target = targs[n]\n",
        "            A = forward_propagate_nagd(network, ins)\n",
        "            backward_propagation_nagd(network, target, A)\n",
        "            no_of_steps+=1\n",
        "            if no_of_steps%config.batch_size==0:\n",
        "                update_nagd(network, config.learning_rate, gamma=0.9)\n",
        "                resetdbanddW(network)\n",
        "                no_of_steps=0\n",
        "            total_cost = total_cost + crossEntropyLoss(target, A)\n",
        "        print('Total cost for this epoch: ', total_cost)\n",
        "        if epoch==4:\n",
        "            calculate_loss_and_accuracy(network, config, x_train, targs, x_val, targs_val, x_test, targs_test, epoch, total_cost)\n",
        "\n",
        "    calculate_loss_and_accuracy(network, config, x_train, targs, x_val, targs_val, x_test, targs_test, epoch, total_cost)"
      ],
      "metadata": {
        "id": "YoOZv2Eb23Dm"
      },
      "id": "YoOZv2Eb23Dm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RMSProp"
      ],
      "metadata": {
        "id": "jM1gekus44zg"
      },
      "id": "jM1gekus44zg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Back propagation for mgd\n",
        "def backward_propagation_rmsprop(network, actual, predicted):\n",
        "    global loss\n",
        "    if loss==\"crossEntropy\":\n",
        "        dZ = -(actual - predicted)\n",
        "    else:\n",
        "        y_label = predicted[np.argmax(actual)]\n",
        "        dZ = 2 * (y_label - 1) * y_label * ( actual - predicted )\n",
        "\n",
        "    for i in range(len(network) - 1, -1, -1):\n",
        "        # print(network[i]['prevA'])\n",
        "        m = network[i]['prevA'].shape[1]\n",
        "        network[i]['dW'] = network[i]['dW'] + 1 / m * np.dot(dZ, network[i]['prevA'].T)\n",
        "        network[i]['db'] = network[i]['db'] + 1 / m * np.sum(dZ, axis=1, keepdims=True)\n",
        "        prevdA = np.dot(network[i]['weights'].T, dZ)\n",
        "        if i != 0:\n",
        "            dZ = sigmoidGrad(prevdA, network[i-1]['prevZ'])"
      ],
      "metadata": {
        "id": "pWQAc49D5TZD"
      },
      "id": "pWQAc49D5TZD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_rmpsprop(network, learning_rate, beta1, eps=1e-8):\n",
        "    for layer in network:\n",
        "      layer['v_w'] = beta1 * layer['v_w'] + (1 - beta1) * layer['dW']**2\n",
        "      layer['v_b'] = beta1 * layer['v_b'] + (1 - beta1) * layer['db']**2\n",
        "\n",
        "      layer['weights'] = layer['weights'] - (learning_rate / np.sqrt(layer['v_w'] + eps)) * layer['dW']\n",
        "      layer['bias'] = layer['bias'] - (learning_rate / np.sqrt(layer['v_b'] + eps)) * layer['db']"
      ],
      "metadata": {
        "id": "PQNYPlRO5TRw"
      },
      "id": "PQNYPlRO5TRw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seed(1)\n",
        "# no_of_inputs=784\n",
        "# no_of_neurons_in_hidden_layer=784\n",
        "# no_of_classes=10\n",
        "# no_of_hidden_layers=2\n",
        "# network = initialize_network(no_of_inputs, no_of_neurons_in_hidden_layer, no_of_classes, no_of_hidden_layers)\n",
        "# setActivationFns('sigmoid')"
      ],
      "metadata": {
        "id": "LxPl8TYT5TK4"
      },
      "id": "LxPl8TYT5TK4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimser_rmsprop(network, config, x_train, targs, x_val, targs_val, x_test, targs_test):\n",
        "    setActivationFns(config.activation)\n",
        "    initialize_v_w_and_v_b(network)\n",
        "    eps, beta1 = 1e-8, 0.9\n",
        "    for epoch in range(config.number_of_epochs):\n",
        "        total_cost = 0\n",
        "        # Do reset accumulated db and dW here\n",
        "        resetdbanddW(network)\n",
        "        no_of_steps=0\n",
        "        for n in range(54000):\n",
        "            ins = x_train[n]\n",
        "            target = targs[n]\n",
        "            A = forward_propagate(network, ins)\n",
        "            backward_propagation_rmsprop(network, target, A)\n",
        "            no_of_steps+=1\n",
        "            if no_of_steps%config.batch_size==0:\n",
        "                update_rmpsprop(network, config.learning_rate, beta1=beta1, eps=eps)\n",
        "                resetdbanddW(network)\n",
        "                no_of_steps=0\n",
        "            total_cost = total_cost + crossEntropyLoss(target, A)\n",
        "        print('Total cost for this epoch: ', total_cost)\n",
        "        if epoch==4:\n",
        "            calculate_loss_and_accuracy(network, config, x_train, targs, x_val, targs_val, x_test, targs_test, epoch, total_cost)\n",
        "\n",
        "    calculate_loss_and_accuracy(network, config, x_train, targs, x_val, targs_val, x_test, targs_test, epoch, total_cost)"
      ],
      "metadata": {
        "id": "LsD4Nhsb5ZcD"
      },
      "id": "LsD4Nhsb5ZcD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adam"
      ],
      "metadata": {
        "id": "_peOYAWyJvTv"
      },
      "id": "_peOYAWyJvTv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propagation_adam(network, actual, predicted):\n",
        "    global loss\n",
        "    if loss==\"crossEntropy\":\n",
        "        dZ = -(actual - predicted)\n",
        "    else:\n",
        "        y_label = predicted[np.argmax(actual)]\n",
        "        dZ = 2 * (y_label - 1) * y_label * ( actual - predicted )\n",
        "\n",
        "    for i in range(len(network) - 1, -1, -1):\n",
        "\n",
        "        m = network[i]['prevA'].shape[1]\n",
        "        network[i]['dW'] += 1 / m * np.dot(dZ, network[i]['prevA'].T)\n",
        "        network[i]['db'] += 1 / m * np.sum(dZ)\n",
        "        prevdA = np.dot(network[i]['weights'].T, dZ)\n",
        "        if i != 0:\n",
        "            dZ = sigmoidGrad(prevdA, network[i-1]['prevZ'])"
      ],
      "metadata": {
        "id": "z0TH6HBca97h"
      },
      "id": "z0TH6HBca97h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_adam(network, epoch, learning_rate, beta1, beta2, optimizer):\n",
        "    \n",
        "        for layer in network:\n",
        "            if epoch!=0:\n",
        "                layer['m_w']=beta1*layer['m_w'] + (1-beta1)*layer['dW']\n",
        "                layer['m_b']=beta1*layer['m_b'] + (1-beta1)*layer['db']\n",
        "\n",
        "                layer['v_w']=beta2*layer['v_w'] + (1-beta2)*np.square(layer['dW'])\n",
        "                layer['v_b']=beta2*layer['v_b'] + (1-beta2)*np.square(layer['db'])\n",
        "\n",
        "            else:\n",
        "                layer['m_w']= (1-beta1)*layer['dW']\n",
        "                layer['m_b']= (1-beta1)*layer['db']\n",
        "\n",
        "                layer['v_w']=(1-beta2)*np.square(layer['dW'])\n",
        "                layer['v_b']= (1-beta2)*np.square(layer['db'])\n",
        "\n",
        "            if optimizer==\"adam\":\n",
        "                layer['m_w_hat']=layer['m_w']/(1-math.pow(beta1,epoch+1))\n",
        "            else:\n",
        "                layer['m_w_hat']=layer['m_w']*beta1+(1-beta1)*layer['dW']/(1-math.pow(beta1,epoch+1))\n",
        "                \n",
        "            layer['m_b_hat']=layer['m_b']/(1-math.pow(beta1,epoch+1))\n",
        "\n",
        "            layer['v_w_hat']=layer['v_w']/(1-math.pow(beta2,epoch+1))\n",
        "            layer['v_b_hat']=layer['v_b']/(1-math.pow(beta2,epoch+1))\n",
        "        \n",
        "        \n",
        "            layer['weights'] = layer['weights'] - (learning_rate / np.sqrt(layer['v_w_hat'] + learning_rate))*layer['m_w_hat'] \n",
        "            layer['bias'] = layer['bias'] - (learning_rate / np.sqrt(layer['v_b_hat'] + learning_rate))*layer['m_b_hat']\n"
      ],
      "metadata": {
        "id": "QMuOL6IfbKht"
      },
      "id": "QMuOL6IfbKht",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seed(1)\n",
        "# no_of_inputs=784\n",
        "# no_of_neurons_in_hidden_layer=10\n",
        "# no_of_classes=10\n",
        "# no_of_hidden_layers=2\n",
        "# network = initialize_network(no_of_inputs, no_of_neurons_in_hidden_layer, no_of_classes, no_of_hidden_layers)\n",
        "# setActivationFns('sigmoid')"
      ],
      "metadata": {
        "id": "DrUXwqcKb6l5"
      },
      "id": "DrUXwqcKb6l5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimser_adam(network, config, x_train, targs, x_val, targs_val, x_test, targs_test):\n",
        "    setActivationFns(config.activation)\n",
        "    initialize_prev_v_w_and_prev_v_b(network)\n",
        "    for epoch in range(config.number_of_epochs):\n",
        "        no_of_steps=0\n",
        "        beta1,beta2=0.9,0.999\n",
        "        total_cost=0\n",
        "        resetdbanddW(network)\n",
        "        for n in range(54000):\n",
        "            ins = x_train[n]\n",
        "            target = targs[n]\n",
        "            A = forward_propagate(network, ins)\n",
        "            backward_propagation_adam(network, target, A)\n",
        "            no_of_steps+=1\n",
        "            if no_of_steps%config.batch_size==0:\n",
        "                  update_adam(network, epoch, config.learning_rate, beta1, beta2,config.optimizer)\n",
        "                  resetdbanddW(network)\n",
        "                  no_of_steps=0\n",
        "        total_cost = total_cost + crossEntropyLoss(target, A)\n",
        "        print('Total cost for this epoch: ', total_cost)\n",
        "        if epoch==4 or config.loss:\n",
        "            calculate_loss_and_accuracy(network, config, x_train, targs, x_val, targs_val, x_test, targs_test, epoch, total_cost)\n",
        "\n",
        "    calculate_loss_and_accuracy(network, config, x_train, targs, x_val, targs_val, x_test, targs_test, epoch, total_cost) "
      ],
      "metadata": {
        "id": "lkxjp52ZcK9p"
      },
      "id": "lkxjp52ZcK9p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "  \"name\" : \"Final_sweep\",\n",
        "  \"method\" : \"random\",\n",
        "  \"parameters\" : {\n",
        "    \"neurons_hidden_layer\" : {\n",
        "      \"values\" : [32, 64, 128]\n",
        "      # \"values\" : [32]\n",
        "    },\n",
        "    \"number_of_epochs\" : {\n",
        "        \"values\" : [5,10]\n",
        "        # \"values\" : [10]\n",
        "    },\n",
        "    \"activation\" : {\n",
        "        \"values\" : [\"sigmoid\" , \"relu\" , \"tanh\"]\n",
        "    },\n",
        "    \"no_of_hidden_layer\" : {\n",
        "      \"values\" : [3, 4, 5]\n",
        "      # \"values\" : [2]\n",
        "    },\n",
        "    \"batch_size\" :{\n",
        "      \"values\" : [16,32,64]\n",
        "    },\n",
        "    \"optimizer\" : {\n",
        "        \"values\" : [\"adam\", \"rmsprop\" ,\"nadam\" , \"sgd\" , \"momentum\" , \"nesterov\"]\n",
        "        # \"values\" : [\"adam\"]\n",
        "    },\n",
        "    \"initialization\": {\n",
        "      \"values\" : [\"random\", \"xavier\"]  \n",
        "    },\n",
        "    \"weight_decay\" : {\n",
        "        \"values\" : [0, 0.0005, 0.5]\n",
        "    },\n",
        "    \"learning_rate\" :{\n",
        "      \"values\" : [0.1,0.01,0.001]\n",
        "    }\n",
        "  }\n",
        "}\n",
        "sweep_id = wandb.sweep(sweep_config,project=\"DL_Assignment_1\",entity=\"cs21m010-cs21m041\")"
      ],
      "metadata": {
        "id": "MyaBmxwUdQSg"
      },
      "id": "MyaBmxwUdQSg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(config=None):\n",
        "\n",
        "    # Initialize a new wandb run\n",
        "    with wandb.init(config=config) as run:\n",
        "\n",
        "        # If called by wandb.agent, as below,\n",
        "        # this config will be set by Sweep Controller\n",
        "        config = wandb.config\n",
        "        \n",
        "        # hl_3_bs_16_ac_tanh \n",
        "        if config.loss:\n",
        "            setLossFns(config.loss)\n",
        "            sweep_name = '{}'.format(config.loss)\n",
        "\n",
        "        else:\n",
        "            setLossFns('crossEntropy')\n",
        "            sweep_name = 'hl_{}_bs_{}_ac_{}_{}'.format(config.no_of_hidden_layer, config.batch_size, config.activation, config.optimizer)\n",
        "\n",
        "        print(sweep_name)\n",
        "        run.name = sweep_name\n",
        "        \n",
        "        np.random.seed(0)\n",
        "        no_of_inputs=784\n",
        "        no_of_neurons_in_hidden_layer=config.neurons_hidden_layer\n",
        "        no_of_classes=10\n",
        "        no_of_hidden_layers=config.no_of_hidden_layer\n",
        "        network = initialize_network(no_of_inputs, no_of_neurons_in_hidden_layer, no_of_classes, no_of_hidden_layers, config )\n",
        "        \n",
        "        \n",
        "\n",
        "        if config.optimizer==\"sgd\":\n",
        "            optimser_sgd(network, config, x_train, targs, x_val, targs_val, x_test, targs_test)\n",
        "        elif config.optimizer==\"momentum\":\n",
        "            optimser_mgd(network, config, x_train, targs, x_val, targs_val, x_test, targs_test)\n",
        "        elif config.optimizer==\"nesterov\":\n",
        "            optimser_nagd(network, config, x_train, targs, x_val, targs_val, x_test, targs_test)\n",
        "        elif config.optimizer==\"rmsprop\":\n",
        "            optimser_rmsprop(network, config, x_train, targs, x_val, targs_val, x_test, targs_test)\n",
        "        elif config.optimizer==\"adam\":\n",
        "            optimser_adam(network, config, x_train, targs, x_val, targs_val, x_test, targs_test)\n",
        "        else:\n",
        "            optimser_adam(network, config, x_train, targs, x_val, targs_val, x_test, targs_test)\n",
        "\n",
        "        if 'loss' in config.keys() and config.loss=='crossEntropy':\n",
        "          print(\"came here\")\n",
        "          predict_label(network,x_test,targs_test,y_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "gNc9ARpNrF6U"
      },
      "id": "gNc9ARpNrF6U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb.agent(sweep_id, train, count=5)"
      ],
      "metadata": {
        "id": "w1a13ybgYnF-"
      },
      "id": "w1a13ybgYnF-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_config = {\n",
        "  \"name\" : \"Best_Sweep\",\n",
        "  \"method\" : \"random\",\n",
        "  \"parameters\" : {\n",
        "    \"neurons_hidden_layer\" : {\n",
        "      \"values\" : [128]\n",
        "    },\n",
        "    \"number_of_epochs\" : {\n",
        "        \"values\" : [10]\n",
        "    },\n",
        "    \"activation\" : {\n",
        "        \"values\" : [\"sigmoid\"]\n",
        "    },\n",
        "    \"no_of_hidden_layer\" : {\n",
        "      \"values\" : [3]\n",
        "    },\n",
        "    \"batch_size\" :{\n",
        "      \"values\" : [32]\n",
        "    },\n",
        "    \"optimizer\" : {\n",
        "        \"values\" : [\"adam\"]\n",
        "    },\n",
        "    \"initialization\": {\n",
        "      \"values\" : [\"xavier\"]  \n",
        "    },\n",
        "    \"weight_decay\" : {\n",
        "        \"values\" : [0]\n",
        "    },\n",
        "    \"loss\" : {\n",
        "        # \"values\" : [\"crossEntropy\",\"squaredError\"]\n",
        "        \"values\" : [\"crossEntropy\"]\n",
        "    },\n",
        "    \"learning_rate\" :{\n",
        "      \"values\" : [0.01]\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(best_config,project=\"DL_Assignment_1\",entity=\"cs21m010-cs21m041\")\n",
        "wandb.agent(sweep_id, train, count=2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        },
        "id": "f7YS0i45QWu-",
        "outputId": "d992f23f-c57b-4b45-8e39-5efa7de67322"
      },
      "id": "f7YS0i45QWu-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f9708a68990>> (for pre_run_cell):\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36m_resume_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resuming backend\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublish_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_jupyter_teardown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface.py\u001b[0m in \u001b[0;36mpublish_resume\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpublish_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0mresume\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResumeRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface_shared.py\u001b[0m in \u001b[0;36m_publish_resume\u001b[0;34m(self, resume)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResumeRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRunRecord\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface_queue.py\u001b[0m in \u001b[0;36m_publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pb.Record\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_check\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The wandb backend process has shutdown\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: The wandb backend process has shutdown"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 6aidzp5k\n",
            "Sweep URL: https://wandb.ai/cs21m010-cs21m041/DL_Assignment_1/sweeps/6aidzp5k\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tk9c6dox with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitialization: xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: crossEntropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tneurons_hidden_layer: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_of_hidden_layer: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_epochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/cs21m010-cs21m041/DL_Assignment_1/runs/tk9c6dox\" target=\"_blank\">neat-sweep-1</a></strong> to <a href=\"https://wandb.ai/cs21m010-cs21m041/DL_Assignment_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "Sweep page: <a href=\"https://wandb.ai/cs21m010-cs21m041/DL_Assignment_1/sweeps/6aidzp5k\" target=\"_blank\">https://wandb.ai/cs21m010-cs21m041/DL_Assignment_1/sweeps/6aidzp5k</a><br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "crossEntropy\n",
            "setting activation function as:  sigmoid\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
            "  after removing the cwd from sys.path.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total cost for this epoch:  1.2601798029943223\n",
            "Accuracy:  0.4013333333333333\n",
            "Accuracy:  0.4036\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MRUXU2VnaHxY"
      },
      "id": "MRUXU2VnaHxY",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "assignment_1.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}